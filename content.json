{"meta":{"title":"JackYip","subtitle":"Blog","description":null,"author":"yezhejack","url":"http://yezhejack.github.io"},"pages":[{"title":"about","date":"2015-12-30T02:41:57.000Z","updated":"2017-05-03T08:40:04.000Z","comments":false,"path":"about/index.html","permalink":"http://yezhejack.github.io/about/index.html","excerpt":"","keywords":null,"text":"E-mail: yezhejack at gmail.comGitHub: https://github.com/yezhejack Current self-motived Plan Every week two papers PRML cs224n Details of every weekMay 1st, 2017 to May 7th, 2017","raw":null,"content":null}],"posts":[{"title":"ACL 2014-2015 情感分析和词向量论文整理","slug":"ACL 2014-2015 情感分析和词向量论文整理","date":"2017-07-02T08:08:47.000Z","updated":"2017-07-02T08:13:19.000Z","comments":true,"path":"2017/07/02/ACL 2014-2015 情感分析和词向量论文整理/","link":"","permalink":"http://yezhejack.github.io/2017/07/02/ACL 2014-2015 情感分析和词向量论文整理/","excerpt":"","keywords":null,"text":"","raw":null,"content":null,"categories":[{"name":"Papers","slug":"Papers","permalink":"http://yezhejack.github.io/categories/Papers/"}],"tags":[{"name":"Paper","slug":"Paper","permalink":"http://yezhejack.github.io/tags/Paper/"},{"name":"ACL","slug":"ACL","permalink":"http://yezhejack.github.io/tags/ACL/"}]},{"title":"wordpress网站搭建","slug":"wordpress网站搭建","date":"2017-06-16T05:09:46.000Z","updated":"2017-07-02T08:16:51.000Z","comments":true,"path":"2017/06/16/wordpress网站搭建/","link":"","permalink":"http://yezhejack.github.io/2017/06/16/wordpress网站搭建/","excerpt":"","keywords":null,"text":"WordPress + ceph网站搭建requirment 阿里云主机 ceph wordpress框架 目标搭建出一个前端基于wordpress，后端基于ceph的视频点播网站 配置单机版ceph下面是这个作业的重头戏，就是配置ceph 安装ceph-deploy这是一个专门用于配置ceph的工具123wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -echo deb http://download.ceph.com/debian-jewel/ trusty main | sudo tee /etc/apt/sources.list.d/ceph.listsudo apt-get update &amp;&amp; sudo apt-get install ceph-deploy 解析hostname通过ifconifg查看当前的ip地址，然后在/etc/hosts中加上一条(假如当前的hostname是ubuntu的时候)1&lt;ip&gt; ubuntu 新建一个专门ceph-deploy用户这个用户需要拥有不需要输入密码就能够使用sudo权限的能力1234sudo useradd -m -s /bin/bash ceph-deploysudo passwd ceph-deployecho \"ceph-deploy ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/ceph-deploysudo chmod 0440 /etc/sudoers.d/ceph-deploy 切换至ceph-deploy从现在开始，配置ceph的过程都是在新的用户ceph-deploy下执行的。1su ceph-deploy 配置ssh需要让中心节点能无密钥访问其他节点12ssh-keygenssh-copy-id ceph-deploy@ubuntu 这里因为是单节点版本因此都是访问自己 建立cluster1234cd ~mkdir my-clustercd my-clusterceph-deploy new ubuntu 向目录下的ceph.conf的尾部增加两条12osd pool default size = 2osd crush chooseleaf type = 0 配置其它节点1ceph-deploy install ubuntu 这一步原本是会通过当前节点去往其它节点上安装ceph的软件，这也是为什么需要一个不用密码就能使用sudo权限的账户来配合部署。 初始化监控器集群至少需要一个监控器1ceph-deploy mon create-initial 虚拟块设备块设备可以看作是一个硬盘，这里需要用到一个小技巧来虚拟出三个块来给ceph使用 可参考http://www.cnblogs.com/YaoDD/p/5217578.html 12cd ~dd if=/dev/zero of=ceph-volumes.img bs=1M count=25600 oflag=direct 8192表示的是初始化的三个块的总存储量为25600M，也就是25G，因为我打算每个块设备分出8G的空间，事实证明，当所有初始化完成后，一个块设备中有5G左右的空间要被其它ceph的文件占据掉。12345678910111213sgdisk -g --clear ceph-volumes.imgsudo apt-get install lvm2sudo vgcreate ceph-volumes $(sudo losetup --show -f ceph-volumes.img)sudo lvcreate -L8G -nceph0 ceph-volumessudo lvcreate -L8G -nceph1 ceph-volumessudo lvcreate -L8G -nceph2 ceph-volumessudo mkfs.xfs -f /dev/ceph-volumes/ceph0sudo mkfs.xfs -f /dev/ceph-volumes/ceph1sudo mkfs.xfs -f /dev/ceph-volumes/ceph2mkdir -p /srv/ceph/&#123;osd0,osd1,osd2,mon0,mds0&#125;sudo mount /dev/ceph-volumes/ceph0 /srv/ceph/osd0sudo mount /dev/ceph-volumes/ceph1 /srv/ceph/osd1sudo mount /dev/ceph-volumes/ceph2 /srv/ceph/osd2 三个osd就是ceph用来存数据的地方了 添加osd设备123ceph-deploy osd prepare ubuntu:/srv/ceph/osd0ceph-deploy osd prepare ubuntu:/srv/ceph/osd1ceph-deploy osd prepare ubuntu:/srv/ceph/osd2 激活osd设备123ceph-deploy osd activate ubuntu:/srv/ceph/osd0ceph-deploy osd activate ubuntu:/srv/ceph/osd1ceph-deploy osd activate ubuntu:/srv/ceph/osd2 如果出现权限错误，则需要修改一下这几个挂载点的权限123sudo chown ceph:ceph /srv/ceph/osd0sudo chown ceph:ceph /srv/ceph/osd1sudo chown ceph:ceph /srv/ceph/osd2 或者123sudo chown ceph-deploy:ceph-deploy /srv/ceph/osd0sudo chown ceph-deploy:ceph-deploy /srv/ceph/osd1sudo chown ceph-deploy:ceph-deploy /srv/ceph/osd2 分发配置和密钥12cd ~/my-clusterceph-deploy admin ubuntu 修改密钥权限1sudo chmod +r /etc/ceph/ceph.client.admin.keyring 查看集群的健康状况1sudo ceph -s 如果显示类似下面这样，有第二行的health HEALTH_OK，说明前面的配置没什么问题了12345678910cluster 6a48dfd2-8910-458f-abb4-385504e24236 health HEALTH_OK monmap e1: 1 mons at &#123;ubuntu=172.17.237.184:6789/0&#125; election epoch 3, quorum 0 ubuntu fsmap e5: 1/1/1 up &#123;0=ubuntu=up:active&#125; osdmap e20: 3 osds: 3 up, 3 in flags sortbitwise,require_jewel_osds pgmap v62: 320 pgs, 3 pools, 15497 kB data, 24 objects 15497 MB used, 9049 MB / 24546 MB avail 320 active+clean 配置基于ceph的文件系统添加mds，如果不添加这个的话，会导致创建出来的文件系统无法挂载12345ceph-deploy mds create ubuntuceph osd pool create cephfs_data 128ceph osd pool create cephfs_metadata 128ceph fs new cephfs cephfs_metadata cephfs_datasudo apt-get install ceph-fs-common 挂载1sudo mkdir -p /var/www/video 希望video文件夹下的文件放在cephs上 1234cat ~/my-cluster/ceph.client.admin.keyring[client.admin] key = AQCv2yRXOVlUMxAAK+e6gehnirXTV0O8PrJYQQ== 记录下其中的密钥，后面挂载的时候需要填写 1sudo mount -t ceph ubuntu:6789:/ /var/www/video -o name=admin,secret=AQCv2yRXOVlUMxAAK+e6gehnirXTV0O8PrJYQQ== 检查一下是否挂载成功1df -h /mnt/mycephfs 如下显示，则成功123yezhe@ubuntu:~$ df -h /var/www/videoFilesystem Size Used Avail Use% Mounted on172.17.237.184:6789:/ 24G 16G 8.9G 64% /var/www/video 安装mysql1234wget https://repo.mysql.com//mysql-apt-config_0.8.6-1_all.debsudo dpkg -i mysql-apt-config_0.8.6-1_all.debsudo apt-get updatesudo apt-get install mysql-server 配置mysql1234mysql -u root -pCREATE DATABASE wordpress;GRANT ALL PRIVILEGES ON wordpress.* TO \"yezhewp\"@\"localhost\" IDENTIFIED BY \"password\";FLUSH PRIVILEGES; 安装php+nginx123sudo apt-get install nginxsudo apt-get install php5-fpmsudo apt-get install php5-mysqlnd-ms 除此之外还需要安装好mysql，并且安装上一步来配置，这里之所以先配置mysql是因为我的服务器上已经安装有mysql了 配置nginx1sudo vim /etc/nginx/sites-available/default 将其中的下面的部分的注释去除，使其生效 12345678910location ~ \\.php$ &#123; fastcgi_split_path_info ^(.+\\.php)(/.+)$; # NOTE: You should have \"cgi.fix_pathinfo = 0;\" in php.ini # With php5-cgi alone: fastcgi_pass 127.0.0.1:9000; # With php5-fpm: #fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_index index.php; include fastcgi_params;&#125; 并且server block里面的index也要修改为1index index.php 或者在前面加上index.php然后保留原有的值 修改根目录root，修改为/var/www 配置php1sudo vim /etc/php5/fpm/pool.d/www.conf 确保其中的1listen = 127.0.0.1:9000 然后重启php和nginx12sudo service php5-fpm restartsudo service nginx restart 下载wordpress123wget https://wordpress.org/latest.tar.gztar -xvf latest.tar.gzsudo mv wordpress /var/www 修改wordpress 权限因为nginx是用用户www-data来访问资源的，所以要保证www-data对video有可读权限，对/var/www/wordpress有所有权123cd /var/wwwsudo chown -R www-data:www-data wordpresssudo chmod +x video 配置wordpress进入http:///wordpress/wp-admin/install.php，然后按照教程初始化，之后把视频放在/var/www/video的目录下，就可以往wordpress的文章内插入视频了，假如有一个视频是/var/www/video/test.m4v对应的url是http:///video/test.m4v","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"wordpress","slug":"wordpress","permalink":"http://yezhejack.github.io/tags/wordpress/"}]},{"title":"mxnet笔记","slug":"mxnet笔记","date":"2017-05-27T14:50:41.000Z","updated":"2017-05-27T14:50:41.000Z","comments":true,"path":"2017/05/27/mxnet笔记/","link":"","permalink":"http://yezhejack.github.io/2017/05/27/mxnet笔记/","excerpt":"","keywords":null,"text":"可以使用的metricsacc f1 mae rmse mse top_k_accuracy ce accuracy","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://yezhejack.github.io/tags/Natural-Language-Processing/"}]},{"title":"Counter-fitting Word Vectors to Linguistic Constraints","slug":"Counter-fitting Word Vectors to Linguistic Constraints","date":"2017-05-15T05:20:00.000Z","updated":"2017-05-15T06:28:47.000Z","comments":true,"path":"2017/05/15/Counter-fitting Word Vectors to Linguistic Constraints/","link":"","permalink":"http://yezhejack.github.io/2017/05/15/Counter-fitting Word Vectors to Linguistic Constraints/","excerpt":"","keywords":null,"text":"Introduction这是一篇出自剑桥大学和苹果公司的论文。 传统的词向量，例如Glove会有两个缺点，需要注入一些额外的知识特征来解决。 Related Work大多数的词向量改进工作都是将焦点集中在将已知语义相近的词对的空间表示的距离拉近的方法上。 现在出现了一些使用轻量化的“后处理”过程来修改现成的词向量，而不是用大量的语料库来训练或重训练词向量。 与本文相近的一个工作Learning semantic word embeddings based on ordinal knowledge constraints。 Counter-fitting Word Vectors to Linguistic ConstraintsA和S是两个带有限制的集合，有多个对组成(i,j)。然后可以有三个约束方程。前两个是让同义词更近，让反义词更远。第三个则是让原有空间的分布式信息尽可能地保留下来。 $$AR(V^\\prime) = \\sum_{(u,w) \\in A} \\tau(\\delta-d(\\mathbf(v^{\\prime}_{\\mu},v^{\\prime}_{w})))$$ $$SA(V^\\prime) = \\sum_{(u,w) \\in A} \\tau(d(\\mathbf(v^{\\prime}_{\\mu},v^{\\prime}_{w}))-\\gamma)$$ $$VSP(V,V^\\prime) = \\sum_{i=1}^{N} \\sum_{j in N(i)} \\tau (d(\\mathbf{v}_i^\\prime, \\mathbf{v}_j^\\prime) - d(\\mathbf{v}_i, \\mathbf{v}_j))$$ 然后讲三个优化目标进行线性组合 $$C(V,V^\\prime) = k_1 AR(V^\\prime) + k_2 SA(V^\\prime) + k_3 VSP(V,V^\\prime)$$ 但是不知道这个是用什么模型来学习的？？？ 词表用了两个词库 PPDB 2.0 只使用其中的Equivalence关系和Exclusion关系，并且只用了single-token的项 WordNet没有使用其中的同义词","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Word Vector","slug":"Word-Vector","permalink":"http://yezhejack.github.io/tags/Word-Vector/"},{"name":"Paper","slug":"Paper","permalink":"http://yezhejack.github.io/tags/Paper/"}]},{"title":"Decision Theory PRML","slug":"Decision Theory PRML","date":"2017-05-03T08:41:00.000Z","updated":"2017-05-03T11:01:01.000Z","comments":true,"path":"2017/05/03/Decision Theory PRML/","link":"","permalink":"http://yezhejack.github.io/2017/05/03/Decision Theory PRML/","excerpt":"","keywords":null,"text":"Exercise 1.19 1.18当空间的维度D接近无穷大的时候，在cube里的球体的相比于cube的体积比会趋于无穷。同时从cube的中心到cube的角的距离同这个球体的半径的比值也会趋于无穷大。这就说明当维度很高的情况下，大部分的体积都集中在cube的边角。 Ex 1.20讨论了高斯分布在高维度的情况下在远端的地方的概率密度更大。","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"PRML","slug":"PRML","permalink":"http://yezhejack.github.io/tags/PRML/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yezhejack.github.io/tags/Machine-Learning/"}]},{"title":"Linguistically Regularized LSTMs for Sentiment Classification","slug":"Linguistically Regularized LSTMs for Sentiment Classification","date":"2017-04-26T06:17:00.000Z","updated":"2017-04-26T13:51:32.000Z","comments":true,"path":"2017/04/26/Linguistically Regularized LSTMs for Sentiment Classification/","link":"","permalink":"http://yezhejack.github.io/2017/04/26/Linguistically Regularized LSTMs for Sentiment Classification/","excerpt":"","keywords":null,"text":"Abstract本文主要目的是为了产生linguistically coherent representations，其中使用了sentiment lexicons, negation words, 和intensity words。 Introduction让计算机能够理解情感一直是AI的核心任务，有许多方法来实现，比如lexicon-based classification以及早期的machine learning based methods，最新的还有CNN，recursive autoencoders和LSTM。 树形结构的模型，例如recursive autoencoder和Tree-LSTM都是需要短语级的标注语料信息，如果只使用句子级的标注信息的话，它们的性能将会大大下降。 序列模型，例如CNN和RNN难以产生在论文中记载的很好的效果。 很多的语言学知识没有应用到神经网络模型中。 这个工作的目标是设计一个简单的序列模型，然后使用语言学资源来帮助情感分类。 不用树形结构的模型，避免人工标注短语级语料 使用三种语言学信息：sentiment lexicon,negation words,intensity words。 Related Work Neural Networks for Sentiment Classification Applying Linguistic Knowledge for Sentiment Classification 否定词在修改文本情感中扮演关键角色。因为每个单独的否定词会影响情感词，shifting hypothesis被提出来，假定否定动作对情感值的影响是一个常量。 intensity words可以改变情感的强度。这对于fine-grained的情感分析是很有用的。 Models Long Short-Term Memory(LSTM) Bidirectional LSTM Linguistically Regularized LSTM当遇到情感词之后，句子的情感应该跟遇到之前有很大的不同。比如this movie is interesting，从左往右走this*，this movie*和this moive is*应该是几乎一样的，而this movie is very interesting*应该和前面几个完全不同（*代表当前的位置）。因为interesting已经被看到了。 Non-Sentiment Regularizer如果两个相邻的词是non-opinion的词，那么两个情感分布应该是尽可能相近的。 Sentiment Regularizer如果词是情感词，那么情感分布应该和前后位置的情感分布不同 Negation Regularizer碰到否定词会让情感飘逸。 Intensity Regularizer遇到intensity让情感加深。 Modified Loss function$E(\\theta)=-\\sum_i y^i log p^i + \\alpha \\sum_i \\sum_j L_t^i + \\beta ||\\theta||^2$ 在Sentiment Regularizer中，所有的同一类型的情感词共用drifting distribution。这里也可以根据大规模的训练集来学习不同的词的情感漂移。","raw":null,"content":null,"categories":[{"name":"Papers","slug":"Papers","permalink":"http://yezhejack.github.io/categories/Papers/"}],"tags":[{"name":"Papers","slug":"Papers","permalink":"http://yezhejack.github.io/tags/Papers/"},{"name":"Word Embedding","slug":"Word-Embedding","permalink":"http://yezhejack.github.io/tags/Word-Embedding/"}]},{"title":"Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings","slug":"Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings","date":"2017-04-23T12:21:00.000Z","updated":"2017-04-24T13:02:05.000Z","comments":true,"path":"2017/04/23/Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings/","link":"","permalink":"http://yezhejack.github.io/2017/04/23/Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings/","excerpt":"","keywords":null,"text":"Introduction这篇论文主要是要解决英文短语的词向量表示问题，英语的词向量可以分成两种，一种是compositional，另一种是non-compositional。前者的短语语义就是组成单词的叠加，而后者则会产生完全不同的意思。本文引入了一个score function来度量一个短语的compositional的程度。 完全依赖non-compositional embedding会产生数据稀疏的问题。而使用compositional embedding的问题更明显，因为有些短语本质上就是一个non-compositional embedding。例如bear fruits表示的是to yield results，这很难通过单词本身推断出来。 因此应该要将两种embedding结合起来。现在主要的学习方法有两种，一种就是都当作compositional的，另一种是两种都学习，然后选一个更好的。 MethodScore function $\\alpha(p)=\\sigma(\\mathbf{W} \\dot \\phi (p))$ compositional embedding $\\mathbf{c}(p)$ non-compositional embedding $\\mathbf{n}(p)$ 然后用下面的公式来得到我们要的最终的embedding。 $\\mathbf{v}(p)=\\alpha(p)\\mathbf{c}(p)+(1-\\alpha(p))\\mathbf{n}(p)$ 当训练数据接近无限大的时候，$\\alpha(p)$会无限接近0，并且non-compositional embedding会占主导地位，因为数据无限大就不存在数据稀疏问题了，而此时non-compositional embedding能更好地拟合数据。但是现实中，数据都是有限的，因此compositional embedding会减轻有限数据带来的问题。","raw":null,"content":null,"categories":[{"name":"Papers","slug":"Papers","permalink":"http://yezhejack.github.io/categories/Papers/"}],"tags":[{"name":"Papers","slug":"Papers","permalink":"http://yezhejack.github.io/tags/Papers/"},{"name":"Word Embedding","slug":"Word-Embedding","permalink":"http://yezhejack.github.io/tags/Word-Embedding/"}]},{"title":"Dont Count Predict An Automatic Approach to Learning Sentiment Lexicons for Short Text","slug":"Dont Count Predict An Automatic Approach to Learning Sentiment Lexicons for Short Text","date":"2017-04-21T11:31:00.000Z","updated":"2017-04-21T13:03:27.000Z","comments":true,"path":"2017/04/21/Dont Count Predict An Automatic Approach to Learning Sentiment Lexicons for Short Text/","link":"","permalink":"http://yezhejack.github.io/2017/04/21/Dont Count Predict An Automatic Approach to Learning Sentiment Lexicons for Short Text/","excerpt":"","keywords":null,"text":"Abstract这是一片发表在ACL2016上的短文，看名字就知道是关于如何自动化构建情感词表的文章。使用了一个神经网络的方法，并且不使用任何人工标注的资源。这个方法的灵感来自于NRC方法，NRC方法在SemEval13中利用了大量推特中的emoticons来获得了最好的结果，这个文章中的方法使用了词和推特情感的互信息来定义词的情感属性。 Introduction在所有的自动化方法中，Mohammad在2013年提出了使用推特的情感符号或者hashtags来作为训练数据。这个方法的主要优点在于这样的训练数据的数量是很巨大的，并且可以避免人工标注。尽管这样会引入噪声，但是很多研究表明这个方法是有效的。Mohammad在2013计算了词和表情符号之间的互信息，然后使用的词表获得了SemEval2013的最好结果。这篇文章中，他们将展示可以通过直接优化预测准确率来获得更好的情感词表，这个过程中将情感词表作为输入，表情作为输出。这个方法和Mohammad的方法的区别在于一个是predicting，一个是counting。 Related work自动学习情感词表主要有三种方法 将现有的词表用情感信息扩展。这种方法严重依赖现有词表，并且有语言限制。 第二种方法是扩展现有的人工标注的词表。例如2014年Tang应用了神经网络来学习面对情感的词向量，所用的数据是少量的标注好的推特，然后将种子情感词集通过测量词之间的距离来扩展变大。Bravo-Marquez在2015年通过对词进行分类，来扩展一个已有的词表，使用了人工抽取的特征。这些方法也都受限于人工资源的领域和语言。 第三种方法则是通过累计大数据中的统计特征来从头构建词表。Turney在2002年提出了通过计算种子词和搜索点击之间的PMI来估计词的情感极性。2013年Mohammad就使用了distance-supervised的数据来代替种子词来改进这个方法。这个方法摆脱了语言和人工标注资源的限制，作为本文的baseline。 BaselineMohammad将推特的表情和相关hashtags作为推特的情感标签。而情感分数则如下所示 $SS(w)=\\mathbf{PMI}(w,pos)-\\mathbf{PMI}(w,neg)$ $\\mathbf{PMI}(w,pos)=log_2 \\frac{freq(w,pos) \\times N}{freq(w) \\times freq(pos)}$ Model每个词都转换成$w=(n,p)$。其中$n$表示的是正面性，而$p$则表示负面性。一个推特可以表示为 $tw=w_1,w_2,...,w_n$然后用一个简单的神经网络来预测这个推特是正面还是负面。主要计算过程是$h=\\sum_i(w_i)$$y=softmax(hW)$ 每个token的初始值都是在[-0.25,0.25]，然后通过有监督的学习方式来学习每个token的值。优化方法用的是stochasti gradient descent。更新规则使用的是AdaDelta。所有的模型都设定了batch size为50，运行5个epoch。 Sentiment Classification训练好了之后，这些信息可以用于无监督和有监督的情感分类。在无监督的情感分类中，可以使用$p-n$作为词的情感分数，然后把一个文档中的所有词的情感分数都加起来，如果大于0这个文档就可以预测称正面。 有监督的方法则将情感词汇表当作特征。给定一个文档抽取如下的特征 情感词的数量，也就是在情感词汇表中情感分数不为0的。 文档的累计情感分数 最大的单词情感分数 正面情感词的总分数，负面情感词的总分数 最后一个词的情感分数 Experiments在除掉中性情感的数据后，本文只做对正负情感判断的分类器，结果表明了本文自动生成的情感词汇表比通过counting生成的NRC情感词汇表效果要好得多。 将Hu and Liu(2004)的手工标注的情感词表作为gold standard，计算本文的方法生成的情感词汇表的准确率为78.2%，而Mohammad(2013)方法生成的词表的准确率为76.9%。","raw":null,"content":null,"categories":[{"name":"Papers","slug":"Papers","permalink":"http://yezhejack.github.io/categories/Papers/"}],"tags":[{"name":"Papers","slug":"Papers","permalink":"http://yezhejack.github.io/tags/Papers/"},{"name":"Sentiment Analysis","slug":"Sentiment-Analysis","permalink":"http://yezhejack.github.io/tags/Sentiment-Analysis/"}]},{"title":"MXNet入门","slug":"MXNet入门","date":"2017-04-14T05:03:33.000Z","updated":"2017-04-29T08:13:43.000Z","comments":true,"path":"2017/04/14/MXNet入门/","link":"","permalink":"http://yezhejack.github.io/2017/04/14/MXNet入门/","excerpt":"","keywords":null,"text":"The basic创建矩阵123import mxnet as mxa=mx.nd.array([1,2,3])b=mx.nd.array([1,2,3],[2,3,4]) 或者也可以通过numpy.ndarray来创建123456import numpy as npimport mathc = np.arange(15).reshape(3,5)# create a 2-dimensional array from a numpy.ndarray objecta = mx.nd.array(c)&#123;'a.shape':a.shape&#125; 可以通过dtype来指定元素类型1234567# float32 is used in deafulta = mx.nd.array([1,2,3])# create an int32 arrayb = mx.nd.array([1,2,3], dtype=np.int32)# create a 16-bit float arrayc = mx.nd.array([1.2, 2.3], dtype=np.float16)(a.dtype, b.dtype, c.dtype) 同时还可以初始化矩阵而不需要给每一个元素指定数值，其中后面两种需要特别注意1234567a=mx.nd.zeros((2,3))b=mx.nd.ones((2,3))# create a same shape array with all elements set to 7c = mx.nd.full((2,3), 7)# create a same shape whose initial content is random and # depends on the state of the memoryd = mx.nd.empty((2,3)) 基础操作算数运算都是元素和元素之间的（elementwise）的，同时一个新的array会被创建并用来存储这个结果。123456789101112a = mx.nd.ones((2,3))b = mx.nd.ones((2,3))# elementwise plusc = a + b# elementwise minusd = - c # elementwise pow and sin, and then transposee = mx.nd.sin(c**2).T# elementwise maxf = mx.nd.maximum(a, c) # convert to print formatf.asnumpy() 矩阵的乘法运算和numpy一样，是使用dot来实现的。123a = mx.nd.ones((2,2))b = a * ac = mx.nd.dot(a,a) +=和*=操作会直接修改现有的array而不是去生成一个新的。1234a = mx.nd.ones((2,2))b = mx.nd.ones(a.shape)b += ab.asnumpy() 索引和切片其中将a的第二行全部设置为了1。123a = mx.nd.array(np.arange(6).reshape(3,2))a[1:2] = 1a[:].asnumpy() 也可以沿着特定的维度进行切片。沿着第二个维度进行切片，也就是将第二个维度中满足(begin,end]的维度切出来。12d = mx.nd.slice_axis(a, axis=1, begin=1, end=2)d.asnumpy() 形状操作将array修改成另一个大小相同的array。123a = mx.nd.array(np.arange(24))b = a.reshape((2,3,4))b.asnumpy() 也可将多个array进行拼接，可以使用concatenate方法，在不给额外参数的情况下是沿着第0维拼接的。可以通过增加axis=1来指定沿其他维度进行拼接，这时候要求这几个array除了被axis指定的维度的大小不一样以外，其他维度都要求是相同的。1234a = mx.nd.ones((2,3))b = mx.nd.ones((2,3))*2c = mx.nd.concatenate([a,b])c.asnumpy() 归约沿着第一维求和，最后会让第一维消失12a=mx.nd.ones((2,3))b=mx.nd.sum(a,axis=1) Broadcast这个操作会自动补齐维度，通过复制的方式123a = mx.nd.array(np.arange(6).reshape(6,1))b = a.broadcast_to((6,2)) # b.asnumpy() 123c = a.reshape((2,1,1,3))d = c.broadcast_to((2,2,2,3))d.asnumpy() *和+会自动使用boardcast让两个矩阵的形状相同 数据拷贝普通的赋值操作并不会产生新的数据，下面的b实际上还是a本身，类似指针。123a = mx.nd.ones((2,2))b = a b is a 1True 下面的操作同样也没有产生新的数据123def f(x): return xa is f(a) 显示1True 可以使用copy操作来进行真正的拷贝12b = a.copy()b is a 利用GPU进行运算可以加上mx.gpu(0)或者mx.gpu()来切换到GPU来进行计算12a = mx.nd.ones((100, 100), mx.gpu(0))a 则显示1&lt;NDArray 100x100 @gpu(0)&gt; 现在的MXNet要求两个矩阵要在同一个设备上才能进行计算，可以将一个矩阵拷贝到GPU上1234567a = mx.nd.ones((100,100), mx.cpu())b = mx.nd.ones((100,100), mx.gpu())c = mx.nd.ones((100,100), mx.gpu())a.copyto(c) # copy from CPU to GPUd = b + ce = b.as_in_context(c.context) + c # same to above&#123;'d':d, 'e':e&#125; 将数据写到磁盘以及从磁盘中读取数据使用pickle123456789import pickle as pkla = mx.nd.ones((2, 3))# pack and then dump into diskdata = pkl.dumps(a)pkl.dump(data, open('tmp.pickle', 'wb'))# load from disk and then unpack data = pkl.load(open('tmp.pickle', 'rb'))b = pkl.loads(data)b.asnumpy() 使用mxnet自带的功能可以dump一个list12345a = mx.nd.ones((2,3))b = mx.nd.ones((5,6))mx.nd.save(\"temp.ndarray\", [a,b])c = mx.nd.load(\"temp.ndarray\")c NDArray和Numpy的区别 NDArray每次只能对一个维度做切片，像x[:,1]这样同时对两个维度做切片是做不到的 切片只能是连续的，不能使用x[1:2:3]这样的 布尔索引是不被支持的 缺乏像max和min这样的reduce函数 Symbol创建三个symbol，并将其命名为a,b。可以使用mx.sym和mx.symbol,两者是一样的东西12345import mxnet as mxa = mx.sym.Variable('a')b = mx.sym.Variable('b')c = a + b(a, b, c) 大部分的NDArray的操作可以直接作用于Symbol。123456789# elemental wise timesd = a * b# matrix multiplicatione = mx.sym.dot(a, b) # reshapef = mx.sym.Reshape(d+e, shape=(1,4))# broadcastg = mx.sym.broadcast_to(f, shape=(2,4))mx.viz.plot_network(symbol=g) 可以将网络可视化输出为pdf只需要调用，但是似乎会报错。尽管会报错，但是输出的也是正常的。1mx.viz.plot_network(symbol=g).view() Symbol和NDArray的关系 Symbol可以提供几乎所有的NDArray的功能。 提供大量的神经网络相关操作，比如卷积，激活和BatchNorm。 提供自动求导的功能 便于构建和操作复杂的计算，例如深度神经网络 便于存储、加载和可视化 便于后端优化计算和内存使用Symbol Save和Load1234print(c.tojson())c.save('symbol-c.json')c2 = mx.symbol.load('symbol-c.json')c.tojson() == c2.tojson() 建立自己的操作123456789101112class Softmax(mx.operator.CustomOp): def forward(self, is_train, req, in_data, out_data, aux): x = in_data[0].asnumpy() y = np.exp(x - x.max(axis=1).reshape((x.shape[0], 1))) y /= y.sum(axis=1).reshape((x.shape[0], 1)) self.assign(out_data[0], req[0], mx.nd.array(y)) def backward(self, req, out_grad, in_data, out_data, in_grad, aux): l = in_data[1].asnumpy().ravel().astype(np.int) y = out_data[0].asnumpy() y[np.arange(l.shape[0]), l] -= 1.0 self.assign(in_grad[0], req[0], mx.nd.array(y)) 这里用asnumpy来将NDArray转变成numpy.ndarray，最后再用CustomOp.assign来将结果写回mxnet.NDArray。12345678910111213141516171819202122# register this operator into MXNet by name \"softmax\"@mx.operator.register(\"softmax\")class SoftmaxProp(mx.operator.CustomOpProp): def __init__(self): # softmax is a loss layer so we don’t need gradient input # from layers above. super(SoftmaxProp, self).__init__(need_top_grad=False) def list_arguments(self): return ['data', 'label'] def list_outputs(self): return ['output'] def infer_shape(self, in_shape): data_shape = in_shape[0] label_shape = (in_shape[0][0],) output_shape = in_shape[0] return [data_shape, label_shape], [output_shape], [] def create_operator(self, ctx, shapes, dtypes): return Softmax() 最后能够使用mx.sym.Custom和register name来使用这个operator1net = mx.symbol.Custom(data=prev_input, op_type='softmax') 但是这个方法定义的operator不能放在GPU上运行。 Advanced UsagesMXNet默认使用32-bit float。有时候我们想要使用低精度的数据类型来获得更好的accuracy-performance trade-off。例如Nvidia Tesla Pascal GPUs已经改进了16-bit float的性能，并且GTX Pascal GPUs(e.g.GTX1080)在8-bit intergers上是很快的。 我们可以使用mx.sym.Cast操作来转换数据类型12345678a = mx.sym.Variable('data')b = mx.sym.Cast(data=a, dtype='float16')arg, out, _ = b.infer_type(data='float32')print(&#123;'input':arg, 'output':out&#125;)c = mx.sym.Cast(data=a, dtype='uint8')arg, out, _ = c.infer_type(data='int32')print(&#123;'input':arg, 'output':out&#125;) Variable Sharing123456789a = mx.sym.Variable('a')b = mx.sym.Variable('b')c = mx.sym.Variable('c')d = a + b * cdata = mx.nd.ones((2,3))*2ex = d.bind(ctx=mx.cpu(), args=&#123;'a':data, 'b':data, 'c':data&#125;)ex.forward()ex.outputs[0].asnumpy() Training and Inference Module在这个tutorial中，我们将使用一个简单的多层感知机和一个合成的数据集123456789101112import mxnet as mxfrom data_iter import SyntheticData# mlpnet = mx.sym.Variable('data')net = mx.sym.FullyConnected(net, name='fc1', num_hidden=64)net = mx.sym.Activation(net, name='relu1', act_type=\"relu\")net = mx.sym.FullyConnected(net, name='fc2', num_hidden=10)net = mx.sym.SoftmaxOutput(net, name='softmax')# synthetic 10 classes dataset with 128 dimension data = SyntheticData(10, 128)mx.viz.plot_network(net) Create Module最广泛使用的模块是Module，这个包含了Symbol和一个或多个Executor。我们构建一个module通过 symbol网络的符号 context执行的设备 data_names数据变量名字的list label_names1234mod = mx.mod.Module(symbol=net, context=mx.cpu(), data_names=['data'], label_names=['softmax_label']) Train, Predict and Evaluate前面我们获得了一个mod，然后这个模块提供了一个fit函数，可以看到其中需要一个数据迭代器来传送训练数据和标准数据。1234567891011# @@@ AUTOTEST_OUTPUT_IGNORED_CELLimport logginglogging.basicConfig(level=logging.INFO)batch_size=32mod.fit(data.get_iter(batch_size), eval_data=data.get_iter(batch_size), optimizer='sgd', optimizer_params=&#123;'learning_rate':0.1&#125;, eval_metric='acc', num_epoch=5) 当整个模型训练好了之后，可以使用predict来预测新的数据，输入的是一个数据迭代器，虽然每次只是输入一个batch，但是它会收集并返回所有的预测结果。1y = mod.predict(data.get_iter(batch_size)) 考虑到如果内存是不够的，那么也可以每次只预测一个batch1234for preds, i_batch, batch in mod.iter_predict(data.get_iter(batch_size)): pred_label = preds[0].asnumpy().argmax(axis=1) label = batch.label[0].asnumpy().astype('int32') print('batch %d, accuracy %f' % (i_batch, float(sum(pred_label==label))/len(label))) 如果是用来在测试集上做评价的话，可以使用score函数来得出结果1mod.score(data.get_iter(batch_size), ['mse', 'acc']) 这里给了MSE和ACC两种度量方式，那么返回的也会是两种1[('mse', 27.438781929016113), ('accuracy', 0.115625)] Save and LoadSave我们可以在每一个epoch结束之后保存模型的参数，这个里的方式就是使用一个回调函数。1234567# @@@ AUTOTEST_OUTPUT_IGNORED_CELL# construct a callback function to save checkpointsmodel_prefix = 'mx_mlp'checkpoint = mx.callback.do_checkpoint(model_prefix)mod = mx.mod.Module(symbol=net)mod.fit(data.get_iter(batch_size), num_epoch=5, epoch_end_callback=checkpoint) Load从磁盘中读取上一次的网络状态，然后将它设置到模型中12345sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, 3)print(sym.tojson() == net.tojson())# assign the loaded parameters to the modulemod.set_params(arg_params, aux_params) 也可以直接直接开始训练1234567# @@@ AUTOTEST_OUTPUT_IGNORED_CELLmod = mx.mod.Module(symbol=sym)mod.fit(data.get_iter(batch_size), num_epoch=5, arg_params=arg_params, aux_params=aux_params, begin_epoch=3) Module as a computation “machine”一个模块有多个状态 Initial state内存还未分配，还没有准备好计算 Binded输入、输出和参数的形状都已经知道了，内存也已经分配给它了，已经准备好了计算 Parameter initialized一个模块如果没有对参数初始化就进行计算的话，会出现没定义好的输出 Optimizer installed指定了网络的优化方式，只有这样才能对权值进行更新。123456789101112131415161718192021222324252627# @@@ AUTOTEST_OUTPUT_IGNORED_CELL# initial statemod = mx.mod.Module(symbol=net)# bind, tell the module the data and label shapes, so# that memory could be allocated on the devices for computationtrain_iter = data.get_iter(batch_size)mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)# init parametersmod.init_params(initializer=mx.init.Xavier(magnitude=2.))# init optimizermod.init_optimizer(optimizer='sgd', optimizer_params=(('learning_rate', 0.1), ))# use accuracy as the metricmetric = mx.metric.create('acc')# train one epoch, i.e. going over the data iter one passfor batch in train_iter: mod.forward(batch, is_train=True) # compute predictions mod.update_metric(metric, batch.label) # accumulate prediction accuracy mod.backward() # compute gradients mod.update() # update parameters using SGD # training accuracyprint(metric.get()) Loading DataBasic Data Iterator这是一个数据迭代器，类似python中的迭代器。12345class SimpleBatch(object): def __init__(self, data, label, pad=0): self.data = data self.label = label self.pad = pad 一个batch的形状应该是batch_size x num_chaneel x height x width Symbol and Data Variables新建一个数据迭代器1234567891011121314151617181920212223242526272829303132333435363738import numpy as npclass SimpleIter: def __init__(self, data_names, data_shapes, data_gen, label_names, label_shapes, label_gen, num_batches=10): self._provide_data = zip(data_names, data_shapes) self._provide_label = zip(label_names, label_shapes) self.num_batches = num_batches self.data_gen = data_gen self.label_gen = label_gen self.cur_batch = 0 def __iter__(self): return self def reset(self): self.cur_batch = 0 def __next__(self): return self.next() @property def provide_data(self): return self._provide_data @property def provide_label(self): return self._provide_label def next(self): if self.cur_batch &lt; self.num_batches: self.cur_batch += 1 data = [mx.nd.array(g(d[1])) for d,g in zip(self._provide_data, self.data_gen)] assert len(data) &gt; 0, \"Empty batch data.\" label = [mx.nd.array(g(d[1])) for d,g in zip(self._provide_label, self.label_gen)] assert len(label) &gt; 0, \"Empty batch label.\" return SimpleBatch(data, label) else: raise StopIteration mxnet.symbol.SoftmaxOutput在前向传播中，这个函数会返回softmax的输出，而在后向传播的时候，这个函数会加上logit loss。 用numpy为mx.nd.array赋值12a=mx.nd.ones((2,3),ctx=mx.gpu())a[:]=np.random.rand(*a.shape) 不能使用1a=np.random.rand(*a.shape)","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"MXNet","slug":"MXNet","permalink":"http://yezhejack.github.io/tags/MXNet/"}]},{"title":"tensorflow Core Dump原因和解决方法","slug":"tensorflow Core Dump原因和解决方法","date":"2017-04-12T16:52:55.000Z","updated":"2017-04-12T17:25:33.000Z","comments":true,"path":"2017/04/13/tensorflow Core Dump原因和解决方法/","link":"","permalink":"http://yezhejack.github.io/2017/04/13/tensorflow Core Dump原因和解决方法/","excerpt":"","keywords":null,"text":"在ubuntu 14.04中安装tensorflow后出现类似core dump，很有可能是因为通过pip安装的numpy同ubuntu 14.04有冲突,而如果是在ubuntu 16.04下安装的话则没有问题。解决方法可以是删除掉numpy，然后通过源码编译安装。下面给出一种相当于源码编译安装的方法 pip install --no-binary=:all: numpy 解决方案来自https://github.com/tensorflow/tensorflow/issues/6968","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yezhejack.github.io/tags/Linux/"},{"name":"服务器","slug":"服务器","permalink":"http://yezhejack.github.io/tags/服务器/"}]},{"title":"latex使用教程","slug":"latex使用教程","date":"2017-04-03T09:36:59.000Z","updated":"2017-04-10T09:35:51.000Z","comments":true,"path":"2017/04/03/latex使用教程/","link":"","permalink":"http://yezhejack.github.io/2017/04/03/latex使用教程/","excerpt":"","keywords":null,"text":"取消表格浮动latex会根据文字排版来调整图片或者表格的位置，但是有些作业的排版需要取消这个功能。可以使用 123\\usepackage&#123;float&#125;\\begin&#123;table&#125;[H]\\end&#123;table&#125; 这样便可取消浮动，按照原始顺序排版。 写表格1234567891011121314\\usepackage&#123;float&#125;\\begin&#123;table&#125;[H] \\centering \\begin&#123;tabular&#125;[t]&#123;*&#123;3&#125;&#123;|c&#125;|&#125; \\hline Method &amp; Linear kernel &amp; RBF kernel \\\\ \\hline one-versus-one &amp; 55.0198\\% &amp; 40.0264\\% \\\\ one-versus-rest &amp; \\textbf&#123;57.3316\\%&#125; &amp; \\textbf&#123;56.7371\\%&#125; \\\\ part-versus-part &amp; 53.1044\\% &amp; 45.7728\\% \\\\ \\hline \\end&#123;tabular&#125; \\caption&#123;Accuracy&#125;\\end&#123;table&#125; 公式下标位置在equation模式下，有的时候我们希望下标的位置能在正下方，可以使用\\limits来改变下标的位置 1\\max \\limits_&#123;x&#125; 如果是自定义的符号想要添加下表的话，可以先将这个符号声明为一个数学符号1\\DeclareMathOperator*&#123;\\argmax&#125;&#123;argmax&#125; 然后就可以使用1\\argmax \\limits_&#123;x&#125; 取消公式的标号可以在后面加上\\nonumber来跳过当前公式的标号。123\\begin&#123;equation&#125;l(f(x),y)=\\mathbf&#123;1&#125;\\&#123;f(x) \\neq y\\&#125;\\nonumber\\end&#123;equation&#125; 带有左括号的方程组12345678910\\begin&#123;equation&#125;\\begin&#123;split&#125;f(x)=\\left\\&#123;\\begin&#123;aligned&#125;1 &amp;\\qquad \\beta P(y=1|x) \\geq \\alpha P(y=0|x) \\\\0 &amp;\\qquad \\alpha P(y=0|x) &gt; \\beta P(y=1|x) \\\\\\end&#123;aligned&#125;\\right.\\nonumber\\end&#123;split&#125;\\end&#123;equation&#125; 插入图片123\\begin&#123;center&#125;\\includegraphics[width=0.8\\textwidth]&#123;figure1&#125;\\end&#123;center&#125;","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"latex","slug":"latex","permalink":"http://yezhejack.github.io/tags/latex/"}]},{"title":"Evaluation of Word Vector Representations by Subspace Alignment","slug":"Evaluation of Word Vector Representations by Subspace Alignment","date":"2017-03-27T11:54:00.000Z","updated":"2017-03-27T14:09:31.000Z","comments":true,"path":"2017/03/27/Evaluation of Word Vector Representations by Subspace Alignment/","link":"","permalink":"http://yezhejack.github.io/2017/03/27/Evaluation of Word Vector Representations by Subspace Alignment/","excerpt":"","keywords":null,"text":"Abstract无监督学习的词向量的评价通常与下游应用没有很大的关联，本文将提出QVEC的评价方法。 Introduction缺乏标准化的对比方式是因为词向量的每个维度依然是无法解释的，如何去给一个无法解释的表示打分依然是不明确的。 本文通过将distribution word vector和人工标注的word vector对其，然后计算每一维的相关度，相加之后就得到了distribution word vector的分数。 人工标注的word vector每一维都是可以解释的，比如fish的第一维是作为NN.ANIMAL的分数。","raw":null,"content":null,"categories":[{"name":"Papers","slug":"Papers","permalink":"http://yezhejack.github.io/categories/Papers/"}],"tags":[{"name":"Papers","slug":"Papers","permalink":"http://yezhejack.github.io/tags/Papers/"},{"name":"Word Embedding","slug":"Word-Embedding","permalink":"http://yezhejack.github.io/tags/Word-Embedding/"}]},{"title":"LSTM","slug":"LSTM","date":"2017-03-21T03:02:00.000Z","updated":"2017-03-21T13:20:58.000Z","comments":true,"path":"2017/03/21/LSTM/","link":"","permalink":"http://yezhejack.github.io/2017/03/21/LSTM/","excerpt":"","keywords":null,"text":"参考资料DeepLearning Bengio Chapter 10 Sequence Modeling: Recurrent and Recursive Nets从multi-layer networks到recurrent network需要用到早期在机器学习中发现的想法：共享参数。共享参数使得网络可以运用到多种形式的数据上，并且在它们之间产生泛化。卷积网络也是一个共享参数的网络，只是它的共享方式是通过共享input中相邻的输入，而RNN则是与之前的输入共享参数。 Unfolding Computational Graphs下面的公式是简化版的RNN公式，这也是一次unfold操作。 $s^{(t)}=f(s^{(t-1)};\\theta)$当系统收到外部信号$x^{(t)}$ 驱动时，式子就变成了$s^{(t)}=f(s^{(t-1)},x^{(t)};\\theta)$ 许多的RNN用类似的公式来描述自己的隐含层单元 $h^{(t)}=f(h^{(t-1)},x^{(t)};\\theta)$ 而输出层就是靠着读取隐含层来作出预测的。 Recurrent Neural Networks 前向过程是 $a^{(t)}=b+Wh^{(t-1)}+Ux^{(t)}$ 那些有从输出层到隐含层的模型是可以使用teacher forcing，也就是在训练过程中不是把前一时间的网络输出输入到后一时刻的网络输入中，而是将ground truth y输入到后一层中。 训练RNN的一个方法是BPTT，其实就是Back Propagation Through Time。只要把这些网络按照时间展开，就可以看作是一个前馈网络。 Bidirectional RNNs双向RNN网络可以使用未来的信息来帮助确定当前的状态。双向RNN网络实际上是结合了两个网络，一个是从序列头开始的，另一个是从序列尾部开始的。 Encoder-Decoder Sequence-to-Sequence Architectures这个网络是用于将一个序列转换成另一个序列。 通常RNN将输入称作context。 2014年才提出了可用于可变长的序列转换的RNN网络。 一个RNN叫做encoder，它产生context，另一个RNN叫做decoder，它消费context。其中的context是固定长度的。 2015年的时候，有人提出了可变context长度的RNN网络。 Deep Recurrent Networks有三种变得deeper的方式。 Recursive Neural NetworksRNN一般都值的是recurrent neural network。 Challenge of long-term dependencies学习这样的网络的时候，传回去的梯度要没会消失要么会变得过大。","raw":null,"content":null,"categories":[{"name":"Papers","slug":"Papers","permalink":"http://yezhejack.github.io/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yezhejack.github.io/tags/Deep-Learning/"}]},{"title":"matplotlib教程","slug":"matplotlib教程","date":"2017-03-16T08:11:32.000Z","updated":"2017-03-16T10:16:51.000Z","comments":true,"path":"2017/03/16/matplotlib教程/","link":"","permalink":"http://yezhejack.github.io/2017/03/16/matplotlib教程/","excerpt":"","keywords":null,"text":"散点图我有两组数据，需要不同颜色的点将他们表示在一个二维空间中。12345678910111213#plot trainning curvef1=plt.figure(1)#plot real training datap1=plt.scatter(x_train[:,0]*x_max,y_train*y_max,marker='.',color='b',label='real price')#plot gradient decent training curvey_pre=np.dot(x_train,w_gd)*y_maxp2=plt.scatter(x_train[:,0]*x_max,y_pre,marker='x',color='r',label='gradient method predict price')#plot normal equation training curvey_pre=np.dot(x_train,w_eq)*y_maxp3=plt.scatter(x_train[:,0]*x_max,y_pre,marker='x',color='g',label='normal equation predict price')plt.legend()plt.show()","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yezhejack.github.io/tags/Machine-Learning/"}]},{"title":"Hadoop IO","slug":"Hadoop IO","date":"2017-03-15T05:41:28.000Z","updated":"2017-03-15T05:56:48.000Z","comments":true,"path":"2017/03/15/Hadoop IO/","link":"","permalink":"http://yezhejack.github.io/2017/03/15/Hadoop IO/","excerpt":"","keywords":null,"text":"Hadoop IOhadoop提供了多种的解压缩方式，但是由于license的问题，得单独下载。 hadoop同时也对文件读写有校验，一旦出现文件错误，就会报告给namenode，namenode则不会再给别的任务派送这个文件块，同时调度其他的replica来恢复。 每个datanode会定期地检查自己的数据。 使用本地的库来进行压缩或解压可以节省很多时间，相对于通过Java实现的来说。几乎所有的压缩方式都有本地的实现，而有的则没有Java的版本。 Apache Hadoop打包好的二进制的hadoop版本，包含了一个libhadoop.so的压缩包。这个版本只为64-bit Linux。其他的平台则要自己编译一个。默认的，Hadoop会寻找本地版本的库来使用，因此不需要去改变配置来使用本地的库。而有的时候你想要关闭本地的库，则需要将io.native.lib.available设置为false。 如果使用了很多的压缩和解压缩的话，可以使用CodecPool来复用压缩器和解压缩器。 压缩与输入切分一个1GB的未压缩的文件存储在HDFS中，会被存储在8个block中，一个使用这个文件的MR任务会使用八个任务来处理这些输入。而如果是gzip压缩的话，无法从任意一个文件偏移开始读取gzip流，因此无法分成8个来分开处理。只能用一个map来串行地处理这个8个输入。MR会根据扩展名来判断，不用担心。这就牺牲了本地化。同时粒度也小了，就需要更长的时间去运行了。 具体什么的压缩格式比较合适，应该选用支持切分的。 通过设置mapreduce.output.fileoutputformat.compress为true，设置mapreduce.output.fileoutputformat.compress.codec为需要使用的压缩codec。 因为map的输出结果需要很大的网络开销，所以可以使用一些能够快速压缩的格式，来压缩map的输出 序列化序列化石把结构化的数据转换为一个字节流，这样可以在网络中传输，或者用于持久化存储。在分布式系统中主要用于进程间的通讯和持久化存储。 需要几个特点，一个是compact，这样可以充分利用带宽资源，另一个是快速，还有可扩展性，最后还有一个就是可内部操作。Hadoop使用了自己特有的序列化格式，只能在Java中使用，而Avro也是一个序列化的系统，用来克服专有格式带来的一些限制。","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yezhejack.github.io/tags/hadoop/"},{"name":"big data","slug":"big-data","permalink":"http://yezhejack.github.io/tags/big-data/"}]},{"title":"Vector-space topic models for detecting Alzheimers's disease","slug":"Vector-space topic models for detecting Alzheimers disease","date":"2017-03-06T12:16:00.000Z","updated":"2017-03-20T14:03:30.000Z","comments":true,"path":"2017/03/06/Vector-space topic models for detecting Alzheimers disease/","link":"","permalink":"http://yezhejack.github.io/2017/03/06/Vector-space topic models for detecting Alzheimers disease/","excerpt":"","keywords":null,"text":"Abstract本文主要是想检测老年痴呆症，因为语义的缺失是一大症状。获得了96.8%的召回率。通过训练随机森林的分类器获得了0.74的F值（二元分类）。并且仅用了12个特征。 IntroductionAD在病情发展的过程中会出现语言的转变，并且这是可以检测得到的。这些变化包括句法复杂度的下降、找词困难和语义内容缺失，信息密度低（有语义的词在所有词中的比例），效率低。 结合上lexicosyntactic和acoustic features会获得81.9%的准确率。之前的文章没有会自动生成ICUs（information content unit)。这篇文章将会自动生成。 在正常的诊断过程中，医生会给出一幅图片，让患者口头描述，而每一幅图片都有对应的hsICUs（human-supplied information content units），根据患者描述中覆盖的hsICUs的数量来给它打分。 MethodologyDataDementiaBank 自动生成ICUs用recall来衡量自动生成的ICUs的性能。 首先训练一般的word vector。这里使用的是GloVe v1.2模型训练，使用的数据是Wikipedia 2014+Gigaword 5。分词使用NLTK v3.1。 只保留了名词和动词。 将出现在CT数据集中的词的词向量用上下文扩充，同样的出现在痴呆症数据集中的也用相同的方法扩充。于是各形成了不同的分布。在这些分布上做k-means聚类，当k=10的时候实验结果最好，获得两个聚类模型Control cluster model和demential cluster model. hsICUs的召回率为了衡量自动生成的ICUs和人工标注的ICUs的匹配程度 为了衡量 Experiments计算hsICUs的召回率，文章中定义了标注的hsICU的距离分数（不是简单的欧几里得距离）。有些不同的hsICUs会被关联到同一个类中。在C和D中各生成了10个类，然后这之间要做对齐。 实验证明，健康人说的话题，患者都有提到，而又一个患者的话题是健康人所没有提及到的，因此可以看到两个群体在所说的话题上区别很小。 Local context weighted vectors$\\phi_w = v_w+\\sum_{i=-N}^{-1}\\alpha_i \\times v_i + \\sum_{i=1}^{N} \\alpha_i \\times v_i$ 对于两个数据集中的word vector需要进行扩充，使用的公式就是上面的这个公式，将前后若干个词按照距离权重加到中心词的general word vector上。从而形成新的word vector。作者用这个方法来说明词在两个数据集中的上下文是差不多的。于是在最后的方法中是没有讲上下文加入到词向量中的。 当使用扩展的特征集的时候，没有进行context扩展的词向量拥有更好的表现。 1In our data, we also found that speakers with and without Alzheimer’s dis- ease generally discuss the same topics and in the same contexts Classification最后的分类的特征有 到C0-C9的距离 到D0-D9的距离 idea密度 idea效率","raw":null,"content":null,"categories":[{"name":"Papers","slug":"Papers","permalink":"http://yezhejack.github.io/categories/Papers/"}],"tags":[{"name":"Papers","slug":"Papers","permalink":"http://yezhejack.github.io/tags/Papers/"},{"name":"Word Embedding","slug":"Word-Embedding","permalink":"http://yezhejack.github.io/tags/Word-Embedding/"}]},{"title":"Tensorflow 基本教程","slug":"tensorflow基本教程","date":"2017-02-01T11:52:17.000Z","updated":"2017-03-26T09:33:55.000Z","comments":true,"path":"2017/02/01/tensorflow基本教程/","link":"","permalink":"http://yezhejack.github.io/2017/02/01/tensorflow基本教程/","excerpt":"","keywords":null,"text":"Tensorflow 是什么Tensorflow是Google开源的一个深度学习库，可以建立计算的图模型和利用tensor结构在途中流动，这大概也是叫做tensorflow的主要原因。 Deep MNIST for Experts1tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) 用来生成的正态分布的随机数。 然后引用 tensorflow库并加载数据1234567#code:utf-8from tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tfimport argparseimport sys#yezheGPUOption=tf.GPUOptions(allow_growth=True) 其中的最后一行让程序不会一次性把GPU所有的内存全部占用掉。 启动构建好的tensorflow计算图1tf.app.run(main=main, argv=[sys.argv[0]] + unparsed) main函数先从tensorflow封装好的库中载入mnist的数据，然后建立两个placeholder来放置数据，1234```mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)x = tf.placeholder(tf.float32, [None, 784])y_ = tf.placeholder(tf.float32, [None, 10]) 然后我们要构建第一个卷积层，跟着卷积层的就是pool tf.nn.conv2dComputes a 2-D convolution given 4-D input and filter tensors. Flattens the filter to a 2-D matrix with shape [filter_height filter_width in_channels, output_channels]. tf.nn.max_pool(value, ksize, strides, padding, data_format=’NHWC’, name=None)执行pool操作，ksize表示滑动窗口的大小，是一个4d的窗口，而strides表示这个窗口在所有维度上的滑动步长。","raw":null,"content":null,"categories":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://yezhejack.github.io/categories/Tensorflow/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yezhejack.github.io/tags/Deep-Learning/"}]},{"title":"iptables","slug":"iptables","date":"2017-01-15T08:58:34.000Z","updated":"2017-01-15T08:58:34.000Z","comments":true,"path":"2017/01/15/iptables/","link":"","permalink":"http://yezhejack.github.io/2017/01/15/iptables/","excerpt":"","keywords":null,"text":"iptables 设置使用的方法将输出的包重定向1iptables -t nat -A OUTPUT -p tcp -d 192.168.1.107 --dport 5001 -j DNAT --to-destination 192.168.1.107:5000 这是把本来发往192.168.1.107:5001端口的包发往192.168.1.107:5000 这里是为了解决一个问题，就是一个NAS的NFS服务器需要一个111端口来进行挂载，但是似乎nfs没有提供可配置选项，如果我们在一个路由器后放上多个NAS的话怎么办，因此需要又一个办法让发出去的111端口重新定位到不同的端口上去，然后再将这个端口转发到111上。 开放特定端口12iptables -A INPUT -p tcp --dport 80 -j ACCEPT/etc/init.d/firewall reload","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yezhejack.github.io/tags/Linux/"},{"name":"iptables","slug":"iptables","permalink":"http://yezhejack.github.io/tags/iptables/"}]},{"title":"ffmpeg用于视频转换","slug":"ffmpeg用于视频转换","date":"2017-01-14T16:24:57.000Z","updated":"2017-01-15T03:06:55.000Z","comments":true,"path":"2017/01/15/ffmpeg用于视频转换/","link":"","permalink":"http://yezhejack.github.io/2017/01/15/ffmpeg用于视频转换/","excerpt":"","keywords":null,"text":"1ffmpeg -i input.mkv -vf \"ass=sub.ass\" -t 00:01:00 out.mp4 1ffmpeg -hwaccel cuvid -c:v h264_cuvid -i input -c:v h264_nvenc -preset slow output.mkv","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"ffmpeg","slug":"ffmpeg","permalink":"http://yezhejack.github.io/tags/ffmpeg/"},{"name":"视频转换","slug":"视频转换","permalink":"http://yezhejack.github.io/tags/视频转换/"}]},{"title":"SSH Reversal Tunnel 反向隧道","slug":"SSH Reversal Tunnel 反向隧道","date":"2016-12-23T07:13:00.000Z","updated":"2017-04-08T13:08:19.000Z","comments":true,"path":"2016/12/23/SSH Reversal Tunnel 反向隧道/","link":"","permalink":"http://yezhejack.github.io/2016/12/23/SSH Reversal Tunnel 反向隧道/","excerpt":"","keywords":null,"text":"使用ssh进行反向代理我在实验室的内网里有一个工作站，安装的Linux系统，具体版本是Ubuntu 16.04。此外，我还有一个aliyun的ECS主机，这样我就有了一个公网IP。现在需要做的就是让我们的22端口能够绑定到aliyun主机的2221端口上。22端口是默认的ssh登陆端口。 1ssh -v -N -R 2221:127.0.0.1:22 aliyun.ip 使用autossh进行反向代理上网看了一下，似乎很多人都是使用autossh来进行反向代理，给出的理由是autossh会自动重连，但是我并没有尝试过。这里给出后台运行的命令 1autossh -M 55551 -f -v -gNR *:2221:127.0.0.1:22 root@aliyun.ip &gt; /home/yezhe/log/autossh.log 2&gt;&amp;1 自动启动写/etc/init.d/脚本在ubuntu系统中，可以在/etc/init.d下编写一个脚本来让指定的程序来自动运行。将下面的内容存储到/etc/init.d目录下，命名为autossh，然后再为其增加一执行权限sudo chmod +x autossh。 1234567891011121314151617#!/bin/sh### BEGIN INIT INFO# Provides: autossh# Required-Start: $remote_fs# Required-Stop: $remote_fs# Default-Start: 2 3 4 5# Default-Stop: 0 1 6# Short-Description: Start or stop the autossh.### END INIT INFOcase \"$1\" in start) start-stop-daemon --start --exec /home/yezhe/Workspace/autossh.sh;; stop) start-stop-daemon --stop --name autosshesac 写执行shell脚本如果你仔细看这个代码，可以看到在后面有一个/home/yezhe/Workspace/autossh.sh脚本，这个脚本里面实际包含了真正的autossh命令。下面贴出里面的内容 123#!/bin/shautossh -M 55551 -f -v -gNR *:2221:127.0.0.1:22 root@hostip1 &gt; /home/yezhe/log/autossh.log 2&gt;&amp;1autossh -M 55552 -f -v -gNR *:2221:127.0.0.1:22 root@hostip2 &gt; /home/yezhe/log/autossh.log 2&gt;&amp;1 将其中的root、hostip1和hostip2都可以改成具体的情况，我这里是配置了可以通过公钥登录hostip1和hostip2。按照/etc/init.d/autossh中写的，我把这个命令存成autossh.sh，放到了/home/yezhe/Workspace/下。 同样的还需要给这个sh脚本增加可执行权限1sudo chmod +x autossh.sh 配置自动登录启动的时候机器执行上面的命令肯定使用root用户的，所以我就需要先切换到root用户来配置自动登录。 下面是一个比较奇怪的地方，以后有时间再去尝试一下。我一般是按照下面的步骤来配置root的自动登录的1234su rootssh-keygenssh-copy-id root@hostip1ssh-copy-id root@hostip2 这种状况对于使用像1ssh root@hostip1 来登录是可以的，但是对于在非root用户的情况下使用1sudo ssh root@hostip1 来登录是不可行的。因此这里采用下面的配置方式，不需要切换到root123sudo ssh-keygensudo ssh-copy-id root@hotsip1sudo ssh-copy-id root@hostip2 让/etc/init.d/autossh 开机启动1sudo update-rc.d autossh defaults 99 到此，可以使用sudo /etc/init.d/autossh start来启动命令，也可以使用sudo /etc/init.d/autossh stop来停止。","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"SSH","slug":"SSH","permalink":"http://yezhejack.github.io/tags/SSH/"}]},{"title":"Shrinkage Methods","slug":"Shrinkage Methods","date":"2016-11-23T11:27:00.000Z","updated":"2016-11-23T17:09:43.000Z","comments":true,"path":"2016/11/23/Shrinkage Methods/","link":"","permalink":"http://yezhejack.github.io/2016/11/23/Shrinkage Methods/","excerpt":"","keywords":null,"text":"Ridge Regression 岭回归式子3.41 $\\hat{\\beta^{ridge}}=argmin_{\\beta}\\{\\sum_1^N (y_i-\\beta_0-\\sum_{j=1}^p x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\\}$ 其中的$\\lambda$是用来控制shrinkage的量的。这种用参数的二次型来控制预测出来的参数的方法在神经网络中也有用到，比如weight decay。 式子3.41可以分成下面两个式子来写。 式子3.42 $\\hat{\\beta^{ridge}}=argmin_{\\beta} \\sum_1^N (y_i-\\beta_0-\\sum_{j=1}^p x_{ij}\\beta_j)^2）$ $subject to \\sum_{j=1}^{p} \\beta_{j}^{2} \\leq t$ 如果模型中有很多相关变量的话，系数的决定会很糟糕，并且会出现很高的方差。 之所以不对$\\beta_{0}$做惩罚，是因为如果对其做了惩罚，那么结果会依赖于$Y$的选择，因为$\\beta_{0}$模拟的是$Y$的bias。为了让ridge在scale之后可以保持不变，需要对input做归一化，让它的bias为0。比如对于$x_{ij}$我们可以用$x_{ij}-\\overline{x_{j}}$来代替。 对于$\\beta_0$我们用$\\overline{y}=frac{1}{N} \\sum_{1}^{N} y_i$来估计。 剩下的就用ridge继续估计 式子3.43 $RSS(\\lambda) = (y-X\\beta)^T (y-X\\beta) + \\lambda \\beta^T \\beta$ 于是它的解就变成了 式子3.44 $\\hat{\\beta^{ridge}} = (X^T X + \\lambda I)^{-1} X^T y$ $I$是$p \\times p$的对角线为1的单位矩阵。 从这个解我们可以看到，在求逆之前，会在$X^T X$的对角线元素上加上非零值，这样可以避免它变成奇异矩阵。","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yezhejack.github.io/tags/Machine-Learning/"}]},{"title":"A Neural Probabilistic Language Model","slug":"A Neural Probabilistic Language Model","date":"2016-11-18T08:20:00.000Z","updated":"2016-11-23T11:27:08.000Z","comments":true,"path":"2016/11/18/A Neural Probabilistic Language Model/","link":"","permalink":"http://yezhejack.github.io/2016/11/18/A Neural Probabilistic Language Model/","excerpt":"","keywords":null,"text":"Data-Parallel Processing每个处理任务处理一个不同的数据子集。计算这个子集上的gradient，然后对parameters执行SGD。但是浪费了大部分的时间在等待上。 他们选择了另一种的异步的实现方法，让所有的update操作任意执行，不加以控制，但是这个会对parameter的update过程引入一些噪声，但这个影响看起来不大。 Parameter-Parallel ProcessingExperiment ResultsBrown corpus vocabulary size V=16383Associated Press News from 1995 and 1996 V=17964 下面的toolkit有n-gram的方法，包括the stat-of-artwww.speech.sri.com/projects/srilm/ Future Work 分解网络 用树结构来表示条件概率 只从一个输出单词的子集中传播gradient 引入先验知识 解释word representation 多义词处理","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Word Vector","slug":"Word-Vector","permalink":"http://yezhejack.github.io/tags/Word-Vector/"},{"name":"Paper","slug":"Paper","permalink":"http://yezhejack.github.io/tags/Paper/"}]},{"title":"Efficient Estimation of Word Representations in Vector Space","slug":"Efficient Estimation of Word Representations in Vector Space","date":"2016-10-30T13:27:00.000Z","updated":"2016-10-30T13:40:30.000Z","comments":true,"path":"2016/10/30/Efficient Estimation of Word Representations in Vector Space/","link":"","permalink":"http://yezhejack.github.io/2016/10/30/Efficient Estimation of Word Representations in Vector Space/","excerpt":"","keywords":null,"text":"Abstract提出两个创新的模型架构，用于在非常大的数据集上计算连续型的词向量表示。这些表示的好坏的用词相似性任务来测量。计算复杂度下降了，同时在准确度上还有巨大的提升。在当时是state-of-the-art的做法。 1 Introduction许多现在的系统将词表示在词库中的索引，并不包含词的相似性的概念。这个选择的原因是：简单、鲁棒以及一个观察到的现象：在大量数据上训练出来的简单模型要比在少量数据上训练出来的复杂模型的表现要好。一个例子就是N-gram模型。 但是简单模型也有很多的局限性，比如在数据量不足的时候。随着这些年机器学习技术的发展，在更大的数据集上训练出复杂模型成为可能，并且它的效果要好于简单模型。最成功的概念就是使用词的分布式表示。例如基于语言模型的神经网络就显著地比N-gram模型好。 1.1 Goals of the Paper文章的主要目的就是介绍一个技术，这个技术可以用于学习高质量的word vector。数据集是包含了数十亿词的巨大的数据集。其中还包含了数百万的词汇量。 我们使用了最近提出的一项技术来测量向量表示的质量。不仅相似的词要互相接近，并且这些词还要有多种程度的相似性。 有点惊喜的是，词表示的相似性不只是简单的句法规则。比如vector(“King”)-vector(“Man”)+vector(“Woman”) 约等于vector(Queen) 在这个文章中，我们尝试去最大化这些向量操作的准确性，通过开发一种新的可以保留词之间的线形规则的模型架构。我们还构建了一个综合的测试集。 2 Model ArchitecturesLSA LDA也用于估计连续的词表示。本文的方法可以比LSA的方法保留更多的线形规则，而LDA在大数据集上的计算复杂性太高了。","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Word Vector","slug":"Word-Vector","permalink":"http://yezhejack.github.io/tags/Word-Vector/"},{"name":"Paper","slug":"Paper","permalink":"http://yezhejack.github.io/tags/Paper/"}]},{"title":"Chapter 14 Database System Concepts","slug":"Chapter 14 Database System Concepts","date":"2016-10-26T13:18:00.000Z","updated":"2016-10-27T14:03:28.000Z","comments":true,"path":"2016/10/26/Chapter 14 Database System Concepts/","link":"","permalink":"http://yezhejack.github.io/2016/10/26/Chapter 14 Database System Concepts/","excerpt":"","keywords":null,"text":"14.1 事务概念构成单一逻辑工作单元的操作集合称作事务。 数据库系统必须以一种能够避免引入不一致性的方式来管理实务的并发执行。 因为事务是不可分割的，所以要么执行其全部内容，要么就根本不执行。这个称作原子性(Atomicity)。 数据库必须采取特殊处理来确保事务正常执行而不被来自并发执行的数据库语句所干扰。这种特性称为隔离性(Isolation)。尽管多个事务可能并发执行，但系统保证，对于任何一对事务$T_i$和$T_j$，在$T_i$看来，$T_j$或者在$T_i$开始之前已经完成执行，或者在$T_j$完成之后开始执行。因此，每个事务都感觉不到系统中有其他事务在并发执行。 即使系统能保证一个事务的正确执行，如果此后系统崩溃，结果系统“忘记”了该事务，那么这项工作的意义也不大了。因此，即使崩溃后事务的操作也必须是持久的。这种特性称为持久性(Durability)。 这三个构成了构造与数据库的交互的一种理想方式。还有一个特性叫做一致性(Consistency)。如何实现这个则是编写事务的程序员的职责。 14.2 一个简单的事务模型保证原子性的基本思路如下：对于事务要执行写操作的数据项，数据库系统在磁盘上记录其旧值。这个信息记录在一个称为日志的文件中。如果事务没能完成它的执行，在数据库系统从日志中恢复旧值，使得看上去事务从未执行过。保证原子性是数据库系统本身的责任。 持久性的保证可以使用下面的两条中的任何一条来保证： 事务做的更新在事务结束前已经写入磁盘 有关事务已执行的更新信息已写到磁盘上，并且此类信息必须充分，能让数据库在系统出现故障后重新启动时重新构造更新。 事务的并发执行是有可能导致数据库系统的不一致的。一种避免事务并发执行而产生问题的途径是串行地执行事务–一个接一个地执行。但是这个方法不符合性能要求。事务的隔离性确保事务并发执行后的系统状态与这些事务以某种次序一个接一个地执行后的状态是等价的。确保隔离性是数据库系统中称作并发控制系统的部件的责任。 14.3 存储结构易失性存储器比如主存储器和高速缓冲存储器。 非易失性存储器比如二级存储设别磁盘和闪存，三级存储设备光介质和磁带。 稳定性存储器理论上是一个永远不会丢失的存储设备。可以通过复制非易失性存储器来组成。为了一个事务能够持久，它的修改应该写入稳定性存储器。同样，为了一个事务是原子的。日志记录需要在对磁盘上的数据库做任何改变之前写入稳定性存储器。 14.4 事务原子性和持久性事务并不是总能成功，没有成功的事务的状态，叫做事务的终止状态。一旦中止事务造成的变更被撤销，我们就说事务已回滚。恢复机制负责事务中止，典型的方法是维护一个日志。每个事务对数据库的修改都首先会记录到日志中。我们记录执行修改的事务标识符、修改的数据项标识符以及数据项的旧值和新值。然后数据库才会修改。成功完成执行的事务称为已提交。一个对数据库进行过更新的已提交事务使数据库进入一个新的一致状态，即使出现故障，这个状态也得保持。 补偿事务用来撤销一个已提交事务。这个是由用户来控制的。 活动的：初始状态，事务执行时处于这个状态。 部分提交的：最后一条语句执行后 失败的：发现正常的执行不能继续后 中止的：事务回滚并且数据库已恢复到事务开始执行前的状态后 提交的：成功完成之后 部分提交和已提交的区别事务从活动状态开始。当事务完成它的最后一个条语句后就进入了部分提交状态。此刻，事务已经完成它的最后一条语句后就进入了部分提交状态。此刻，事务已经完成执行，但由于实际输出可能仍临时驻留在主存储器中，因此一个硬件故障可能阻止其成功完成。于是事务仍有可能不得不中止。接着数据库系统往磁盘上写入足够的信息，确保即使出现故障时事务所做的更新也能在系统重启后重新创建。 外部可见的写在处理外部可见的写的时候，大多数系统只允许这种写操作在事务进入提交状态后发生。实现这种模式的一种方法是在非易失性存储设备中临时写下与外部写相关的所有数据，然后在事务进入提交状态后再执行真正的写操作。如果在事务进入提交状态后而外部写操作尚未完成之前，系统出现了故障，数据库系统就可以在重启后执行外部写操作了。有的时候重启应该执行一个补偿操作，比如外部写是支付现金，因为人可能已经走了。 14.5 事务隔离性两个允许并发的理由： 提高吞吐量和资源利用率 减少等待时间，可以减少平均响应时间。 数据库必须控制事务之间的交互，以防止它们破坏数据库的一致性。系统通过称为并发控制机制的一系列机制来保证这一点。 一个由n个指令组成的事务，有n!个调度方案。并发情况下并不是所有的调度组合都可以正确执行从而得到一个最终一致的状态。 如果让操作系统来控制事务的并发执行，那么许多调度都是可能的，那么就可能会让数据库处于不一致的状态。因此保证所执行的任何调度都能使数据库处于一致状态，这是数据库系统的任务，数据库中完成此任务的是并发控制部件。 在并发执行中，通过保证所执行的任何调度的效果都与没有并发执行的调度效果一样，我们可以确保数据库的一致性。也就是说调度应该在某种意义上等价于一个串行调度。这种调度称为可串行化调度。 14.6 可串行化冲突可串行化对于一个调度S，其中有分别属于$I$和$J$的两条连续指令$I_i$和$J_j$。如果$I$和$J$引用了不同的数据项，则交换它们不会影响调度中任何指令的结果。然而，如果它们引用了相同的数据项$Q$，则两者的顺序是重要的。现在只考虑有read和write指令，则有下面4种情形： $I=read(Q)$,$J=read(Q)$。那么它们的次序没有关系。 $I=read(Q)$,$J=write(Q)$。那么它们的次序是有关系的。 $I=write(Q)$,$J=read(Q)$。那么它们的次序是有关系的。 $I=write(Q)$,$J=write(Q)$。那么它们的次序是有关系的。 只有当两个操作都为read(Q)的时候或者引用不同的数据项，它们的次序才是没有关系的。也就是它们是无冲突的。当一个调度S可以经过调换一系列无冲突的连续指令（属于不同事务）的执行顺序后得到了一个新的调度$S \\prime$，那么这两个调度之间是冲突等价的。 并不是所有的串行调度之间都是冲突等价的。将两个事务完全调换顺序，就有可能不是冲突等价的。 如果一个调度S与一个串行调度是冲突等价的，那么调度S是冲突可串行化的。 如何判断一个调度是冲突可串行化的考虑下表中的一个调度7 $T_3$ $T_4$ read(Q) write(Q) write(Q) 可以由调度构造一个有向图，称为优先图。 边$T_i \\rightarrow T_j$的存在只有在下列三个条件中的一个成立了： 在$T_j$执行read(Q)之前，$T_i$执行了write(Q) 在$T_j$执行write(Q)之前，$T_i$执行read(Q) 在$T_j$执行write(Q)之前，$T_i$执行了write(Q) 也就是有冲突的指令，都是$T_i$先执行时。如果优先图有环，则这个调度是非冲突可串行化，如果优先图无环，则这个调度是冲突可串行化。 存在比冲突等价定义限制松一些的调度等家定义。 14.7 事务隔离性和原子性调度除了要考虑一致性以外，还要考虑事务的故障恢复。 14.7.1 可恢复调度一个事务在读取了另一个在活跃状态写入的一个值之后，提交了自己。则说明前者依赖后者。如果活跃状态的事务进入了中止状态，而这个事务已经提交了，无法恢复了。这就是一个不可恢复的调度。 一个可恢复调度应该满足：对于每对事务$T_i$和$T_j$，如果$T_j$读取了$T_i$所写的数据项，则$T_i$要优先于$T_j$提交。 14.7.2 无级联调度即使一个调度是可恢复的，要从事务$T_i$的故障中正确恢复，可能需要回滚若干事务。 比如事务$T_8$写入A的值，事务$T_9$读取了A的值。事务$T_9$写入A的值，事务$T_10$读取了A的值。如果$T_8$事务失败，$T_8$则必须回滚，由于$T_9$依赖于$T_8$，因此事务$T_9$必须回滚。同样$T_10$也因为$T_9$而回滚了。这种因单个事务故障导致一系列事务回滚的现象称为级联回滚。 但是级联回滚会导致大量的撤销工作，因此我们希望避免级联回滚发生。这样的调度称为无级联调度。 一个无级联调度应满足：对于每对事务$T_i$和$T_j$，如果$T_j$读取了先前由$T_i$所写的数据项，那么$T_i$必须在$T_j$这一读操作前提交。这个调度也必然是可恢复调度。 14.8 事务隔离性级别对于某些应用，保证可串行性的那些协议可能只允许极小的并发度。在这种情况下，我们采用较弱级别的一致性。为了保证数据库的正确性，使用弱级别一致性给程序员增加了额外的负担。 SQL标准也允许一个事务以一种与其他事务部可串行化的方式执行。例如一个事务可能在未提交读级别上操作，这里允许事务读取甚至还未提交的记录。SQL为那些不要求精确结果的长事务提供这种特征。如果这些事务要在可串行化的方式下执行，它们就会干扰其他事务，造成事务执行的延迟。 SQL标准规定的隔离性级别如下。 可串行化：通常保证可串行化调度。 可重复读：只允许读取已提交数据，而且在一个事务两次读取一个数据项期间，其他事务不得更新该数据。例如，当一个事务在查找满足某些条件的数据时，它可能找到一个已提交事务插入的一些数据，但可能找不到该事务插入的其他数据。 已提交读：只允许读取已提交数据，但不要求可重复读。 未提交读：允许读取未提交数据。这是SQL允许的最低一致性级别。 以上所有隔离性级别都不允许脏写，即如果一个数据项已经被另外一个尚未提交或中止的事务写入，则不允许对该数据项执行写操作。 许多数据库系统运行时的默认隔离性级别时已提交读。有些数据库虽然在隔离性上设置为可串行化，但实际上数据库可能仍然采用了较弱的隔离性。 隔离性级别的实现并发控制机制的目的是获得高度的并发性、同时保证所产生的调度是冲突可串行化或视图可串行化的、可恢复的，并且是无级联的。 并发控制的实现14.9.1 锁共享锁和排他锁。共享锁用于读事务，排他锁用于事务写的数据项。许多事务可以同时持有一个数据项上的共享锁。当且只有当其他事务在一个数据项上不持有任何锁时，一个事务才能持有它的排他锁。这两种锁结合两阶段锁封锁协议在保证可串行化的前提下允许数据的并发读。 14.9.3 时间戳通常在事务开始的时候为它分配一个时间戳，对于数据项，系统维护两个时间戳。数据项的读时间戳记录该数据项的事务的最大（最近的）时间戳。数据项的写时间戳记录写入该数据项当前值的事务的时间戳。 14.9.3 多版本和快照隔离通过维护数据项的多个版本，一个事务允许读取一个旧版本的数据项。有许多多版本并发控制技术，其中一个是实际中广泛应用的称为快照隔离的技术。不会让写操作等待。 14.10 事务的SQL语句表示比如select ID,name from instructor where salary &gt; 90000;，如果有人插入了一个新的人，那么查询的结果就会取决于插入和查询的执行顺序了。这称为幻象。简单模型是无法发现这个冲突的。因为它要求提供一个具体的数据项作为操作的参数。还有一个是谓词锁，封锁一些具体的更新动作。","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://yezhejack.github.io/tags/Database/"}]},{"title":"Chapter 15 Database System Concepts","slug":"Chapter 15 Database System Concepts","date":"2016-10-26T13:18:00.000Z","updated":"2016-11-10T12:58:33.000Z","comments":true,"path":"2016/10/26/Chapter 15 Database System Concepts/","link":"","permalink":"http://yezhejack.github.io/2016/10/26/Chapter 15 Database System Concepts/","excerpt":"","keywords":null,"text":"15.1 基于锁的协议确保隔离性的方法之一是要求对数据项以互斥的方式进行访问。实现该需求的最常用的方法是只允许事务访问当前该事务持有锁的数据项。 15.1.1 锁共享型锁和排他型锁 过早释放数据项可能会导致别的事务看到一个不一致的状态。还可能会导致事务的死锁和饿死状态。 我们如果不使用封锁，或者我们对数据项进行读写之后立即解锁，那么我们可能会进入不一致的状态。另一方面，如果在申请对另一数据项加锁之前如果我们不对当前锁住的数据项解锁，则可能会发生死锁。如果采用封锁，那么死锁是肯定会发生的，但总比发生不一致要好。因为可以通过回滚事务来解决死锁问题。 两个事务$T_i$和$T_j$对于一个事务持有不相容的锁。如果有不相容的锁，并且在调度中$T_i$会先持有锁，那么说$T_i$先于$T_j$，同时拥有一个从$T_i$到$T_j$的箭头。跟优先图类似，如果存在环的话，就说明这个调度不是合法的。 15.1.2 锁的授予按照如下方式来授权锁可以避免事务饿死，当事务$T_i$对数据项Q加M型锁时，并发控制管理器授权加锁的条件： 不存在在数据项Q上持有与M型锁冲突的锁的其他事务 不存在等待对数据项Q加锁且先于$T_i$申请加锁的事务。 15.1.3 两阶段封锁协议该协议要求每个事务分两个阶段提出加锁和解锁申请。 增长阶段：事务可以获得锁，但不能释放锁 缩减阶段：事务可以释放锁，但是不能获得新锁 一个事务获得其最后加锁的位置（增长阶段结束点）称为事务的封锁点。多个事务可以根据它们的封锁点进行排序，实际上，这个顺序就是事务的一个可串行化顺序。 这个协议并不保证不会发生死锁。同时也不保证时无级联的。 级联回滚可以通过讲两阶段封锁修改为严格两阶段封锁协议加以避免。这个协议要求在两阶段封锁协议的基础上限制所有的排他锁的释放都必须在事务提交之后才能释放。这样可以让别的事务在他还没提交的时候是无法读取到他写的值。避免了级联回滚。 另一个两阶段封锁的变体是强两阶段封锁协议，它要求事务提交之前不的释放任何锁，连共享锁都不释放了。 为了更好的并发性，允许锁的转换，从共享锁到排他锁称为升级，反之则称为降级。升级只能发生在增长阶段，降级只能发生在缩减阶段。 严格两阶段封锁与强两阶段封锁包含锁转换，在商用数据库系统中广泛使用。 这里介绍一种自动加锁、解锁的指令 当事务$T_i$进行read(Q)的操作时，系统就产生一条lock-S(Q)指令，该read(Q)指令紧跟其后。 事务$T_i$进行write(Q)操作时，系统检查$T_i$是否已在Q上持有共享锁。若有，则系统发出upgrade(Q)指令，后接write(Q)指令。否则系统发出lock-X(Q)指令，后接write(Q)指令。 当一个事务提交或中止后，该事务持有的所有锁都被释放。 15.1.4 封锁的实现锁管理器可以实现为一个过程，它从事务接受消息并反馈信息。锁管理器过程针对锁请求消息返回授予锁信息，或者要求事务回滚的消息（发生死锁时）。 锁管理器为目前已加锁的每个数据项维护一个链表，每一个请求为链表中的一个记录，按请求到达的顺序排序。它使用一个以数据项名称为索引的散列表来查找链表中的数据项。这个表叫做锁表。锁表采用的溢出链。 处理过程 当一条锁请求消息到达时，如果相应数据项的链表存在，在该链表末尾增加一个记录；否则新建一个仅包含该请求记录的链表。在当前没有加锁的数据项上总是授予第一次加锁请求，但当事务向已被加锁的数据项加锁时，只有当该请求与当前持有的锁相容，并且所有先前的请求都已授予锁的条件下，锁管理器才为该请求授予锁，否则该请求只好等待。 当锁管理器收到一个事务的解锁信息时，它将与该事务相对应的数据项链表中的记录删除，然后检查随后的记录，如果有，如前所述，就看该请求能否被授权，如果能，锁管理器授权该请求并处理其后记录，如果还有，类似地一个接一个地处理。 如果一个事务中止，锁管理器删除该事务产生的正在等待加锁的所有请求。一旦数据库系统采取适当动作撤销该事务，该中止事务持有的所有锁将被释放。 这个算法保证了锁请求无饿死现象，因为在先前接收到的请求正在等待加锁时，后来者不可能获得授权。 15.1.5 基于图的协议如果要开发非两阶段协议，我们需要有关每个事务如何存取数据库的附加信息。最简单的模型要求我们事先知道访问数据项的顺序。 为了获取这些事先的知识，我们要求所有数据项集合$D={d_1,d_2,...,d_n}$满足偏序$\\rightarrow$：如果$d_i \\rightarrow d_j$，则任何既访问$d_i$又访问$d_j$的事务必须首先访问$d_i$然后再访问$d_j$。这种偏序可以是数据的逻辑或物理组织的结果，也可以只是为了并发控制而加上的。 偏序意味着会是个无环图，称为数据库图。 给出一个称为树形协议的简单协议，该协议只使用排他锁。在树形应协议中，可用的加锁指令只有lock-X。每个事务$T_i$对一数据项最多能加一次锁，并且遵从以下规则： $T_i$首次加锁可以对任何数据进行 此后，$T_i$对数据项Q加锁的前提是$T_i$当前持有Q的父项上的锁 对数据项解锁可以随时进行 数据项$T_i$加锁并解锁后，$T_i$不能再对数据项加锁。 所有满足树形协议的调度是冲突可串行化的。并且保证不会产生死锁。但是不保证可恢复性和无级联回滚。为了保证可恢复性和无级联回滚，将协议修改为在事务结束前不允许释放排他锁。但是这就降低了并发度，有一个替代方案，但只保证了可恢复性：为每一个发生了提交写操作的数据项，我们记录是哪个事务最后对它执行了写操作，当事务$T_i$执行了对未提交数据项的读操作，我们就在最后对该数据项执行了写操作的事务上记录一个$T_i$的提交依赖，在有$T_i$的提交依赖的所有事务提交完成之前，$T_i$不得提交。如果其中任何一个事务中止，$T_i$也必须中止。 树形协议不产生死锁，所以不需要回滚。树形封锁协议的另一个优点是可以在较早时候释放锁，但是它封锁了一些额外的数据，会对并发性造成影响。 15.2 死锁处理死锁处理主要有两种方式 死锁预防，不让系统出现任何死锁的状况 死锁的检测和恢复 两种方法都会导致事务的回滚。 15.2.1 死锁预防一种方式是通过对加锁请求进行排序或要求同时获得所有的锁来保证不会发生循环等待。另一种方法比较接近死锁恢复，每当等待有可能导致死锁，进行事务回滚而不是等待加锁。 防止死锁的第一种方法是要求事务在开始之前就把所有需要使用到的数据项加上锁。另一个机制是对所有的数据项加上一个次序，同时要求事务只能按次序规定的顺序封锁数据项。比如使用全序关系，一旦一个事务锁住了某个特定的数据项，它就不能申请顺序中位于该数据项前面的数据项上的锁了。 第二种方法就是使用抢占和事务回滚。系统通过时间戳来判断是否允许强占。若一个事务回滚，则使用它原先的时间戳。 已提出的利用时间戳的两种不同的死锁预防机制。 wait-die机制基于非强占。当一个事务$T_i$想要获得的锁被比他新的事务所占据的时候，允许$T_i$等待，否则就让它自己滚（回滚）！ wound-wait机制是基于抢占的技术。当事务$T_i$申请的数据项被$T_j$持有，仅当$T_i$比$T_j$年轻，允许$T_i$等待，否则就让$T_j$滚。 这两种机制都需要面临不必要的回滚。 还有一个基于锁超时的方法。但是超时的阈值难以判断，因此使用不多。 15.2.2 死锁检测与恢复系统想要破解一个死锁的状况的话，系统必须 维护当前数据项分配给事务的有关信息，以及任何尚未解决的数据项请求信息 提供一个使用这些信息判断系统是否进入死锁状态的算法 当检测算法判定存在死锁时，从死锁中恢复 15.2.2.1 死锁检测死锁可以用称为等待图的有向图来精确描述。$T_i \\rightarrow T_j$表示事务$T_i$在等待$T_j$释放所需要的数据项。这个图是动态变化的。一旦出现环，说明存在死锁，环上的所有事务都是处在死锁的状态下。 15.2.2.2 从死锁中恢复解除死锁最通常的做法是回滚一个或多个事务。 需要解决三个问题 一个是选择谁来回滚，通常是选择系统认为代价小的事务来回滚 回滚的距离，是彻底回滚还是部分回滚 要避免有些事务不断被回滚而饿死。 15.3 多粒度通过允许各种大小的数据项并定义数据粒度的层次结构，其中小粒度数据项嵌套在大粒度数据项中来实现。当事务对一个结点加锁，或为共享锁或为排他锁，该事务也以同样类型的锁隐式地封锁这个结点的全部后代结点。加锁的时候要延路径查看，当路径上所有结点的锁有一个出现互斥的话，就无法加锁。 意向锁来解决当要大粒度封锁的时候，检查小粒度是否上锁了。如果一个结点加了意向锁，则说明它在树的较低层次进行显示加锁了。在一个结点显示加锁之前，要对它的所有祖先结点加上意向锁。共享型意向锁(IS)、排他型意向锁（IX）。若一个结点加上了共享排他型意向锁(SIX)，则说明以该结点为根的子树显式地加了共享锁，并且在树的更低层显示地加了排他锁。 15.4 基于时间戳的协议前面所说的所有的封锁协议中，每一对冲突事务的次序是在执行时由二者都申请的，但类型不相容的第一个锁决定的。另一种决定事务可串行化次序的方法是事先选定事务的次序。其中最常用的方法就是时间戳排序机制。 15.4.1 时间戳对于一个事务$T_i$，把唯一一个时间戳$TS(T_i)$和它联系起来。 时间戳可以是系统时钟或逻辑计数器，其中逻辑计数器在每赋予一个时间戳之后自加1。 还需要两个时间戳： W-timestamp(Q)表示成功执行write(Q)的所有事务的最大时间戳 R-timestamp(Q)表示成功执行read(Q)的所有事务的最大时间戳 15.4.2 时间戳排序协议 假设事务$T_i$发出read(Q)。 若$TS(T_i) \\gt W-timestamp(Q)$，则说明$TS(T_i)$需要读入的Q值已被覆盖。因此read操作被拒绝，$TS(T_i)$回滚 若$TS(T_i) \\leq W-timestamp(Q)$ 假设事务发出了write(Q)： 若$TS(T_i) \\le R-timestamp(Q)$，则$T_i(Q)$产生的Q值是先前所需要的，且系统已经假定该值不会再产生。因此write被拒绝，并且事务被回滚 若$TS(T_i) \\le W-timestamp(Q)$，则$T_i$试图写入的Q值已过时。因此，write操作被拒绝，事务回滚 其他情况下，系统执行write操作，将W-timestamp(Q)设置为$TS(T_i)$ 被回滚的事务都会有一个新的时间戳。该协议保证无死锁，因为不存在等待的事务。但是有可能会饿死。该协议还会产生不可恢复的调度，但可以用下面的任何一个方法来保证可恢复性。 在事务末尾执行所有的写操作，并且在写的时候不允许访问已写完的任何数据项 可恢复性和无级联性也可通过使用一个受限的封锁形式来保证，由此，对未提交数据项的读操作被推迟到更新该数据项的事务提交之后 可恢复性可以通过跟踪未提交写操作来单独保证。一个事务读取了别的事务所写的数据后，只能等别的事务也提交了之后他才能提交。 15.4.3 Thomas写规则这里提供一个例子 $T_{27}$ $T_{28}$ read(Q) write(Q) write(Q) 当$T_{27}$执行write(Q)是，发现已经被$T_{28}$写了，就会被回滚。但这是不必要的。 可以修改一下时间戳协议，其中关于读的不变，对于写的部分做了修改。对于其中的第二条 若$TS(T_i) \\le W-timestamp(Q)$，则$T_i$试图写入的Q值已过时。因此，write操作被拒绝，事务回滚 修改为 若$TS(T_i) \\le W-timestamp(Q)$，则$T_i$试图写入的Q值已过时。则将write忽略。 这样就避免了回滚。当然也要首先检查第一条规则。 他产生视图可串行化调度。 视图可串行化每个冲突可串行化调度都是视图可串行化调度，但存在非冲突可串行化的视图可串行化调度。 基于有效性检查的协议要求每个事务$T_i$在其生命周期中按两个或三个阶段执行，这取决于该事务是一个只读事务还是一个更新事务。 读阶段：在这一阶段中，系统执行事务$T_i$。各数据项值被读入并保存在事务$T_i$的局部变量中。所有write操作都是对局部临时变量进行的，并不对数据库进行真正的更新。 有效性检查阶段：对事务$T_i$进行有效性测试。判定是否可以执行write操作而不违反可串行性。如果事务有效性测试失败，则系统终止这个事务。 写阶段：若事务$T_i$已通过有效性检查，则保存$T_i$任何写操作结果的临时局部变量值都被复制到数据库中。 只读事务只有前面两个阶段。 给事务定义另外三个时间 Start($T_i$) Validation($T_i$) Stop($T_i$) 其中将Validation($T_i$)作为事务本省的时间戳TS($T_i$)。并利用时间戳排序技术决定可串行性顺序。选择Validation($T_i$)来作为TS，是我因为在冲突频率很低的读阶段可以获得更快的响应时间。 有效性测试：对于任何满足TS($T_k$) &lt; TS($T_i$)的事务TS($T_k$)必须满足下面条件之一： Finish($T_k$) &lt; Start($T_i$) $T_k$所写的数据项集与$T_i$所读数据项集不相交。并且$T_k$的写阶段在$T_i$开始有效性检查之前就开始了。即$Start(T_i) \\le Finish(T_k) \\le Validation(T_i)$。这个条件保证了二者写不重叠。 15.6 多版本机制前面的并发控制机制要么延迟一项操作要么中止发出该操作的事务来保证可串行性。在多版本并发控制机制中，每个write(Q)操作创建Q的一个新版本。当事务发出一个read(Q)操作时，并发控制管理器选择Q的一个版本进行读取。并发控制机制必须保证用于读取的版本的选择能保持可串行性。 15.6.1 多版本时间戳排序对于每个数据项Q，有一个版本序列$Q_1,Q_2,Q_3,...,Q_k$。每个版本都包含三个数据字段： Content(Q) W-timestamp(Q) R-timestamp(Q)如果事务通过发出write(Q)操作创建了一个Q的新版本，那么它的两个读写时间都用事务的时间戳来初始化。 事务读取离现在最近的一个版本当事务发出写请求的时候，如果事务时间戳小于W-timestamp(Q)的话，则事务回滚，若等于则覆盖，若大于则创建新的版本。（针对最近的一个版本的时间戳） 除此之外还多了一个需要删除无用的版本。当$Q_k$和$Q_j$的时间戳都小于系统中最老的事务的时间戳，则其中的较老的一个需要删除。 读请求从不失败且不需要等待。但是读请求需要写时间戳，产生磁盘操作。 通过扩展来获得可恢复性和无级联性。 15.6.2 多版本两阶段锁该协议对只读事务和更新事务加以区分。 更新事务执行两阶段封锁协议；即它们持有全部锁到事务结束。因此它们可以按提交顺序进行串行化。用计数器作为时间戳。 只读事务在执行读操作时遵从多版本时间戳协议。 当更新事务读取一个数据项时，它在获得该数据项上的共享锁后读取该数据项最新版本的值。当更新事务想写一个数据项时，它首先要获得该数据项上的排他锁，然后为此数据项创建一个新版本。新版本的时间戳初始化为无穷大。当更新事务完成后，再设置成计数器加1的值。这个协议保证了调度是可恢复的和无级联的。版本删除同多版本时间戳协议一样。 15.7快照隔离这种隔离对于只读事务来说是理想的，因为它们不用等待，也不会被并发管理器中止。 15.7.1 更新事务的有效性检验步骤先提交者获胜检查是否有与$T$并发执行的事务，对于$T$打算写入的某些数据，该事务已经将更新写入数据库。 如果发现这样的事务，则$T$中止。 如果没有发现这样的事务，则$T$提交，并且将更新写入数据库。 先更新者获胜当事务$T_i$试图更新一个数据项时，它请求该数据项的一个写锁，拿到锁之后它需要检查是否被并发事务更新过，如果发现被写过则需要中止。 快照隔离并不保证调度时可串行的 比如同时创建订单，两个任务在创建订单的时候看到的都是相同的已有的订单集合，但订单的编号时逐个加1的话，就会出现订单号相同的情况了。 插入操作、删除操作与谓词读","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://yezhejack.github.io/tags/Database/"}]},{"title":"Deep Learning 参考资料","slug":"Deep Learning 参考资料","date":"2016-10-24T12:05:00.000Z","updated":"2017-06-16T05:03:40.000Z","comments":true,"path":"2016/10/24/Deep Learning 参考资料/","link":"","permalink":"http://yezhejack.github.io/2016/10/24/Deep Learning 参考资料/","excerpt":"","keywords":null,"text":"推荐的一个关于深度学习的参考资料声明这个文章的内容转载自a link Deep Learning Papers Reading Roadmap If you are a newcomer to the Deep Learning area, the first question you may have is “Which paper should I start reading from?” Here is a reading roadmap of Deep Learning papers! The roadmap is constructed in accordance with the following four guidelines: From outline to detail From old to state-of-the-art from generic to specific areas focus on state-of-the-art You will find many papers that are quite new but really worth reading. I would continue adding papers to this roadmap. 1 Deep Learning History and Basics1.0 Book[0] Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. “Deep learning.” An MIT Press book. (2015). [pdf] (Deep Learning Bible, you can read this book while reading following papers.) :star::star::star::star::star: 1.1 Survey[1] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. “Deep learning.” Nature 521.7553 (2015): 436-444. [pdf] (Three Giants’ Survey) :star::star::star::star::star: 1.2 Deep Belief Network(DBN)(Milestone of Deep Learning Eve)[2] Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. “A fast learning algorithm for deep belief nets.” Neural computation 18.7 (2006): 1527-1554. [pdf](Deep Learning Eve) :star::star::star: [3] Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. “Reducing the dimensionality of data with neural networks.” Science 313.5786 (2006): 504-507. [pdf] (Milestone, Show the promise of deep learning) :star::star::star: 1.3 ImageNet Evolution（Deep Learning broke out from here）[4] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems. 2012. [pdf] (AlexNet, Deep Learning Breakthrough) :star::star::star::star::star: [5] Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014). [pdf] (VGGNet,Neural Networks become very deep!) :star::star::star: [6] Szegedy, Christian, et al. “Going deeper with convolutions.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [pdf] (GoogLeNet) :star::star::star: [7] He, Kaiming, et al. “Deep residual learning for image recognition.” arXiv preprint arXiv:1512.03385 (2015). [pdf] (ResNet,Very very deep networks, CVPR best paper) :star::star::star::star::star: 1.4 Speech Recognition Evolution[8] Hinton, Geoffrey, et al. “Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.” IEEE Signal Processing Magazine 29.6 (2012): 82-97. [pdf] (Breakthrough in speech recognition):star::star::star::star: [9] Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. “Speech recognition with deep recurrent neural networks.” 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [pdf] (RNN):star::star::star: [10] Graves, Alex, and Navdeep Jaitly. “Towards End-To-End Speech Recognition with Recurrent Neural Networks.” ICML. Vol. 14. 2014. [pdf]:star::star::star: [11] Sak, Haşim, et al. “Fast and accurate recurrent neural network acoustic models for speech recognition.” arXiv preprint arXiv:1507.06947 (2015). [pdf] (Google Speech Recognition System) :star::star::star: [12] Amodei, Dario, et al. “Deep speech 2: End-to-end speech recognition in english and mandarin.” arXiv preprint arXiv:1512.02595 (2015). [pdf] (Baidu Speech Recognition System) :star::star::star::star: [13] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig “Achieving Human Parity in Conversational Speech Recognition.” arXiv preprint arXiv:1610.05256 (2016). [pdf] (State-of-the-art in speech recognition, Microsoft) :star::star::star::star: After reading above papers, you will have a basic understanding of the Deep Learning history, the basic architectures of Deep Learning model(including CNN, RNN, LSTM) and how deep learning can be applied to image and speech recognition issues. The following papers will take you in-depth understanding of the Deep Learning method, Deep Learning in different areas of application and the frontiers. I suggest that you can choose the following papers based on your interests and research direction. #2 Deep Learning Method 2.1 Model[14] Hinton, Geoffrey E., et al. “Improving neural networks by preventing co-adaptation of feature detectors.” arXiv preprint arXiv:1207.0580 (2012). [pdf] (Dropout) :star::star::star: [15] Srivastava, Nitish, et al. “Dropout: a simple way to prevent neural networks from overfitting.” Journal of Machine Learning Research 15.1 (2014): 1929-1958. [pdf] :star::star::star: [16] Ioffe, Sergey, and Christian Szegedy. “Batch normalization: Accelerating deep network training by reducing internal covariate shift.” arXiv preprint arXiv:1502.03167 (2015). [pdf] (An outstanding Work in 2015) :star::star::star::star: [17] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. “Layer normalization.” arXiv preprint arXiv:1607.06450 (2016). [pdf] (Update of Batch Normalization) :star::star::star::star: [18] Courbariaux, Matthieu, et al. “Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1.” [pdf] (New Model,Fast) :star::star::star: [19] Jaderberg, Max, et al. “Decoupled neural interfaces using synthetic gradients.” arXiv preprint arXiv:1608.05343 (2016). [pdf] (Innovation of Training Method,Amazing Work) :star::star::star::star::star: [20] Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. “Net2net: Accelerating learning via knowledge transfer.” arXiv preprint arXiv:1511.05641 (2015). [pdf] (Modify previously trained network to reduce training epochs) :star::star::star: [21] Wei, Tao, et al. “Network Morphism.” arXiv preprint arXiv:1603.01670 (2016). [pdf] (Modify previously trained network to reduce training epochs) :star::star::star: 2.2 Optimization[22] Sutskever, Ilya, et al. “On the importance of initialization and momentum in deep learning.” ICML (3) 28 (2013): 1139-1147. [pdf] (Momentum optimizer) :star::star: [23] Kingma, Diederik, and Jimmy Ba. “Adam: A method for stochastic optimization.” arXiv preprint arXiv:1412.6980 (2014). [pdf] (Maybe used most often currently) :star::star::star: [24] Andrychowicz, Marcin, et al. “Learning to learn by gradient descent by gradient descent.” arXiv preprint arXiv:1606.04474 (2016). [pdf] (Neural Optimizer,Amazing Work) :star::star::star::star::star: [25] Han, Song, Huizi Mao, and William J. Dally. “Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding.” CoRR, abs/1510.00149 2 (2015). [pdf] (ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup) :star::star::star::star::star: [26] Iandola, Forrest N., et al. “SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 1MB model size.” arXiv preprint arXiv:1602.07360 (2016). [pdf] (Also a new direction to optimize NN,DeePhi Tech Startup) :star::star::star::star: 2.3 Unsupervised Learning / Deep Generative Model[27] Le, Quoc V. “Building high-level features using large scale unsupervised learning.” 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [pdf] (Milestone, Andrew Ng, Google Brain Project, Cat) :star::star::star::star: [28] Kingma, Diederik P., and Max Welling. “Auto-encoding variational bayes.” arXiv preprint arXiv:1312.6114 (2013). [pdf] (VAE) :star::star::star::star: [29] Goodfellow, Ian, et al. “Generative adversarial nets.” Advances in Neural Information Processing Systems. 2014. [pdf] (GAN,super cool idea) :star::star::star::star::star: [30] Radford, Alec, Luke Metz, and Soumith Chintala. “Unsupervised representation learning with deep convolutional generative adversarial networks.” arXiv preprint arXiv:1511.06434 (2015). [pdf] (DCGAN) :star::star::star::star: [31] Gregor, Karol, et al. “DRAW: A recurrent neural network for image generation.” arXiv preprint arXiv:1502.04623 (2015). [pdf] (VAE with attention, outstanding work) :star::star::star::star::star: [32] Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. “Pixel recurrent neural networks.” arXiv preprint arXiv:1601.06759 (2016). [pdf] (PixelRNN) :star::star::star::star: [33] Oord, Aaron van den, et al. “Conditional image generation with PixelCNN decoders.” arXiv preprint arXiv:1606.05328 (2016). [pdf] (PixelCNN) :star::star::star::star: 2.4 RNN / Sequence-to-Sequence Model[34] Graves, Alex. “Generating sequences with recurrent neural networks.” arXiv preprint arXiv:1308.0850 (2013). [pdf] (LSTM, very nice generating result, show the power of RNN) :star::star::star::star: [35] Cho, Kyunghyun, et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078 (2014). [pdf] (First Seq-to-Seq Paper) :star::star::star::star: [36] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. “Sequence to sequence learning with neural networks.” Advances in neural information processing systems. 2014. [pdf] (Outstanding Work) :star::star::star::star::star: [37] Bahdanau, Dzmitry, KyungHyun Cho, and Yoshua Bengio. “Neural Machine Translation by Jointly Learning to Align and Translate.” arXiv preprint arXiv:1409.0473 (2014). [pdf] :star::star::star::star: [38] Vinyals, Oriol, and Quoc Le. “A neural conversational model.” arXiv preprint arXiv:1506.05869 (2015). [pdf]) (Seq-to-Seq on Chatbot) :star::star::star: 2.5 Neural Turing Machine[39] Graves, Alex, Greg Wayne, and Ivo Danihelka. “Neural turing machines.” arXiv preprint arXiv:1410.5401 (2014). [pdf] (Basic Prototype of Future Computer) :star::star::star::star::star: [40] Zaremba, Wojciech, and Ilya Sutskever. “Reinforcement learning neural Turing machines.” arXiv preprint arXiv:1505.00521 362 (2015). [pdf] :star::star::star: [41] Weston, Jason, Sumit Chopra, and Antoine Bordes. “Memory networks.” arXiv preprint arXiv:1410.3916 (2014). [pdf] :star::star::star: [42] Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. “End-to-end memory networks.” Advances in neural information processing systems. 2015. [pdf] :star::star::star::star: [43] Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly. “Pointer networks.” Advances in Neural Information Processing Systems. 2015. [pdf] :star::star::star::star: [44] Graves, Alex, et al. “Hybrid computing using a neural network with dynamic external memory.” Nature (2016). [pdf] (Milestone,combine above papers’ ideas) :star::star::star::star::star: 2.6 Deep Reinforcement Learning[45] Mnih, Volodymyr, et al. “Playing atari with deep reinforcement learning.” arXiv preprint arXiv:1312.5602 (2013). [pdf]) (First Paper named deep reinforcement learning) :star::star::star::star: [46] Mnih, Volodymyr, et al. “Human-level control through deep reinforcement learning.” Nature 518.7540 (2015): 529-533. [pdf] (Milestone) :star::star::star::star::star: [47] Wang, Ziyu, Nando de Freitas, and Marc Lanctot. “Dueling network architectures for deep reinforcement learning.” arXiv preprint arXiv:1511.06581 (2015). [pdf] (ICLR best paper,great idea) :star::star::star::star: [48] Mnih, Volodymyr, et al. “Asynchronous methods for deep reinforcement learning.” arXiv preprint arXiv:1602.01783 (2016). [pdf] (State-of-the-art method) :star::star::star::star::star: [49] Lillicrap, Timothy P., et al. “Continuous control with deep reinforcement learning.” arXiv preprint arXiv:1509.02971 (2015). [pdf] (DDPG) :star::star::star::star: [50] Gu, Shixiang, et al. “Continuous Deep Q-Learning with Model-based Acceleration.” arXiv preprint arXiv:1603.00748 (2016). [pdf] (NAF) :star::star::star::star: [51] Schulman, John, et al. “Trust region policy optimization.” CoRR, abs/1502.05477 (2015). [pdf] (TRPO) :star::star::star::star: [52] Silver, David, et al. “Mastering the game of Go with deep neural networks and tree search.” Nature 529.7587 (2016): 484-489. [pdf] (AlphaGo) :star::star::star::star::star: 2.7 Deep Transfer Learning / Lifelong Learning / especially for RL[53] Bengio, Yoshua. “Deep Learning of Representations for Unsupervised and Transfer Learning.” ICML Unsupervised and Transfer Learning 27 (2012): 17-36. [pdf] (A Tutorial) :star::star::star: [54] Silver, Daniel L., Qiang Yang, and Lianghao Li. “Lifelong Machine Learning Systems: Beyond Learning Algorithms.” AAAI Spring Symposium: Lifelong Machine Learning. 2013. [pdf] (A brief discussion about lifelong learning) :star::star::star: [55] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 (2015). [pdf] (Godfather’s Work) :star::star::star::star: [56] Rusu, Andrei A., et al. “Policy distillation.” arXiv preprint arXiv:1511.06295 (2015). [pdf] (RL domain) :star::star::star: [57] Parisotto, Emilio, Jimmy Lei Ba, and Ruslan Salakhutdinov. “Actor-mimic: Deep multitask and transfer reinforcement learning.” arXiv preprint arXiv:1511.06342 (2015). [pdf] (RL domain) :star::star::star: [58] Rusu, Andrei A., et al. “Progressive neural networks.” arXiv preprint arXiv:1606.04671 (2016). [pdf] (Outstanding Work, A novel idea) :star::star::star::star::star: 2.8 One Shot Deep Learning[59] Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. “Human-level concept learning through probabilistic program induction.” Science 350.6266 (2015): 1332-1338. [pdf] (No Deep Learning,but worth reading) :star::star::star::star::star: [60] Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. “Siamese Neural Networks for One-shot Image Recognition.”(2015) [pdf] :star::star::star: [61] Santoro, Adam, et al. “One-shot Learning with Memory-Augmented Neural Networks.” arXiv preprint arXiv:1605.06065 (2016). [pdf] (A basic step to one shot learning) :star::star::star::star: [62] Vinyals, Oriol, et al. “Matching Networks for One Shot Learning.” arXiv preprint arXiv:1606.04080 (2016). [pdf] :star::star::star: [63] Hariharan, Bharath, and Ross Girshick. “Low-shot visual object recognition.” arXiv preprint arXiv:1606.02819 (2016). [pdf] (A step to large data) :star::star::star::star: 3 Applications3.1 NLP(Natural Language Processing)[1] Antoine Bordes, et al. “Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing.” AISTATS(2012) [pdf] :star::star::star::star: [2] Mikolov, et al. “Distributed representations of words and phrases and their compositionality.” ANIPS(2013): 3111-3119 [pdf] (word2vec) :star::star::star: [3] Sutskever, et al. ““Sequence to sequence learning with neural networks.” ANIPS(2014) [pdf] :star::star::star: [4] Ankit Kumar, et al. ““Ask Me Anything: Dynamic Memory Networks for Natural Language Processing.” arXiv preprint arXiv:1506.07285(2015) [pdf] :star::star::star::star: [5] Yoon Kim, et al. “Character-Aware Neural Language Models.” NIPS(2015) arXiv preprint arXiv:1508.06615(2015) [pdf] :star::star::star::star: [6] Jason Weston, et al. “Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks.” arXiv preprint arXiv:1502.05698(2015) [pdf] (bAbI tasks) :star::star::star: [7] Karl Moritz Hermann, et al. “Teaching Machines to Read and Comprehend.” arXiv preprint arXiv:1506.03340(2015) [pdf] (CNN/DailyMail cloze style questions) :star::star: [8] Alexis Conneau, et al. “Very Deep Convolutional Networks for Natural Language Processing.” arXiv preprint arXiv:1606.01781(2016) [pdf] (state-of-the-art in text classification) :star::star::star: [9] Armand Joulin, et al. “Bag of Tricks for Efficient Text Classification.” arXiv preprint arXiv:1607.01759(2016) [pdf] (slightly worse than state-of-the-art, but a lot faster) :star::star::star: 3.2 Object Detection[1] Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. “Deep neural networks for object detection.” Advances in Neural Information Processing Systems. 2013. [pdf] :star::star::star: [2] Girshick, Ross, et al. “Rich feature hierarchies for accurate object detection and semantic segmentation.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. [pdf] (RCNN) :star::star::star::star::star: [3] He, Kaiming, et al. “Spatial pyramid pooling in deep convolutional networks for visual recognition.” European Conference on Computer Vision. Springer International Publishing, 2014. [pdf] (SPPNet) :star::star::star::star: [4] Girshick, Ross. “Fast r-cnn.” Proceedings of the IEEE International Conference on Computer Vision. 2015. [pdf] :star::star::star::star: [5] Ren, Shaoqing, et al. “Faster R-CNN: Towards real-time object detection with region proposal networks.” Advances in neural information processing systems. 2015. [pdf] :star::star::star::star: [6] Redmon, Joseph, et al. “You only look once: Unified, real-time object detection.” arXiv preprint arXiv:1506.02640 (2015). [pdf] (YOLO,Oustanding Work, really practical) :star::star::star::star::star: [7] Liu, Wei, et al. “SSD: Single Shot MultiBox Detector.” arXiv preprint arXiv:1512.02325 (2015). [pdf] :star::star::star: [8] Dai, Jifeng, et al. “R-FCN: Object Detection viaRegion-based Fully Convolutional Networks.” arXiv preprint arXiv:1605.06409 (2016). [pdf] :star::star::star::star: [9] He, Gkioxari, et al. “Mask R-CNN“ arXiv preprint arXiv:1703.06870 (2017). [pdf] :star::star::star::star: 3.3 Visual Tracking[1] Wang, Naiyan, and Dit-Yan Yeung. “Learning a deep compact image representation for visual tracking.” Advances in neural information processing systems. 2013. [pdf] (First Paper to do visual tracking using Deep Learning,DLT Tracker) :star::star::star: [2] Wang, Naiyan, et al. “Transferring rich feature hierarchies for robust visual tracking.” arXiv preprint arXiv:1501.04587 (2015). [pdf] (SO-DLT) :star::star::star::star: [3] Wang, Lijun, et al. “Visual tracking with fully convolutional networks.” Proceedings of the IEEE International Conference on Computer Vision. 2015. [pdf] (FCNT) :star::star::star::star: [4] Held, David, Sebastian Thrun, and Silvio Savarese. “Learning to Track at 100 FPS with Deep Regression Networks.” arXiv preprint arXiv:1604.01802 (2016). [pdf] (GOTURN,Really fast as a deep learning method,but still far behind un-deep-learning methods) :star::star::star::star: [5] Bertinetto, Luca, et al. “Fully-Convolutional Siamese Networks for Object Tracking.” arXiv preprint arXiv:1606.09549 (2016). [pdf] (SiameseFC,New state-of-the-art for real-time object tracking) :star::star::star::star: [6] Martin Danelljan, Andreas Robinson, Fahad Khan, Michael Felsberg. “Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking.” ECCV (2016) [pdf] (C-COT) :star::star::star::star: [7] Nam, Hyeonseob, Mooyeol Baek, and Bohyung Han. “Modeling and Propagating CNNs in a Tree Structure for Visual Tracking.” arXiv preprint arXiv:1608.07242 (2016). [pdf] (VOT2016 Winner,TCNN) :star::star::star::star: 3.4 Image Caption[1] Farhadi,Ali,etal. “Every picture tells a story: Generating sentences from images“. In Computer VisionECCV 2010. Springer Berlin Heidelberg:15-29, 2010. [pdf] :star::star::star: [2] Kulkarni, Girish, et al. “Baby talk: Understanding and generating image descriptions“. In Proceedings of the 24th CVPR, 2011. [pdf]:star::star::star::star: [3] Vinyals, Oriol, et al. “Show and tell: A neural image caption generator“. In arXiv preprint arXiv:1411.4555, 2014. [pdf]:star::star::star: [4] Donahue, Jeff, et al. “Long-term recurrent convolutional networks for visual recognition and description“. In arXiv preprint arXiv:1411.4389 ,2014. [pdf] [5] Karpathy, Andrej, and Li Fei-Fei. “Deep visual-semantic alignments for generating image descriptions“. In arXiv preprint arXiv:1412.2306, 2014. [pdf]:star::star::star::star::star: [6] Karpathy, Andrej, Armand Joulin, and Fei Fei F. Li. “Deep fragment embeddings for bidirectional image sentence mapping“. In Advances in neural information processing systems, 2014. [pdf]:star::star::star::star: [7] Fang, Hao, et al. “From captions to visual concepts and back“. In arXiv preprint arXiv:1411.4952, 2014. [pdf]:star::star::star::star::star: [8] Chen, Xinlei, and C. Lawrence Zitnick. “Learning a recurrent visual representation for image caption generation“. In arXiv preprint arXiv:1411.5654, 2014. [pdf]:star::star::star::star: [9] Mao, Junhua, et al. “Deep captioning with multimodal recurrent neural networks (m-rnn)“. In arXiv preprint arXiv:1412.6632, 2014. [pdf]:star::star::star: [10] Xu, Kelvin, et al. “Show, attend and tell: Neural image caption generation with visual attention“. In arXiv preprint arXiv:1502.03044, 2015. [pdf]:star::star::star::star::star: 3.5 Machine Translation Some milestone papers are listed in RNN / Seq-to-Seq topic. [1] Luong, Minh-Thang, et al. “Addressing the rare word problem in neural machine translation.” arXiv preprint arXiv:1410.8206 (2014). [pdf] :star::star::star::star: [2] Sennrich, et al. “Neural Machine Translation of Rare Words with Subword Units“. In arXiv preprint arXiv:1508.07909, 2015. [pdf]:star::star::star: [3] Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. “Effective approaches to attention-based neural machine translation.” arXiv preprint arXiv:1508.04025 (2015). [pdf] :star::star::star::star: [4] Chung, et al. “A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation“. In arXiv preprint arXiv:1603.06147, 2016. [pdf]:star::star: [5] Lee, et al. “Fully Character-Level Neural Machine Translation without Explicit Segmentation“. In arXiv preprint arXiv:1610.03017, 2016. [pdf]:star::star::star::star::star: [6] Wu, Schuster, Chen, Le, et al. “Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation“. In arXiv preprint arXiv:1609.08144v2, 2016. [pdf] (Milestone) :star::star::star::star: 3.6 Robotics[1] Koutník, Jan, et al. “Evolving large-scale neural networks for vision-based reinforcement learning.” Proceedings of the 15th annual conference on Genetic and evolutionary computation. ACM, 2013. [pdf] :star::star::star: [2] Levine, Sergey, et al. “End-to-end training of deep visuomotor policies.” Journal of Machine Learning Research 17.39 (2016): 1-40. [pdf] :star::star::star::star::star: [3] Pinto, Lerrel, and Abhinav Gupta. “Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours.” arXiv preprint arXiv:1509.06825 (2015). [pdf] :star::star::star: [4] Levine, Sergey, et al. “Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection.” arXiv preprint arXiv:1603.02199 (2016). [pdf] :star::star::star::star: [5] Zhu, Yuke, et al. “Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning.” arXiv preprint arXiv:1609.05143 (2016). [pdf] :star::star::star::star: [6] Yahya, Ali, et al. “Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.” arXiv preprint arXiv:1610.00673 (2016). [pdf] :star::star::star::star: [7] Gu, Shixiang, et al. “Deep Reinforcement Learning for Robotic Manipulation.” arXiv preprint arXiv:1610.00633 (2016). [pdf] :star::star::star::star: [8] A Rusu, M Vecerik, Thomas Rothörl, N Heess, R Pascanu, R Hadsell.”Sim-to-Real Robot Learning from Pixels with Progressive Nets.” arXiv preprint arXiv:1610.04286 (2016). [pdf] :star::star::star::star: [9] Mirowski, Piotr, et al. “Learning to navigate in complex environments.” arXiv preprint arXiv:1611.03673 (2016). [pdf] :star::star::star::star: 3.7 Art[1] Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (2015). “Inceptionism: Going Deeper into Neural Networks“. Google Research. [html] (Deep Dream):star::star::star::star: [2] Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. “A neural algorithm of artistic style.” arXiv preprint arXiv:1508.06576 (2015). [pdf] (Outstanding Work, most successful method currently) :star::star::star::star::star: [3] Zhu, Jun-Yan, et al. “Generative Visual Manipulation on the Natural Image Manifold.” European Conference on Computer Vision. Springer International Publishing, 2016. [pdf] (iGAN) :star::star::star::star: [4] Champandard, Alex J. “Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks.” arXiv preprint arXiv:1603.01768 (2016). [pdf] (Neural Doodle) :star::star::star::star: [5] Zhang, Richard, Phillip Isola, and Alexei A. Efros. “Colorful Image Colorization.” arXiv preprint arXiv:1603.08511 (2016). [pdf] :star::star::star::star: [6] Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. “Perceptual losses for real-time style transfer and super-resolution.” arXiv preprint arXiv:1603.08155 (2016). [pdf] :star::star::star::star: [7] Vincent Dumoulin, Jonathon Shlens and Manjunath Kudlur. “A learned representation for artistic style.” arXiv preprint arXiv:1610.07629 (2016). [pdf] :star::star::star::star: [8] Gatys, Leon and Ecker, et al.”Controlling Perceptual Factors in Neural Style Transfer.” arXiv preprint arXiv:1611.07865 (2016). [pdf] (control style transfer over spatial location,colour information and across spatial scale):star::star::star::star: [9] Ulyanov, Dmitry and Lebedev, Vadim, et al. “Texture Networks: Feed-forward Synthesis of Textures and Stylized Images.” arXiv preprint arXiv:1603.03417(2016). [pdf] (texture generation and style transfer) :star::star::star::star: 3.8 Object Segmentation[1] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation.” in CVPR, 2015. [pdf] :star::star::star::star::star: [2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. “Semantic image segmentation with deep convolutional nets and fully connected crfs.” In ICLR, 2015. [pdf] :star::star::star::star::star: [3] Pinheiro, P.O., Collobert, R., Dollar, P. “Learning to segment object candidates.“ In: NIPS. 2015. [pdf] :star::star::star::star: [4] Dai, J., He, K., Sun, J. “Instance-aware semantic segmentation via multi-task network cascades.” in CVPR. 2016 [pdf] :star::star::star: [5] Dai, J., He, K., Sun, J. “Instance-sensitive Fully Convolutional Networks.” arXiv preprint arXiv:1603.08678 (2016). [pdf] :star::star::star:","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/tags/NLP/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yezhejack.github.io/tags/Deep-Learning/"}]},{"title":"Deep Learning","slug":"Deep Learning","date":"2016-10-21T09:11:00.000Z","updated":"2016-10-23T17:16:24.000Z","comments":true,"path":"2016/10/21/Deep Learning/","link":"","permalink":"http://yezhejack.github.io/2016/10/21/Deep Learning/","excerpt":"","keywords":null,"text":"《Deep Learning》by LeCun&amp;Bengio&amp;Hinton介绍这是自然杂志上的一篇关于深度学习的论文，主要讲了深度学习如何替代传统的机器学习方法来实现人工智能的系统。 而文章的三个作者都是DL领域的鼻祖级人物。 有监督的学习“浅层”的分类器在面对一个分辨图片中狼的种类的任务的时候，可能会因为狼在图片中的位置、姿势相近而导致分类错误。这也是为什么一个“浅层”的分类器需要提取好的特征来进行学习。 为了让分类器的性能更强，可以使用普通的非线性特征，如同核方法。但是泛化的特征，例如那些和高斯核方法一同出现的特征并不会让学习器在离训练样本很远的时候有很好的泛化能力。传统的方法是手工构造特征提取器，这就要求相当的工程技巧和特定域的专业知识。但是如果好的特征可以在一个普适的学习过程中被自动地学习到，那么这些都是可以避免的（大量的工程技巧和领域内的专业知识）。而这个正是深度学习的关键优势。 深度学习的每一层都会转换它的输入，使之更具选择性和不变性。带有多层非线形层的系统（一般有5到20的深度）就可以实现非常复杂的函数，同时对微小的细节保持敏感性，除此之外还会对大的无关的变量保持不敏感，比如背景、姿势、灯光和周围的物体。 在多层结构中运用反向传播算法来训练现在最流行的非线形函数是rectified linear unit(ReLU)，简单来说就是个半波整流器$f(z)=max(z,0)$。在过去的几十年，神经网络还使用了$tanh(z)$或$\\frac{1}{1+exp(-z)}$。但是ReLU在多层网络中学习的速度更快，允许训练一个深度有监督的网络而不需要无监督的预训练。 在90年代，神经网络和后向传播被机器学习社区所背叛，并且被计算机视觉和语音识别社区所忽略。那时候大家普遍认为使用很少的先验知识学习有用的、多阶段的、特征提取的方法是不可行的。特别是，简单的梯度下降法被认为会陷入到局部最小。 实际上，差劲的局部最小在大的网络工作的时候几乎不是个问题。同时，最近的许多理论和经验结果强有力地证明了局部最小总的来说不是一个严重的问题。 在2006年左右，对于深度前馈网络的兴趣又兴起了，主要是一组由Canadian Institute for Advanced Research(CIFAR)组织到一起的研究者。 当数据集比较小的时候，无监督的预训练是需要的。 卷积神经网络ConvNets被设计用来处理那些数据的输入格式是多个数组的，比如一个由三个二维数据组成的图片。一维数据的有信号和序列，包括语言；二维的有图片和语音图谱；三维的有视频和立体图片。卷积网络后有四个关键点导致他的高性能：局部连接、共享权值、pooling以及多层结构。 www.deeplearningbook.org是一个深度学习教材的网站。 一个一维的卷积神经网被叫做时间延迟神经网，可以用于识别音素和简单词语。 用深度卷积神经网进行图像理解自然语言处理学习一个word vector来份不是表示每个单词，通过预测下一个词是什么。 循环神经网络BP首先被介绍的时候，它最激动人心的应用是用来训练RNN网络。他可以保存一定的记忆，同时在此基础上可以修改成为LSTM，拥有一个更长久的记忆。","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Reading Note","slug":"Reading-Note","permalink":"http://yezhejack.github.io/tags/Reading-Note/"}]},{"title":"Hopfield Neural Network","slug":"Hopfield Neural Network","date":"2016-10-18T03:17:00.000Z","updated":"2016-10-21T08:27:55.000Z","comments":true,"path":"2016/10/18/Hopfield Neural Network/","link":"","permalink":"http://yezhejack.github.io/2016/10/18/Hopfield Neural Network/","excerpt":"","keywords":null,"text":"Hopefield网络简介Hopefield网络是一个非同步的神经网络，是一个异步的，跟BP网络不同。同时它分为两种类型，离散型和连续型。 同步与异步网络正确设计一个递归神经网络的问题在于所有计算元素之间的充分的同步。在MP模型的网络中，我们假设每个计算元素被激活都需要消耗一定的时间。这里将要讨论的是没有全局同步的神经网络。计算节点会在不同的时间被激活，并且会在一定的时间后提供计算，这个网络是随机自动的。 双向联想记忆在循环神经网络中(recurrent neural network)，第一层的输出的除了传到第二层之外，还会从第二层再传回自己，从而作为自己的输入。在这个联想记忆的模型中，我们可能会问这个网络在经过了多次迭代之后，是否会达到一个稳定的状态。这样的一个网络叫做谐振网络(resonance network)或者是双向联想记忆(bidirectional associative memory, BAM)。单元的激活函数是符号函数，并且信息被编码成二值。下图的这个网络将一个n维的行向量$x_0$映射到一个k维的行向量$y_0$。我们定义一个$n \\times k$的权值矩阵$W$，从而让计算的第一步可以写成 $y_0=sgn(x_0 W)$ 在反馈步骤中$y_0$被当作了输入，那么新的计算就变成了 $x_1^T=sgn(Wy_0^T)$ Energy Function 能量函数动力学行为前馈网络具备了很强的非线性映射能力，从计算的观点看，前馈型神经元网络大部分是学习网络而不具有动力学行为。反馈式网络是通过网络神经元状态的变迁而最终稳定于某一状态，从而得到联想存储或者神经计算的结果。 在反馈式网络中，所有节点（单元）都是一样的，它们之间可以相互连接，所以反馈式神经元网络可以用一个无向的完备图来表示。 网络接受一个输入，然后网络将不断演化，最终趋于一个定态，称为状态空间中的不动点吸引子，这个定态便是网络的输出图形（矢量）。 前馈网络和反馈式网络的比较前馈型神经网络取连续或离散变量，一般不考虑输出与输入在时间上的滞后效应，只表达输出与输入的映射关系。反馈式网络也可使用离散变量也可使用连续取值，考虑输入与输出之间在时间上的延迟。因此需要用动态方程（差分方程或微分方程）来描述神经元和系统的数学模型。 Hopefield计算过程快、收敛速度快，与电子电路存在明显的对应关系，使得该网络易于理解和易于用硬件实现。 一些结论离散型Hopfield神经元网络在不同工作方式下的性能有以下一些结论（均假设$\\theta=0$，这并不失一般性）： 若权矩阵为对称阵，而且对角线元素非负，那么网络在异步方式下必收敛于一个稳定状态。 若权矩阵为对称阵，网络在同步工作方式下必收敛到一个稳定状态或者周期为2的极限环。 若权矩阵为正交投影矩阵，那么在同步工作方式下必收敛到一个稳定状态。 在稳定性分析中，同步方式工作的神经元网络可以等价于另一个异步方式工作的神经网络。 联想的原理自联想记忆设在学习过程中存入M个样本${X(l),l=1,2,...,M}$，使用时要求：若输入$X^l=X^a+V$，其中$X^a$是M个学习样本之一，V是偏差项（可以代表噪声、图形的缺损、畸变），要求输出为$y=X^a$，即使它复原。 异联想记忆规定两组样本之间有一定的对应关系$X^i-&gt;y^i,l=1,2,...,M$，使用时若输入$X^l=X^a+V$（含义同上），要求输出$y=y^a$。 人脑的联想方式人脑中对给出一种事物得出其对应的事物的途径有两种形成方式。一种是按时间顺序对相关事情进行思考，例如通过时间安排表来回忆某一阶段的工作，另一种就是通过事物本质特征的对比来确认事物的属性，从提示信息或者局部信息对事物进行回忆或确认。这两种基本方式抽象成计算技术中按地址寻找和按内容寻找两种探索方法。 网络运行过程 学习 形成W，与输入的样本有关系 初始 $v(0)=x$ 运行 $v(t+1)=sgn(v(t)W)$ 稳定输出 $v=x^k$ 学习规则有很多的学习方法，最常见的是Hebb学习规则：设有n个神经元互相连接，每个神经元的激活状态$x_i$只能取0或1，分别表示抑制和兴奋，学习过程中$w_{ij}$调节原则是，若i与j两个神经元同时处于兴奋状态，那么它们之间的连接应加强，即 $\\Delta w_{ij}=\\alpha x_i x_j,\\alpha &gt; 0$ 具体实现是外积规则，对于多个模式的学习，对给定的一组向量$M={U_1,U_2,...,U_m}$，外积规则可写为 $W=\\sum_{k=1}^{m}(U_k U_k^T-I),I是n \\times n单位阵$需要注意的是这事不带自环的Hopfield网络的情况，就是$w_{ij}=0$。若网络连接强度$w_{ij} \\neq 0$，则称为有自环的网络。由于不带自环的Hopfield网络稳定性易于保证。对于特定的网络规模，存在模式数目的上限，当学习的模式数大于这个上限的时候，网络不但无力记忆以后输入的模式，而且对以前的记忆也渐渐遗忘了。## 旅行商问题 ##### 问题介绍 ###旅行商问题是一个最优路径问题（简称为TSP问题），是人工智能中的一个典型问题。假定有m个城市的集合${C_1,C_2,...,C_n}$，它们之间的相互距离分别是$d_{ij}(d_{ij}=d_{ji})$。试找出一条经过每个城市仅一次的最短而且回到开始的出发地的路径。传统的穷举法会导致传统的串行计算机难以在有限时间内得到圆满解决的方案。用Hopfield网来解决TSP问题避免了计算复杂性太大的问题，因为它解决这种问题时体现了人脑的一些特征。使用一个$n \\times n$神经元，用神经元的状态来表示某一个城市在某一条有效路径中的位置，例如神经元$x_i$的状态用$v_{x_i}$表示，用$C_i$表示在路径中的第i个城市。如果$v_{x_i}=1$则说明$C_x$在路径中第i个位置出现，如果这个值为0则说明在路径中的第i个位置不出现，也就是说明此时第i个位置上是其他城市可见v矩阵可以表示n个城市的TSP问题，它的大小是$n \\times n$。为了保证每个城市只去一次（当然不包括初识出发城市），那么关联矩阵v上每一行只能有一个为1，其他为0。列上的限制也是一样的。为了解决TSP问题，必须构成这样的网络：在网络运行时，计算能量降低。网络稳定后其输出状态代表城市被访问的次序，即构成上图所示的换位阵。网络能量的极小点对应于最佳或者较佳路径的形成。为了保证输出换位阵，因此有行约束条件$E_1=\\frac{A}{2}\\sum_x\\sum_i\\sum_{j \\neq i}v_{x_i}v_{x_j}$ 其中A&gt;0为常数。$E_1$保证当矩阵v的每一行不多于一个1时，$E_1$达到最小$E_1min=0$。 同理构成列约束条件 $E_2=\\frac{B}{2}\\sum_i\\sum_x\\sum_{y \\neq x}v_{x_i}v_{y_i}$ 其中B&gt;0为常数，也保证了当矩阵v的每一列不多于一个1时，$E_2$达到最小$E_2min=0$。 构成全局约束 $E_3=\\frac{C}{2}(\\sum_x\\sum_iv_{xi}-n)^2$其中C&gt;0为常数。$E_3$保证当矩阵v中的1的个数恰好为n时即整个矩阵有n个1时，$E_3$达到最小$E_3min=0$ $E_1,E_2和E_3$只和达到最小时，能保证网络输出状态矩阵v构成换位阵。","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Neural Network","slug":"Neural-Network","permalink":"http://yezhejack.github.io/tags/Neural-Network/"}]},{"title":"setuptools教程","slug":"setuptools教程","date":"2016-10-11T13:08:36.000Z","updated":"2016-10-14T08:25:20.000Z","comments":true,"path":"2016/10/11/setuptools教程/","link":"","permalink":"http://yezhejack.github.io/2016/10/11/setuptools教程/","excerpt":"","keywords":null,"text":"setuptools 教程基本使用在project目录下创建一个setup.py的文件，其中的内容是 1234567891011121314from setuptools import setup, find_packages setup( name = \"ConvertSubToUTF8\", version = \"0.0.1\", packages = find_packages(), scripts=['ConvertSubToUTF8.py'], install_requires=['chardet&gt;0'], author='Zhe Ye', author_email='yezhejack@gmail.com', description='This is a useful tool for convert sub file code format', keywords='convert sub subtitles files utf8', url='http://yezhejack.github.io/2016/10/11/ConvertToUTF8/', license='PSF',) 然后在目录下运行python setup.py sdist 然后就可以看到目录出现了新的东西","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yezhejack.github.io/tags/python/"}]},{"title":"ConvertToUTF8","slug":"ConvertToUTF8","date":"2016-10-11T12:59:33.000Z","updated":"2016-10-11T12:59:33.000Z","comments":true,"path":"2016/10/11/ConvertToUTF8/","link":"","permalink":"http://yezhejack.github.io/2016/10/11/ConvertToUTF8/","excerpt":"","keywords":null,"text":"This is a small tool for myselfif you have any questions, you can comment on this page","raw":null,"content":null,"categories":[{"name":"My Projects","slug":"My-Projects","permalink":"http://yezhejack.github.io/categories/My-Projects/"}],"tags":[{"name":"Tool","slug":"Tool","permalink":"http://yezhejack.github.io/tags/Tool/"}]},{"title":"numpy常用语句","slug":"numpy常用语句","date":"2016-09-29T07:18:59.000Z","updated":"2017-03-16T10:15:58.000Z","comments":true,"path":"2016/09/29/numpy常用语句/","link":"","permalink":"http://yezhejack.github.io/2016/09/29/numpy常用语句/","excerpt":"","keywords":null,"text":"numpy常用语句矩阵以及向量操作初始化一个向量或者矩阵12import numpy as nptheta=np.array([[1,1,1],[1,1,1]]) 上面的代码初始化了一个2x3的矩阵。 1np.ones((3,4),dtype=int16) 上面的代码初始化了一个3x4的全一矩阵，同时指定了它的类型是int16 转置numpy.transpose() 矩阵乘法12A.dot(B)np.dot(A,B) enp.exp(A) 生成随机数1A=np.random.random((2,3)) 开方1np.sqrt(A) index12b[2,3]b[0:5,1] #这个会获得一个横向量 变换形状1a.reshape(3,-1) 如果某个维度的参数出现了-1，那么这个维度的长度是会自动计算的 逆矩阵比如a是一个矩阵，然后a.I会因为维度太大而报错，因此可以使用下面的替代。1np.linalg.inv","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yezhejack.github.io/tags/Machine-Learning/"}]},{"title":"rsync","slug":"rsync","date":"2016-08-23T11:23:36.000Z","updated":"2016-08-25T17:20:08.000Z","comments":true,"path":"2016/08/23/rsync/","link":"","permalink":"http://yezhejack.github.io/2016/08/23/rsync/","excerpt":"","keywords":null,"text":"rsync安装3.1.0版本的rsync为了显示进度条，我需要把我的路径下的rsync更新到3.1.0的版本1wget https://download.samba.org/pub/rsync/src/rsync-3.1.2.tar.gz 如果上述链接失效，则需要到https://rsync.samba.org上 我的使用方法12rsync -a --progress -h -e 'ssh -p 55555' --partial zye@202.118.250.16:/data/ltp/ltp-models backup/rsync -a --progress -h -e 'ssh -p 55555' --partial backup rsync@180.168.92.130:/volume1/hpc_data 如果在源地址后面加上斜杠的话，就只是表示拷贝下面的内容 介绍rsync是一个快速的、多用途的文件拷贝工具。它可以实现本地的、远程的。两种远程方式，一个是通过rsync服务，一个是通过ssh服务。它提供了很多的选项来控制它的每一种行为和定义需要拷贝的文件。它因为它的增量拷贝算法而著名。广泛用于备份和镜像，并且作为一个改进的拷贝命令作为日常使用。 rsync可以找得到需要传输的文件，这是基于一个快速的检查算法（默认），它检查那些大小和最后修改时间有变化的文件。其他有变化的被保留的属性也会作用到目标文件上，如果快速检查表明文件的数据并不需要更新。 一些特点 支持拷贝链接、设备、所有者、所有组和权限 排除机制 一个版本控制可以忽略的机制 可以使用shell来传输，比如ssh rsh 不需要超级用户权限 管道传输，减少延迟消耗 支持匿名或者验证的rsync守护进程 总体rsync可以把文件拷到或从远程主机拷贝，或者只在本地拷贝。（它无法在两个远程主机之间拷贝文件）。 rsync有两种方法和远程系统通讯：使用一个远程shell程序作为传输的工具（比如ssh或rsh）或者通过TCP协议直接和rsync守护进程连接。远程shell传输在主机名后面使用单个（:）的时候，远程shell传输会被使用。当使用（::）的时候或者用rsync:// URL的时候才会直接使用rsync守护进程来通讯。 如果没有目的地址的话，文件只会被列出来，像ls -l一样。 同样的，如果源地址和目的地址都不是远程主机上的话，那么拷贝的行为只会发生在本地。 rsync的本地是作为客户端的，远程端是作为服务器端。不要将服务器和一个rsync守护进程搞混了。一个守护进程肯定是一个服务，但是一个服务可以是一个守护进程也可以是一个远程shell生成的一个进程。 设置一旦安装好了之后，你就可以对任意你能够通过远程shell登陆的主机使用rsync（包括那些你能用rsync守护进程模式协议登陆的主机）。对于远程传输，一个现代的rsync使用ssh来进行通讯，但是它可以被配置去使用一个别的远程shell，比如rsh或remsh 你能指定任何你喜欢的远程shell，可以通过使用-e这个命令行选项或者设置RSYNC_RSH环境变量。 注意，rsync必须在源主机和目的主机上都安装好。 使用你可以按照rcp的方法来使用rsync，指定一个源地址和目的地址 1rsync -t *.c foo:src/ 这个会把同模式*.c匹配的文件都拷贝到机器foo去。如果有个文件已经有了，那么rsync远程更新的协议就会涌来更新这个文件。 1rsync -avz foo:src/bar /data/tmp 这个命令可以将src/bar这个文件夹下的所有内容都传输到/data/tmp/bar文件夹中。这些文件是在archive模式下传输的，这个会保证软链接、设备、属性、权限和所属等都被保留下来。另外，压缩也会在传输中被使用。 1rsync -avz foo:src/bar/ /data/tmp 相比于上面多出现的一个斜杠可以避免拷贝的时候在目的地多出现一层文件结构。你可以这样来认为，多加一个斜杠在源地址表明了拷贝这个目录的内容，而不是按照名字拷贝目录，但是无论哪种，这些文件夹的属性都会用相同的方式传输过去。下面两个命令做的事情是一样的 12rsync -av /src/foo /destrsync -av /src/foo/ /dest/foo 但是对于指向主机或者模块的地址，并不要求一个斜杠来拷贝默认文件夹的内容，比如，下面两个例子就都是把文件的内容拷贝到/dest中: 12rsync -av host: /destrsync -av host::module /dest 你可以使用本地模式的rsync，就是其中的两个地址都不带有:。就像一个改进过的拷贝命令。 1rsync somehost.mydomain.com:: 这个可以列出所有可用的模块。 高级用法可以给出多个源地址，和前面host相同的可以忽略 123rsync -av host:file1 :file2 host:file&#123;3,4&#125; /dest/rsync -av host::modname/file&#123;1,2&#125; host::modname/file3 /dest/rsync -av host::modname/file1 ::modname/file&#123;3,4&#125; 老版本的rsync要求对于源地址加上单引号 如果需要传输一个带有空格的文件名，你可以使用--protect-args(-s)选项或者你可以加\\。比如 1rsync -av host:`file\\ name\\ with\\ spaces` ／dest 连接一个rsync守护进程这个时候使用的是一个监听873的TCP接口，这个当然需要守护进程在远程服务器上运行着。 你可以试使用双冒号来代替但冒号来分割主机名和路径，或者使用rsync://URL。 path的第一个词是一个模块名字 远程守护进程能够打印一个当天的信息，当你连接的时候 如果没有指定路径名字，那么就会列出能够访问的路径 如果没有指定目的地址，那么就会列出匹配的文件 不能指定--rsh(-e)选项 一个拷贝远程主机中模块src中的所有文件的例子 1rsync -av host::src /dets 如果需要认证的话，你会被要求输入密码。你可以设置一个RSYNC_PASSWD的环境变量来避免这个过程。或者使用--password-file，这个方法对于在脚本中使用rsync有用。 在某些系统中，环境变量对所有用户都是可见的，在这种系统中，推荐使用--password-file。 设置环境变量RSYNC_PROXY来进行代理，格式为hostname:port。这个代理必须能够支持对端口873的连接。 可以通过设置环境变量RSYNC_CONNECT_PROG为代理命令。 通过远程shell连接使用rsync-daemon功能rsync支持使用一个远程shell连接，然后启动一个守护服务，这个服务会读取在home地址下的配置文件。这个主要是可以加密那些daemon-style的传输数据。另一种加密一个守护传输的，可以考虑使用ssh来加密一个本地借口到远程机器，然后配置一个rsync服务，只允许来自localhost的连接。 需要明确地设置远程shell程序，用--rsh==COMMAND选项。在环境变量中设置RSYNC_RSH并不会打开这个功能。例如 1rsync -av --rsh=ssh host::module /dest 可以指定ssh的用户 1rsync -av -e \"ssh -l ssh-user\" rsync-user@host::module /dest ssh-user会被用于ssh的登陆，而rsync-user会被用于模块的登陆 可以对传输顺序排序rsync会把指定的文件名放入它内部的传输列表中，并排序。这个可以快速地筛除同名文件，会跟用户给出的文件顺序不同。 选项 --msgs2stderr 这个选项会把所有信息输到stderr，如果没有--remote-option 比如-M--msgs2stderr -I --ignore-times 同样大小的，但是时间不同的也会被略去 --no-motd会影响使用rsync host::来列出所有模块，未理解 --size-only只看文件大小来确定文件是否需要传输 -@,--modify-window默认是0，是看两个时间戳之间的差值，小于这个选项所给的值则认为相等。如果是正数则表示的单位是秒，如果是负数，表示的单位是纳秒。对于MS Windows FAT文件系统，需要设置为1. -c --checksum 确保文件是被修改过的，而不是采用快速检查修改时间和大小。 -a --archive 是一个快速模式，告诉rsync基本上你什么都想保留，除了硬连接。 --no-OPTION用来关闭前面已经生效的选项，只可以是那些被整合进别的选项的选项，或者实在不同时候有不同默认值的选项。可以用长或短的格式，比如--no-R和--no-relative是一样的。这个选项的顺序是非常重要的。比如要关闭-a中带来的-r，就得把--no-r放在-a的后面，否则无法关闭。 -r --recursive 可以递归地传输目录，在3.0.0版本会使用一个增量的扫描方法，这个方法减少了非常多的在传输前所需要的内存空间。并且传输在扫描了一些文件夹之后就开始了。这个增量扫描只影响了递归算法，对于非递归的不影响。如果想要使用的话必需保证传输的两端的rsync版本是3.0.0。 -R --relative 可以保留路径的信息，比如rsync -av /foo/bar/baz.c remote:/tmp/会在/tmp/下新建一个叫做baz.c的文件，但是rsync -avR /foo/bar/baz.c remote:/tmp/则会在/tm//foo/bar建立一个新的baz.c --no-implied-dirs 会影响-R的默认行为。当这个被打开的时候，前面被提到的目录的属性不会被传输过去。如果有新的目录被创建，那么就会是默认的属性，已有的目录的属性不变。 -b,--backup这个选项之前已经存在的目标文件会被转移走或者被删除。 --backup-dir=DIR它告诉--backup选项把备份存在哪里，这可以用于增量备份，但是这个不可以跑到模块外面去。 --suffix=SUFFIX用来表示--backup的前缀 -u,--update这个会跳过所有已经存在的并且修改时间更新的。 --inplace 可以将数据直接写到文件的位置上，但它有几个影像，一个是硬连接会被保留，二是在使用的二进制包不能被改写，三是文件容易出现不一致。一个rsync无法写的文件将不会被更新。 --append 这个会用追加的方式来更新文件，但是对于那些可能不是从尾部更新的文件，这个会导致错误。 --append-verity会校验一下追加更新的文件，如果校验失败，则回退。* --chmod可以用来修改文件的读写权限。 --owner这个选项可以用来保证传过去的文件的所有者和源文件是一样的 --super这个选项会告诉接收端尝试使用超级用户权限。 --existing,--ignore-non-existing是告诉rsync跳过需要创建的文件 --ignore-existing用来告诉rsync跳过那些已经存在的文件 --temp-dir=DIR 可以设置一个临时的文件夹来存储正在拷贝的文件，有利于在磁盘空间不够的时候，将临时文件设置在另一个磁盘上。 --partial-dir=DIR 设置一个临时文件夹用于断点续传，但是它并不开启断点续传 --partial开启断点续传 --delay-updates当文件全部传完了之后再把文件放到目的位置，这样可以使操作显得原子。这个选项与--partial冲突。 --info=FLAGS可以显示特定的信息，比如--info=progress2可以显示整体传输的进度。","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yezhejack.github.io/tags/Linux/"},{"name":"rsync","slug":"rsync","permalink":"http://yezhejack.github.io/tags/rsync/"}]},{"title":"非递归遍历二叉树","slug":"非递归遍历二叉树","date":"2016-08-16T17:11:06.000Z","updated":"2016-08-17T11:02:30.000Z","comments":true,"path":"2016/08/17/非递归遍历二叉树/","link":"","permalink":"http://yezhejack.github.io/2016/08/17/非递归遍历二叉树/","excerpt":"","keywords":null,"text":"非递归遍历二叉树先根遍历对于递归来说，有一个很大的特征，就是先访问到的函数需要越晚结束掉。因此这就对应上栈这种数据结构。对于先根遍历来说递归函数是这样写的 12345dg(root)&#123; fangwen(root); dg(root-&gt;left); dg(root-&gt;right);&#125; 每次都需要把root入栈，因为只有这样才能找到左右儿子。因此对于当前节点P，先对其进行访问，然后把P入栈，然后再去访问P的左儿子，等到左儿子全都访问完毕后，会从栈里弹出P，从而找到右儿子。因此，有左儿子的情况下，需要把P入栈（之前访问过了），然后将P设为左儿子，从而进入左子树。 如果没有左儿子的话，就去看右儿子，由于已经找到右儿子，所以就不需要把当前节点P入栈了，直接将P设成当前节点的右儿子。 如果没有右儿子的话，则继续弹出，直到有右儿子为止，或者栈弹空了。 中根遍历递归的写法 12345dg(root)&#123; dg(root-&gt;left); fangwen(root); dg(root-&gt;right);&#125; 可以看出来，我们将root也就是P放进栈，是为了到时候拿出来进行访问，并且需要从而进入右儿子。 对于当前节点P，如果它有左子树的话，则将其入栈，并且将P=P-&gt;left 如果它没有左子树，则对其进行访问。如果有右子树，则将P设置为P=P-&gt;right。如果也没有右子树的话，则从栈里再弹出一个元素，并设置为P。 后根遍历递归的写法12345dg(root)&#123; dg(root-&gt;left) dg(root-&gt;right) fangwen(root);&#125; 入栈是为了找到右子树和访问中间节点。因此只有第二次出现在栈顶的时候（进去的时候不算）才能被弹出，第一次是为了找到右节点，第二次是为了访问。 因此对于当前的P，如果它有左子树，则将其P-&gt;record=0，然后P=P-&gt;left，如果没有左子树，则将其P-&gt;record=1，然后P=P-&gt;right。record=0表示还没出现在栈顶过，如果是1则表示左边已经不需要考虑，发现等0则不需要弹栈，如果等1，则需要弹栈 无左有右的情况下，需要把record设为1，然后将当前节点设置为P=P-&gt;right 无左无右的情况下，直接访问当前节点，然后弹出一个。如果P-&gt;record=1，则访问这个P，然后接着弹，直到一个P-&gt;record==0或者弹空，等0的时候，则将其设置为1，再放入栈里，然后再将P设置为它的右儿子。","raw":null,"content":null,"categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://yezhejack.github.io/categories/leetcode/"}],"tags":[{"name":"Tree","slug":"Tree","permalink":"http://yezhejack.github.io/tags/Tree/"}]},{"title":"pyspider爬虫框架源码阅读","slug":"pyspider爬虫框架源码阅读","date":"2016-07-30T13:53:58.000Z","updated":"2016-08-02T12:19:39.000Z","comments":true,"path":"2016/07/30/pyspider爬虫框架源码阅读/","link":"","permalink":"http://yezhejack.github.io/2016/07/30/pyspider爬虫框架源码阅读/","excerpt":"","keywords":null,"text":"pyspider爬虫框架源码阅读主要使用的组件tornado一个异步可并发的网络库 有4个部分组成 Web Framework，RequestHandler HTTP的客户端和服务端 HTTPServer AsyncHTTPClient 一个异步的网络库 IOLoop IOStream 一个协程库 tornado.gen flask用于搭建web server phantomjs用于爬取js页面 click用于建立一个好的命令行接口的库 1@click.command() 表示这是一个子命令。 1@click.option 表示一个命令行的option 1@click.option('--shout/--no-chout',default=False) 表示这是一个boolean flag 12@click.command()@click.option('--hash-type',type=click.Choice(['md5','sha1'])) 这两个装饰器会对hash-type这个参数进行检查，如果不是之一的话，会报错。123456#### prompt ####当用户没有指定一些参数的时候，让程序去追问用户，叫做prompt parameters#### 从环境变量中读取值 ####有两种方式，一种是从环境变量中读取自定义的 @click.command()@click.option(‘–username’)def greet(username): click.echo(‘hello %s!’ % username) if name==’main‘: greet(auto_envvar_prefix=’GREETER’)12然后在命令行中 $ export GREETER_USERNAME=john$ greethello john1#### 可变参数 #### @click.command()@click.argument(‘src’,nargs=-1)@click.argument(‘dst’,nargs=1)123这样子src就拥有了可变参数的特性，可以吃掉任意多的参数。但是只能有一个可变参数存在。如果想要至少提供一个参数的可变参数的话，需要设置```requeired=True 文件安全提供了lazy mode和atomic mode来保证文件的读写安全，lazy模式下，读文件会得到立即的反应，而写文件则会到第一次IO操作的时候才有反应。 复杂应用上下文环境 context当一个click命令执行的时候，一个12环境对象建立一个链接表，直到他们到达了最上面的一个。每个环境都会和它的父环境链接。这样子，每个环境对象都能够保存自己的信息，而不用担心会影响到其他命令的状态了。同时也保证了当父环境的数据需要读取的时候，可以到达。 @click.pass_context1可以将上下文传输过来 @click.pass_obj则是只传递Context.obj```字段过来。 当使用了这两个装饰器之后，被装饰的函数的第一个参数就是对应的Context或Context.obj run.py在./pyspider/run.py中从cli()函数开始运行,看之前最好看一下click的使用，因为整个结构是基于click的。 lambda 匿名函数这个是用来绑定一个匿名函数的","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yezhejack.github.io/tags/爬虫/"},{"name":"pyspider","slug":"pyspider","permalink":"http://yezhejack.github.io/tags/pyspider/"}]},{"title":"YARN","slug":"YARN","date":"2016-06-24T08:19:33.000Z","updated":"2016-06-25T19:06:52.000Z","comments":true,"path":"2016/06/24/YARN/","link":"","permalink":"http://yezhejack.github.io/2016/06/24/YARN/","excerpt":"","keywords":null,"text":"YARN简介YARN在hadoop 2中被引入，用于改善MapReduce，但是它也被用于支持别的计算框架。YARN提供API用于请求和使用集群资源，但这些不是直接被用户的代码使用的。 如何工作的YARN通过两种守护进程来提供它的核心服务，一个是resource manager，另一个是node managers。后者会启动和监控containers。一个container会执行一个特定程序的进程，同时包括一系列的受限制的计算资源。一个container可以是一个unix的进程，或者是linux的cgroup。 本地化是可以让集群的带宽的利用率达到最大。通常情况下，会在存有数据的node上启动，再不行就在同一个rack上启动，再不行就随机任意一个node。 Spark是事先就申请好固定的资源用于计算，而MapReduce则是动态地，先为map申请资源，然后再为reduce申请资源。 应用生命周期 一种模型是每个用户job一个应用，MapReduce采用的就是这样 第二个是每个流程或用户对话一个应用，这个更高效，因为container可以复用，Spark采用的是这种。 第三种模型是长期运行的、被用户共享的应用。这种应用的好处在于反应快。 建立YARN应用Slider可以用来直接启动已有的应用在YARN上。 Apache Twill也是类似于Slider。 MapReduce 1中有jobtracker和多个tasktracker来控制job的执行，其中jobtracker负责任务的调度以及任务的监控（失效重启等）。 MapReduce 1中的jobtracker对应着yarn中的 Resource Manager、application master和timeline server，而tasktracker对应着node manager，slot对应着Container YARN最大的好处在于它不再只为MapReduce使用，它可以为很多别的计算框架提供服务。 YARN调度提供了三种scheduler FIFO Capacity Fair Scheduler 但是FIFO会导致小任务被大任务阻挡，Capacity Scheduler有专用队列，每个队列有它的容量，这个容量表示它所能给队列中的任务分配的最大计算资源。但是在有空余资源的情况下，系统是有可能给队列中等待的资源分配一个大于它的容量的计算资源。它不会通过杀死其他Container来抢占资源，所以当自己的计算资源被弹性队列机制拿走的时候，它也只能等着别的队列用完之后换给它。为了避免这个情况，是可以配置一个最大容量，如果没配置则有可能会用光这一级的所有资源。 Fair Scheduler则是让每次新来的任务都从前一个任务中分出一半的资源。 延迟调度为的是能够增大在本地运行的机会","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yezhejack.github.io/tags/hadoop/"},{"name":"big data","slug":"big-data","permalink":"http://yezhejack.github.io/tags/big-data/"}]},{"title":"HDFS","slug":"HDFS","date":"2016-06-23T02:49:58.000Z","updated":"2016-06-25T19:06:53.000Z","comments":true,"path":"2016/06/23/HDFS/","link":"","permalink":"http://yezhejack.github.io/2016/06/23/HDFS/","excerpt":"","keywords":null,"text":"HDFS 学习HDFS文件系统具有高容错率，每个block的size为128MB，为的是减少寻址时间。整个hadoop集群中分为了Namenode和Datanode，其中Namenode保存着metadata，也就是整个HDFS文件系统的树和块信息。而Datanode则负责具体block的存取，并且周期性地向Namenode报告它所拥有的数据块。 datanode自身因为会把block复制几遍，所以已经有了容错机制了。而namenode则可以将metadata持久化到其他的文件系统中，或者运行一个secondary namenode。这个secondary namenode的主要作用是不断地合并log，避免log文件过大。但是由于这两个namenode之间还是有延迟的，所以数据的丢失是肯定的。在主要的namenode失效的情况下的做法通常是将文件系统中的metadata拷贝到secondary namenode中，让它作为primary namenode。 HDFS结合可以以有多个namenode，分管一部分namespace。 HDFS高可用性namenode失效恢复机制 装载命名空间镜像到内存 重现编辑log 接受足够多的块报告，以便离开安全模式 这个通常会花费超过30mins的时间。因此有计划的宕机更重要。Hadoop 2通过增加高可用性来解决了这个问题。通过增加一个随时待命的namenode。而这个standby的namenode也将secondary namenode的职责也承担下来了。 故障转移从激活的namenode转移到standby namenode需要failover controller。需要许多的failover controller来 fencing用于保证下线的namenode不会造成数据冲突 QJM一次只允许一个namenode往编辑log里写东西。然而，之前的namenode还是会为旧的客户请求提供服务，因此需要设置一个SSH fencing command来杀死这个namenode的进程。更激进的fencing方法用于NFS共享log。因为不可能只让每时刻只有一个namenode写log。这也是为什么QJM是被推荐的。一个方法是取消这个namenode访问共享存储目录的权限，关闭它的网络接口。最后的招数是直接击毙这个节点，也就是关闭电源。而客户端会逐个尝试配置文件中的namenode address。 命令行接口有两个属性需要设置 1fs.defaultFS=hdfs://localhost/ HDFS守护进程会使用这个东西去查找HDFSnamenode的地址和端口。HDFS clients也会通过这个查找namenode。 另一个属性是1dfs.replication=1 这样HDFS就不会讲每个块都复制三遍了。当只有单个datanode的时候，HDFS无法复制三遍，所以会不停地警告，这个设置会解决这个问题。 基本操作12hadoop fs -copyFromLocal input/docs/quangle.txt \\hdfs://localhost/user/tom/quangle.txt 如果查找不到datanode的话12sudo rm -R /tmp/*hdfs namenode -format 然后重启 hdfs client读文件过程 客户端先打开一个FileSystem对象，对于HDFS文件系统来说是DistributedFileSystem。它会返回前几个block的地址。同时这些地址，也就是datanode的地址，会根据它们的拓扑情况排序。除此之外，还会返回一个FSDataInputStream，其中包含着一个DFSInputStream。 客户端通过read()来读取文件，在DFSInputStream中保存着datanode的地址，DFSInputStream会一块一块地读取文件，但是客户端只会觉得是从一个流里面读取数据。当这一批的block都读取完了，它会向namenode请求下一批block的地址。当结束的时候，它会调用call()。 如果DFSInputStream在和datanode通讯的时候遇到了错误，那么它会记录这个datanode是坏掉的，然后会尝试别的datanode。 这个过程中namenode只是回复查询块位置的请求。 网络拓扑和Hadoop通过计算两个节点到它们最近的共同祖先的距离。 同一个节点的距离 &lt; 同一个机架的两个不同节点 &lt; 同一个数据中心，不同机架的两个节点 &lt; 不同数据中心的两个节点 hdfs client 写文件的过程 客户端在DistributedFileSystem上调用create() DistributedFileSystem向namenode调用并创建一个文件在命名空间里，但是并没有实际的blocks和它联系在一起。 namenode会运行多种检查来确保文件并不是已经存在的，并且用户有足够的权限去创建这个文件。如果检查通过了，那么namenode会为这个文件创建一个记录。如果失败了，则会抛出IOException。 成功之后，DistributedFileSystem会返回一个FSDataOutputStream，用于客户端写数据。正如读过程一样，FSDataOutputStream包着一个DFSOutputStream。它负责和datanode和namenode的通讯。 DFSOutputStream会把数据分成好几份，然后放在一个queue中，然后会有若干个datanode在等待，如果是将每个block复制三遍的话，则会想给第一datanode，然后第一个datanode再给第二个datanode，然后第二个datanode再给第三个datanode。 DFSOutputStream还会维护一个ack queue，只有当一个packet的所有datanode都表示回复了ack之后，才会从这个queue中移除。 hdfs是如何选择节点去存储一个block的它优先选一个client在的节点，如果是外部请求，则随机一个。第二个拷贝会换一个rack，第三个会放在和第二个拷贝的相同机架上，但是在不同的节点。后面的拷贝则是在集群中随机选择。 FSDataOutputStream中的hflush强制让缓存中的数据写到datanodes中，并且让其是对外可见的，但是这个只是保证到达了datanode的内存中，如果掉电了，还是会存在数据丢失的情况。hsync()是一个更强的保证，保证数据已经写到文件中了。 关闭一个HDFS中的文件，就隐式地调用了hflush hdfs的路径问题在hdfs-site.xml中设置，否则会放在tmp文件夹下，但是如果设置了tmp文件夹的话也就没什么关系了。因为tmp会定时删除。","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yezhejack.github.io/tags/hadoop/"},{"name":"big data","slug":"big-data","permalink":"http://yezhejack.github.io/tags/big-data/"}]},{"title":"最大熵模型","slug":"最大熵模型","date":"2016-06-13T13:10:00.000Z","updated":"2016-06-13T13:34:19.000Z","comments":true,"path":"2016/06/13/最大熵模型/","link":"","permalink":"http://yezhejack.github.io/2016/06/13/最大熵模型/","excerpt":"","keywords":null,"text":"最大熵模型解决的两个问题 What exactly is meant by “uniform”, and how can we measure the uniformity of a model Having determined a suitable answer to these questions, how do we go about finding the most uniform model subject to a set of constrainsts. 当没有足够的信息去判断两个事件的可能性谁更大的时候，最好的策略是将它们认为是相等的。 $p(y|x)$是一个条件概率 feature 和 constraint的区别 feature是指二元的函数 constraint则是一个等式，也就是特征函数在模型中的期望值和在训练数据中的（经验期望）的等式。","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yezhejack.github.io/tags/Machine-Learning/"},{"name":"Maxent","slug":"Maxent","permalink":"http://yezhejack.github.io/tags/Maxent/"},{"name":"Maximum Entropy Method","slug":"Maximum-Entropy-Method","permalink":"http://yezhejack.github.io/tags/Maximum-Entropy-Method/"}]},{"title":"支持向量机","slug":"支持向量机","date":"2016-06-12T12:10:00.000Z","updated":"2016-06-12T16:23:48.000Z","comments":true,"path":"2016/06/12/支持向量机/","link":"","permalink":"http://yezhejack.github.io/2016/06/12/支持向量机/","excerpt":"","keywords":null,"text":"第七章 支持向量机7.1线性可分支持向量机与硬间隔最大化7.1.1线性可分支持向量机一般的，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个，线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解时唯一的。 定义7.1（线性可分支持向量机） 超平面 $w^{*} \\cdot x+b^{*}=0$ 决策函数 $f(x)=sign(w^{*} \\cdot x +b^{*})$ 7.1.2 函数间隔和几何间隔定义函数间隔 $\\hat{\\gamma_i}=y_i(w \\cdot x_i+b)$超平面(w,b)关于训练数据集T的函数间隔为超平面(w,b)关于T中所有样本点的函数间隔的最小值。为了统一标准，引入几何间隔概念。$\\gamma_i=y_i(\\frac{w}{||w||} \\cdot x_i+\\frac{b}{||w||})$ 7.1.3 间隔最大化支持向量机学习的基本思想是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。这里的间隔最大化又称为硬间隔最大化。 这个问题可以表述为下面的约束最优化问题 $$\\max_{w,b} \\gamma \\\\ s.t. y_i(\\frac{w}{||w||} \\cdot x_i+\\frac{b}{||w||}) \\geq \\gamma,i=1,2,...,N$$","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yezhejack.github.io/tags/Machine-Learning/"},{"name":"Support Vector Machine","slug":"Support-Vector-Machine","permalink":"http://yezhejack.github.io/tags/Support-Vector-Machine/"}]},{"title":"Stanford NLP 笔记","slug":"NLPofStanford","date":"2016-06-11T10:00:00.000Z","updated":"2016-06-11T15:36:28.000Z","comments":true,"path":"2016/06/11/NLPofStanford/","link":"","permalink":"http://yezhejack.github.io/2016/06/11/NLPofStanford/","excerpt":"","keywords":null,"text":"Generative vs Discrimnative Models这是两种模型，分别是使用了Joint Prob. 和 Conditional Prob. 也就是实用P(class,data)和P(class|data)的区别。","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://yezhejack.github.io/tags/Natural-Language-Processing/"},{"name":"Coursera","slug":"Coursera","permalink":"http://yezhejack.github.io/tags/Coursera/"}]},{"title":"第五章－神经网络学习","slug":"第五章－神经网络学习","date":"2016-05-29T14:16:00.000Z","updated":"2016-10-18T03:18:17.000Z","comments":true,"path":"2016/05/29/第五章－神经网络学习/","link":"","permalink":"http://yezhejack.github.io/2016/05/29/第五章－神经网络学习/","excerpt":"","keywords":null,"text":"神经网络5.1 神经元模型M-P神经元模型：$y=f(\\sum_{i=1}^{n}w_i x_i - \\theta)$ 最后还需要一个激活函数来处理并产生神经元的输出。因为以阶跃函数作为激活函数的话，其具有不连续、不光滑等不太好的性质，因此通常使用Sigmoid函数作为激活函数。它把可能在较大范围内变化的输入值挤压到（0，1）输出值范围内，因此有时也称为挤压函数（squashing function）。 把许多个这样的神经元按一定的层次结构连接起来，就得到了神经网络。 5.2 感知机与多层网络感知机（Perceptron）由两层神经元组成，输入层接收外界输入信号后传递给输出层，输出层是M-P神经元，亦称阈值逻辑单元（thresholod logic unit）。 感知机能容易地实现与、或、非运算。这个可以通过手工设定参数$w$和$\\theta$。更一般的应该是能够给定数据集，然后权重$w_i$以及阈值$\\theta$可以通过学习得到。阈值$\\theta$可以看作一个规定输入为-1.0的哑节点(dummy node)所对应的连接权重$w_{n+1}$。 学习规则 对于训练样例$({\\bf x},y)$，若当前感知机的输出为$\\hat{y}$，则感知机权重将这样调整： $w_i \\leftarrow w_i+\\Delta w_i$$\\Delta w_i = \\eta (y-\\hat(y))x_i$ 当$y=\\hat{y}$的时候，感知机的学习结束，权重不再调整。只有当两类模式是线性可分的，即存在一个线性超平面能将它们分开，则感知机的学习过程一定会收敛（converge），但是如果不是线性可分的，例如要模拟一个异或的话，因为其线性不可分，所以无法模拟。因为感知机只有输出层神经元进行激活函数处理。 要解决非线性可分的问题，就需要使用多层功能神经网络。比如一个简单的两层感知机就能解决异或问题。输入层和输出层之间加入一层神经元，这一层被称为是阴层（隐含层 hidden layer），隐含层和输出层神经元都是拥有激活函数的功能呢神经元。 每层神经网络与下一层神经网络全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络通常称为多层前馈神经网络（multi-layer feedforward neural networks），其中输入层神经元仅接收外界输入。前馈并不意味着网络中的信号不能向后传，而是指网络拓扑结构上不存在环或回路。 5.3 误差逆传播算法误差逆传播算法（error BackPropagation）简称BP算法可以用来学习一个神经网络模型。其不仅可以用于多层前馈神经网络，还可以用于其他类型，例如训练递归神经网络，但通常说的BP网络指的是用BP算法训练的多层前馈神经网络。 对训练样例$(x_k,y_k)$来说，假定神经网络的输出为${\\bf {\\hat{y_j^k}}}=(\\hat{y}_1^k,\\hat{y}_2^k,...,\\hat{y}_l^k$即 ${\\bf {\\hat{y_j^k}}}=f(\\beta_j-\\theta_j)$ 则网络在$(x_k,y_k)$上的均方误差为 $E_k =\\frac{1}{2} \\sum_{j=1}^{l} (\\hat{y_j^k}-y_j^k)^2$ 这里的$\\frac{1}{2}$是为了后续求导方便。 网络中的参数个数，从输入层到隐层，因为只有权值，所以有$d \\times q$个权值，从隐层到输出的权值也有$q \\times l$个，然后还有 $q$个隐层的阈值，$l$个输出层的阈值。 任意参数$v$的更新估计公式为 $v \\leftarrow v + \\Delta v$ BP算法基于梯度下降策略，以目标的负剃度方向对参数进行调整。 $\\Delta w_h^j =-\\eta \\frac{\\partial E_k}{\\partial w_h^j}$ 简单推导一下 $\\Delta w_h^j =-\\eta \\frac{\\partial E_k}{\\partial w_h^j}=\\frac{\\partial E_k}{\\partial \\hat{y_j^k}} \\cdot \\frac{\\partial \\hat{y_j^k}}{\\partial \\beta_j} \\cdot \\frac{\\partial \\beta_j}{\\partial w_h^j}$ 再根据$\\beta_j$的定义（看图），显然有 $\\frac{\\partial \\beta_j}{\\partial w_h^j}=b_h$ 下面要解决前面两个偏导。我们知道理想的激活函数是Sigmoid函数 $sigmoid=\\frac{1}{1+e^{-x}}$ 这个函数有一个很好的特性 $f\\prime (x)=f(x)(1-f(x))$ 利用Sigmoid函数的特性，以及前面求得的 $$\\hat{y_j^k}=f(\\beta_j-\\theta_j) \\\\ E_k=\\frac{1}{2} \\sum_{j=1} (\\hat{y_j^k} - y_j^k)$$ 可以得到 $$g_j=- \\frac{\\partial E_k}{\\partial \\hat{y_j^k}} \\cdot \\frac{\\partial \\hat{y_j^k}}{\\partial \\beta_j} \\\\ ＝-(\\hat{y_j^k}-y_j^k)f\\prime(\\beta_j-\\theta_j) \\\\ =\\hat{y_j^k}(1-\\hat{y_j^k})(y_j^k-\\hat{y_j^k})$$ 这里我们得到了隐含层到输出层的权值的更新公式 $\\Delta w_h^j=\\eta g_j b_h$ 除此之外我们还有三种参数需要更新，分别是$\\theta v \\gamma$。 书中没有给出具体的推导，但是我们可以来自己算一下。首先是$\\theta$ $$\\begin{align} \\theta_j&amp;=\\theta_j - \\frac{\\partial E_k}{\\partial \\theta_j} \\\\ \\frac{\\partial E_k}{\\partial \\theta_j}&amp;= \\frac{\\partial E_k}{\\partial \\theta_j} \\cdot \\frac{\\partial \\hat{y_j^k}}{\\partial \\theta_j} \\\\ &amp;=-(\\hat{y_j^k} - y_j^k) \\cdot -f\\prime(\\beta_j-\\theta_j) \\\\ &amp;=(\\hat{y_j^k} - y_j^k)\\hat{y_j^k}(1-\\hat{y_j^k}) \\\\ &amp;=g_j \\end{align}$$ 所以呢 $\\Delta \\theta_j=-\\eta g_j$ 下面再说$v_ih$和$\\gamma_h$ $$\\begin{align} e_h &amp; =-\\frac{\\partial E_k}{\\partial b_h} \\cdot \\frac{\\partial b_h}{\\partial \\alpha_h} \\\\ &amp; =- \\sum_{j=1}^{l} \\frac{\\partial E_k}{\\partial \\beta_j} \\cdot \\frac{\\partial \\beta_j}{\\partial b_h} f^{\\prime}(\\alpha_h-\\gamma_h) \\\\ &amp; = \\sum_{j=1}^{l} w_h^j g_j f^{\\prime}(\\alpha_h-\\gamma_h) \\\\ &amp; = b_h(1-b_h)\\sum_{j=1}^{l} w_h^j g_j \\end{align}$$ 像$g_j$一样，它们是输出层和隐含层的梯度。 $$\\begin{align} \\Delta v_ih&amp;=\\eta e_h x_i \\\\ \\Delta \\gamma_h&amp;=-\\eta e_h \\end{align}$$ 以上就是标准BP算法。这个算法的目标是最小化当前的样例的误差，很可能出现上一个样例的更新被下一个样例所产生的更新给抵消了。 累积BP算法的目的在于最小数据集D的累积误差。具体计算方法就是把所有的$E_k$的偏导都加起来，在更新权值的时候也是。有空的时候我再做推导。现在要回宿舍睡觉了～ 在很多人任务里，会先使用累积BP算法先训练，当总体误差降得很低了之后，再使用标准BP算法进行训练，尤其是在训练集D特别大的时候效果特别好。 缓解BP网络的过拟合 早停：将数据分成训练集和验证集，但训练集误差降低，但是验证集误差上升时停止训练，返回具有最小验证集误差的参数。 正则化：将目标公式变为$E=\\lambda \\frac{1}{m} \\sum_{k=1}^{m} E_k +(1-\\lambda) \\sum_i w_i^2$ 5.4 全局最小与局部最小使用方法有： 以多组不同的参数值初始化多个神经网络，按标准方法训练后，取其重误差最小的作为最终参数。 使用模拟退火 使用随机梯度下降 遗传算法 这些方法大多都是启发式的，缺乏理论保障，不保证效果。 5.5 其他常见神经网络5.5.1 RBF网络RBF（Radial Basis Function)径向基函数``网络是一种单隐层前馈神经网络，它使用径向基函数作为激活函数，输出层则是对隐层神经元输出的线性组合。假定输入为d维向量x，输出为实值，则RBF网络可以表示成 $$\\begin{align} \\varphi(x) = \\sum_{i=1}^{q} w_i \\rho(x,c_i) \\end{align}$$ 其中q为隐层神经元个数，$c_i$和$w_\\i$分别是第i个隐层神经元所对应的中心和权重。 $$\\begin{align} \\rho(x,c_i)=e^{-\\beta_i||x-c_i||^2} \\end{align}$$ 训练步骤： 确定神经元中心$c_i$，通常使用随机采样、聚类 利用BP算法来确定参数$w_i$和$\\beta_i$ 5.5.2 ART网络竞争型学习是神经网络中一种常用的无监督学习策略，各个输出层神经元互相竞争，每一时刻仅有一个竞争获胜的神经元被激活。这种机制叫做胜者通吃（winner-take-all）原则。 ART(Adaptive Resonance Theory，自适应谐振理论)网络是竞争学习的重要代表。由比较层 识别层 识别阈值 重置模块构成。其中比较层负责接收输入的向量，然后将其传递给识别层,识别层的每个神经元代表一个模式类，神经元数目可在训练过程中动态增长以增加新的模式类。 竞争最简单的方式是计算输入向量与每个识别层神经元所对应的模式类的代表向量之间的距离，距离最小者胜。获胜神经元将向其他识别层神经元发送抑制激活信号。若相似度大于阈值，则当前输入样本将被归为该代表向量所属类别，同时网络的连接权将更新，使得以后在接收到相似输入样本时该模式类会计算出更大的相似度。若相似度不大于阈值，则在识别层增加一个新的神经元，其代表向量就是当前向量。 最早的ART网络只能处理布尔型输入数据，后来发展出了能处理实值输入的ART2网络，结合模糊处理的FuzzyART网络，以及可进行监督学习的ARTMAP网络。 5.5.3 SOM网络SOM(Self-Organizing Map)，自组织映射也是一种竞争学习型的无监督神经网络。 它能将高维输入数据映射到低维空间，通常是二维空间，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。 输出神经元以矩阵的形式排布在二维空间上，同时每个神经元都有一个代表自己的权向量，同样是距离最近的神经元成为竞争获胜者。然后最佳匹配单元及其邻近神经元的权向量将被调整，以使得这些全向量与当前输入样本的距离缩小。 5.5.4 级联相关网络这个网络多了一个学习目标，就是网络的结构。与前馈神经网络相比，级联相关网络无需设置网络层数、隐层神经元数目，且训练速度较快，但其在数据较小时易陷入过拟合。 5.5.5 Elman 网络这是一个递归神经网络（recurrent neural network），它允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号。这样的结构与信息反馈过程，使得网络在t时刻的输出状态不仅与t时刻的输入相关，还和t-1时刻的网络状态相关，从而能处理与时间有关的动态变化。 网络的训练需要通过推广的BP算法进行。 5.5.6 Boltzmann机能量最小化时网络达到理想状态。它也是一种递归神经网络，其神经元分为两层显层和隐层。显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达。并且神经元都是布尔型的。 令向量$\\bf s$是一个n维的0、1向量，表示n个神经元的状态，$w_ij$ 表示神经元i与j之间的连接权，$\\theta_i$表示神经元i的阈值。状态向量$\\bf s$的Boltzmann机的能量定义为 $$\\begin{align} E(s)=-\\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} w_{ij}s_i s_j- \\sum_{i=1}^{n} \\theta_i s_i \\end{align}$$ 其出现的概率则是有下面的式子决定 $$\\begin{align} P(s)=\\frac{e^{-E(s)}}{\\sum_t e^{-E(t)}} \\end{align}$$","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Neural Network","slug":"Neural-Network","permalink":"http://yezhejack.github.io/tags/Neural-Network/"},{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://yezhejack.github.io/tags/Natural-Language-Processing/"}]},{"title":"普通linux用户如何安装程序","slug":"普通linux用户如何安装程序","date":"2016-05-26T08:24:00.000Z","updated":"2016-07-21T07:10:42.000Z","comments":true,"path":"2016/05/26/普通linux用户如何安装程序/","link":"","permalink":"http://yezhejack.github.io/2016/05/26/普通linux用户如何安装程序/","excerpt":"","keywords":null,"text":"安装screen因为最早是使用screen的，所以想要一个screen来代替tmux。 12wget http://ftp.gnu.org/gnu/screen/screen-4.3.1.tar.gztar -xvf screen-4.3.1.tar.gz 运行 1./configure --prefix=$HOME 之后报错了，大概是ncurses找不到。于是我们需要先安装一下ncurses。 下面的安装步骤参考了这个网址里的东西 https://davidgao.github.io/LFSCN/chapter06/ncurses.html 123456wget http://ftp.gnu.org/gnu/ncurses/ncurses-5.9.tar.gztar -xvf ncurses-5.9.tar.gzcd ncurses-5.9./configure --prefix=/home/xxx/makemake install 这个会把它安装到/home/xxx/bin和/home/xxx/lib中，替代之前的安装到/usr/local中，其中各种内容会被放到对应的这个路径下的文件夹中 这时候回到我们的screen目录，我们除了将其安装程序的位置重定向之外，我们还需要额外告诉configure程序我们的额外的lib位置，因为在当前情况下我们的ncurses会有些库安装到了/home/xxx/lib中，这些库是当前screen需要的，因此安装命令变为 123./configure --prefix=/home/xxx/ LDFLAGS=\"-L/home/xxx/lib\"makemake install 默认情况下当前用户目录下的bin也会在$PATH中，如果没有的话就需要google一下怎么添加了。 这时候完成了 安装java因为我需要一个1.8版本的java，因此我需要在我的路径下能够用1.8版本的java。我们去下载一个jdk for linxu-64bit，然后解压它，放到/home/xxx/java中，然后在.bashrc的最后加上， 1234JAVA_HOME=/home/xxx/java/jdk1.8.0_73CLASSPATH=.:$JAVA_HOME/lib.tools.jarPATH=$JAVA_HOME/bin:$HOME/bin:$PATHexport PATH JAVA_HOME CLASSPATH 每个路径用:隔开，然后搜索的顺序也是按照这个顺序的（有待证实）因为我把PATH那一行中讲$PATH放到第一个之后，一直是使用全局的jdk，我调换一下位置之后就可以了。大概是优先在前面的路径中搜索相应的东西，没有的话再逐级往后找。 Virtualenv这个适合与python，可以自己单独地安装想要的package apt apt-cache search package 搜索包 apt-cache show package 获取包的相关信息，如说明、大小、版本等 sudo apt-get install package 安装包 sudo apt-get install package - - reinstall 重新安装包 sudo apt-get -f install 修复安装”-f = ——fix-missing” sudo apt-get remove package 删除包 sudo apt-get remove package - - purge 删除包，包括删除配置文件等 sudo apt-get update 更新源 sudo apt-get upgrade 更新已安装的包 sudo apt-get dist-upgrade 升级系统 sudo apt-get dselect-upgrade 使用 dselect 升级 apt-cache depends package 了解使用依赖 apt-cache rdepends package 是查看该包被哪些包依赖 sudo apt-get build-dep package 安装相关的编译环境 apt-get source package 下载该包的源代码 sudo apt-get clean &amp;&amp; sudo apt-get autoclean 清理无用的包 sudo apt-get check 检查是否有损坏的依赖","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yezhejack.github.io/tags/Linux/"},{"name":"服务器","slug":"服务器","permalink":"http://yezhejack.github.io/tags/服务器/"}]},{"title":"词性标注","slug":"词性标注","date":"2016-05-25T14:16:00.000Z","updated":"2016-05-25T15:21:39.000Z","comments":true,"path":"2016/05/25/词性标注/","link":"","permalink":"http://yezhejack.github.io/2016/05/25/词性标注/","excerpt":"","keywords":null,"text":"词性标注10.1标记中的信息源 观察感兴趣词的邻近上下文的其它词的词性 利用词本身提供的信息，有人做了一个词性标注起dumb，这个标注器只是将所有词最常用的词性标注给这个词，就取得了90%的准确率。 因此词之间用法及其不平均，因此用统计的方法会更甚一筹。 10.2马尔可夫模型标注器10.2.1概率模型马尔可夫链的两个特性 有限视野（limited horizon）$P(X_{i+1}=t^j|X_1,...X_i)=P(X_{i+1}=t^j|X_i)$ 时间不变形$P(X_{i+1}=t^j|X_1,...X_i)=P(X_{2}=t^j|X_1)$ 下表指示标注预料库和句子中特殊位置的词和词性，用上标表示词典中的词和标注集中的词性类别。 有限视野的式子可以简化为 $P(t_{i+1}|t_{1,i})=P(t_{i+1}|t_i)$ 标记$t^k$跟随$t^j$的最大似然估计来自于不同的标记跟随某个特定标记的相对频率估计。 $P(t_{i+1}|t_i)＝\\frac{C(t^j,t^k)}{C(t^j)}$ 这个问题应用到马尔可夫里，对应的状态就是词的标注，而每次离开一个状态发射出的词就是我们的观察序列。 发射概率： $P(O_n=k|X_n=s_i,X_{n+1}=s_j)＝b_{ijk}$ 可以通过最大似然估计来直接估计一个词被一个特定的状态（标记）发射出来的概率： $P(w^l|t^j)＝\\frac{C(w^l,t^j}{C(t^j)}$ 我们最终要解决这个词性标注问题的话，我们所有求解的是下面这样的式子： $$\\arg\\max_{t_{1,n}}P(t_{1,n}|w_{1,n})=\\arg\\max_{t_{1,n}}\\frac{P(w_{1,n}|t_{1,n})P(t_{1,n})}{P(w_{1,n})}\\\\ =\\arg\\max_{t_{1,n}}P(w_{1,n}|t_{1,n})P(t_{1,n})$$ 这个式子在这里还是可以继续化解下去的，只需要再做两个假设，首先是词语之间是独立的，这样式子就变成了 $P(w_{1,n}|t_{1,n})=\\prod_{i=1}^n P(w_i|t_{1,n}) P(t_n|t_{1,n-1}) P(t_{n-1}|t_{1,n-2})...P(t_2|t_1)$ 然后在假设一下每个词的出现只依赖于自己本身的标注，那么整个式子又变为了这样 $$P(w_{1,n}|t_{1,n})=\\prod_{i=1}^n P(w_i|t_{i}) P(t_n|t_{n-1}) P(t_{n-1}|t_{1,n-2})...P(t_2|t_1)\\\\ =\\prod[P(w_i|t_i)P(t_i|t_{i-1})]$$ 10.2.2 Viterbi算法至此，可能已经一脸懵逼了，但是实际上我们已经将问题化解为了马尔可夫问题，我们可以放心地用马尔可夫链来解这个问题了。我们有了转移概率和发射概率了分别是$P(t_{i}|t_{i-1})$和$P(w_{i}|t_{i})$。然后呢，这个是一个给了观测序列，叫求状态的问题，因此用动态规划对应的Viterbi算法。在马尔可夫中有讲到。 10.2.3 算法的变形未登录词 在实践中不同的标注器在不同语料库上的不同准确率主要是由为登录词的比例决定的，智能化的标注器就需要能够对未登录词的词性进行一定的猜测。 eischedel在1993年的论文中基于三种信息估计了词语生成概率： 一个标注可以生成一个未登录词的概率有多大 生成大写词或者小写词的概率有多大 生成连字符或者特殊后缀的可能性 $P(W^l|t^j)=\\frac{1}{Z} P(unknown word|t^j) P(capitalized|t^j) P(endings or hyph|t^j)$ 三元语法标注器 之前使用的基本都是二元语法，就是只看当前词的前一个词，而三元语法会保留前面两个词的信息，保留了更多的信息。 插值和可变记忆 三元标注器会存在数据稀疏的问题。为了解决这个问题，可以采用一元、二元和三元的概率的线性插值。 $P(t_i|t_{1,i-1})=\\lambda_1 P_1(t_i) + \\lambda_2 P_2(t_i|t_{i-1})+\\lambda_3 P_3(t_i|t_{i-1},t_{i-2})$ 这种线性插值方法将在第六章中提及，怎样使用HMM来估计参数$\\lambda_i$ 已经在第九章中讲过。 还有可变记忆马尔可夫模型（Variable Memory Markov Model,VMMM）。用混合长度的状态代替了二元或者三元语法标注器中的固定长度状态，一个VMMM标注器可以从一个记忆了前两个标记（对应三元语法模型）的状态转移到一个记忆了前三个标记（对应四元语法模型）的状态，再转移到一个没有记忆（对应一元语法模型）的状态。 平滑线性插值是平滑估计的一种方法。例如Charniak(1993)使用了类似加1法的一种思想。 $P(t^j|t^{j-1})=(1-\\epsilon)\\frac{C(t^{j-1},t^j)}{C(t^{j-1})} +\\epsilon$ 可逆性一个从左到右译码（标注）的马尔可夫模型。实际上，从右到左译码是等同的。 $P(t_{1,n})=P(t_1)P(t_{1,2}|t_1)P(t_{2,3}|t_2) ... P(t_{n-1,n}|t_{n-1})=\\frac{P(t_1)P(t_{1,2})P(t_{2,3})...P(t_{n-1,n})}{P(t_1)P(t_2)...P(t_{n-1})}=P(t_n)P(t_{n-1,n}|t_n)...P(t_{2,3}|t_3)P(t_{1,2}|t_2)$ 10.3 隐马尔可夫标注器在我们有很大的标注语料库的时候，马尔可夫模型标注器可以工作得很好，但是这个情况不常见，我们会希望标注一个特定领域内的文本，这个领域内的词语生成概率与可获得的训练文本是不一样的。 10.3.1 隐马尔可夫模型在词性标注中的应用就算没有训练数据，也可以使用HMM来学习标记序列的规则。第九章中介绍的HMM包含如下的元素： 一个状态集 一个输出字母表 初始状态概率 状态转移概率 符号发射概率 有两种方法可以来处理初始化HMM的所有参数，一种是基于每个词语的统计，而另一种是基于每个等价类的统计，将词语聚集到词语等价类中，让同一类的词语允许同样的标注。我们用$b_{j.l}$来表示词语（或词类）l由标记j发射的概率。 Jelinek的方法 $b_{j.l}=\\frac{b^{*}_{j.l} C(w^l)}{\\sum_{w^m} b^{*}_{j.m} C(w^m)}$ $b^{*}_{j.l}=0 如果t^j不是w^l所允许的词性$ $b^{*}_{j.l}=\\frac{1}{T(w^l)} 其它$ Kupiec的方法 这个和前面的Jelinke的方法差不多，只是不针对每个词了，而是针对等价类，但是如果有一个很完美的数据库的话，这个方法就散失优势了。 10.3.2 隐马尔可夫模型训练中的初始化的作用为了防止训练过度，可以在训练集上留出一个验证集，每次迭代后都测试一下，在性能下降的时候就停止迭代。 10.4 基于转换的学习我们需要一个已经标注好的语料库和一个词典作为输入数据。首先用最常用的标记来标注训练语料库中的每个词，这就是我们需要词典的原因。接下来学习算法构建了一个转换的排序表，它把初始的标注转化为接近正确的标注。通过再次初始化来选择每个词最常用的标记，再应用转换，这个排序表就可以用来标注新的文本。 10.4.1 转换一个转换包括两个部分，一个是触发环境，另一个是重写规则。大概的意思就是在特定的位置上出现特殊的标注的时候，这个标注需要进行转换，触发重写规则。 10.4.2 学习算法12345678C0:=corpus with each word tagged with its most frequent tagfor k:=0 step 1 do v:=the transformation ui that minimizes E(ui(Ck)) if (E(Ck))-E(v(Ck)))&lt;e then break fi C(k+1)=v(Ck) T(k+1)=vendOutput sequence:T1...Tk 最初我们使用最常见的标记标注每个词。在每次迭代中，我们选择最可能减少错误率的转换，通过标注过的语料库$C_{k}$中的被错误标注的词语的数目来衡量错误率$E(C_k)$。 如何应用转换也有两种方式，一种是具有立即效果的，另一种是具有延迟效果的。如果是延迟转换的话A-&gt;B会将AAAA转换为ABBB。而立即转换的话则会变为ABAB。 这个标注模型的一个应用Brill(1995b)。在HMM标注中，非监督学习唯一可以获得的信息是每个词有哪些标记是允许的，我们可以利用很多词只有一种词性标记这个事实，并把它作为选择转换的积分计分函数。而且文章中展示的问题没有训练过度的问题。这个文章中使用的无监督的学习，什么是无监督的学习（unsupervised learning）呢，就是看输入的数据是没有标签的，如果有，就是有监督的学习。 会有一个转换模版，其中有context，一个待转换tags集合，一个转换目标tag，所以可见这个模版是可穷举的。然后我们假设有一个Score，先不管这个Score是怎么来了，总之每个转换（transition）都可以得出一个Score，然后选一个Score最大的，作为这个迭代产出的转换。 为了得到这个Score的计分标准比较有趣，因为如果是有监督的学习，那么可以看错误个数，但是这个方法确实无监督的。每次都是以当前已有的转换作为标准。刚开始的时候每个词都是被标注上它所有的可能词性（文中就是tags）。公式就不列举了，公式大概的意思就是找到一个好的能够消除词性标注歧义的转换是 一个通过测量在同一个context和同一个单词中无歧义地出现的一个tag的可能性大于其他tag的tag，最后的结果也不一定是完全消歧的，一个单词仍然可能含有多个可能的tag。 $freq(Y)/freq(Z)*incontext(Z,C)$这个公式中freq(Y)是在语料库中，tag Y无歧义地出现的次数，同理freq(Z)是tag Z在语料库中无歧义出现的次数,而incontext(Z,C)就是白表示在C的条件下一个单词被无歧义地被标注为Z的个数。而R是一个能让这个式子最大化的一个Z，然后Score就是等于 $incontext(Y,C)-freq(Y)/freq(R)*incontext(R,C)$公式的主体是incontext(Y,C)和incontext(R,C)，而另外的freq(Y)/freq(R)则是用于协调相对概率的。 其中有提到一种监督学习和非监督学习结合的HMM词性标注器，就是先通过监督学习，从corpus中学习到HMM的初始参数，然后再通过Baum－Welch算法在一个未标注的corpus中调整参数。 10.4.3 与其他模型的关系决策树有点类似基于转换的方式，但是它很容易特化，在以最小化错误率为目标的情况下它很容易在训练集上获得100%的正确率，但迁移到新的数据集上的时候就会显得性能很差。 10.6 标注准确率和标注器的应用10.6.1 标注准确率看论文的时候，经常看到说某某标注器提高了一个百分点或者两个百分点的准确率，通常会被窝嗤之以鼻，认为在实际应用中，这么一丁点的差别不会影响很大。但是例如一个97%准确率的标注器有63%的可能性讲一个有15个词的句子标注对，而另一个准确率达到98%的标注器则有74%的可能性将一个有15个词的句子完全标注对。因此一个百分点的提高也会对应用产生挺大的影响。","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://yezhejack.github.io/tags/Natural-Language-Processing/"},{"name":"词性标注","slug":"词性标注","permalink":"http://yezhejack.github.io/tags/词性标注/"},{"name":"统计自然语言处理基础","slug":"统计自然语言处理基础","permalink":"http://yezhejack.github.io/tags/统计自然语言处理基础/"}]},{"title":"NAACL-2013-Socher-Manning-DeepLearning","slug":"NAACL-2013-Socher-Manning-DeepLearning","date":"2016-05-24T14:16:00.000Z","updated":"2016-05-29T08:44:48.000Z","comments":true,"path":"2016/05/24/NAACL-2013-Socher-Manning-DeepLearning/","link":"","permalink":"http://yezhejack.github.io/2016/05/24/NAACL-2013-Socher-Manning-DeepLearning/","excerpt":"","keywords":null,"text":"Introduction本来是在看一些关于NLP的一本老教材，但是鉴于现在组里RNN、CNN满天飞，忍不住先来窥探一下Deep Learning在NLP一些传统问题上有什么魔法。 The neural word embedding approach的优势相比于LSA方法，neural word embeddings 可以变得更有意义，通过对一个或多个任务增加监督。 无监督的词向量学习主要思想：一个词和它的上下文环境是一个学习正例；一个随机的词和前面那个一个样的上下文环境则给出了一个学习的反例。例如一个正例是cat chills on a mat，一个反例是cat chills Jeju a mat。 将其形式化：score(cat chills on a mat)&gt;score(cat chills Jeju a mat)。 利用神经网络 将每个词和一个n维向量联系在一起 目标是让正例的score比反例的score高。 s=score(cat chills on a mat)sc=score(cat cills Jeju a mat)minimize J=max(0,1-s+sc)","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yezhejack.github.io/tags/Deep-Learning/"},{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://yezhejack.github.io/tags/Natural-Language-Processing/"},{"name":"词性标注","slug":"词性标注","permalink":"http://yezhejack.github.io/tags/词性标注/"}]},{"title":"论文笔记：A Practical Part-of-Speech Tagger","slug":"论文笔记：A Practical Part-of-Speech Tagger","date":"2016-05-24T14:16:00.000Z","updated":"2017-03-21T05:42:50.000Z","comments":true,"path":"2016/05/24/论文笔记：A Practical Part-of-Speech Tagger/","link":"","permalink":"http://yezhejack.github.io/2016/05/24/论文笔记：A Practical Part-of-Speech Tagger/","excerpt":"","keywords":null,"text":"摘要实现了基于隐马尔可夫模型的词性标注器。这个方法只用很少的资源就可以实现鲁棒的准确的词性标注。只需要一个词表和未标注的文本。准确率超过96%。 必要条件自动化的文本标注是在更大的语料库中发现语言结果的重要的第一步。词性标注为更高层次的分析提供基础。例如识别名词和其他文本中的模式。 一个标注器得具备的几个特性： Robust鲁棒性 Efficient高效性 Accurate 准确性 Tunable 可调性，可以利用先验知识来解决系统性错误。 Reusable 可服用性 方法2.1 背景已有的几种词性标注的方法： rule-based 基于规则的 statistical methods 基于统计的 对于参数估计也有两种方法，一种需要标注好的语料库；而另一种则是利用前向－后向算法，这个不需要标注好的语料库。 2.2 论文中的方法 使用HMM可以允许在选择训练语料库上有更大的灵活性。 3 隐马尔可夫模型应用HMM有两个任务，估计参数和先验概率，在训练集合上。计算隐藏状态的转移序列。 数值稳定性因为前向后向算中使用到的乘积的结果是0到1之间的数值，很容易下溢，所以需要调整放大一下。首先是前向概率，重新计算一遍，分子是当前的前向概率，分母是当前时刻的所有前向概率的和。同样的处理过程同样应用在后向概率上。B的稀疏性可以减少一下一些运算，可以先检测对应的位置上是否有0，有的话就不用计算了。 训练集的处理方式我看这篇论文的主要目的在于知道隐马尔可夫对训练集的处理方式。如果是显马尔可夫的话，只要用最大似然的方式统计A、B以及Pi就可以了，但是如果是隐马尔可夫的话，我们需要一个序列来作为训练集。正常情况下就是把整个文本当作一个序列，作为估计，而不是以句子为单位。但是这篇文章中为了优化，将其分为多个长度相同的部分训练参数，最后将参数做平均。如果是按照每句话划分的话，最后合并的时候因为matrix的size不同，会导致无法平均。 下载地址A Practical Part-of-Speech Tagger.pdf","raw":null,"content":null,"categories":[{"name":"Papers","slug":"Papers","permalink":"http://yezhejack.github.io/categories/Papers/"}],"tags":[{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://yezhejack.github.io/tags/Natural-Language-Processing/"},{"name":"词性标注","slug":"词性标注","permalink":"http://yezhejack.github.io/tags/词性标注/"}]},{"title":"spawn-fcgi源码阅读","slug":"spawn-fcgi源码阅读","date":"2016-05-18T23:33:00.000Z","updated":"2016-05-26T05:48:52.000Z","comments":true,"path":"2016/05/19/spawn-fcgi源码阅读/","link":"","permalink":"http://yezhejack.github.io/2016/05/19/spawn-fcgi源码阅读/","excerpt":"","keywords":null,"text":"spawn-fcgi源码阅读收获可以解除到socket编程，在unix系统下的一些特性，比如一切皆文件。连socket也是一个文件描述符，这也是为什么在提升服务器并发性的时候需要增加文件描述符的数量，因为有些系统的文件描述符的上限只有1024个，大大限制了可以接收的请求数量。而且其实我的本意是要看一下并发模型的，结果发现这个spawn-fcgi根本没有实现任何的并发代码，感觉只是用了内核自带的并发模型。 参数解析部分 argc记录了参数的数量 argv是一个纪录参数的数据 optind是指向当前的参数指针，初始值为1 getopt会让optind不断下移，当没有更多的参数的时候返回getopt 返回－1 getopt的第三个参数是optstring，如果有冒号，则说明这个选项需要一个参数 每次都会把参数的指针放到optarg中 strtol 用于将一个字符串转换为对应基数的长整型，它会先忽略optarg的前面的尽量多的空格，然后遇到一个非空格字符后，就开始尽可能地转换字符 返回的时候如果endptr不为空的话，会让它指向翻译后的第一个字符。猜测：如果完全翻译完，是会返回NULL的，这样才可以看输入是否合法 socket的类型socket有两种，一种是绑定端口的，一种是绑定文件的(unixsocket)，其对应的协议族也是不一样的。 open需要打开文件来表示运行的进程。这里还涉及了很多unix类系统的文件操作。 unix权限保护运行程序的用户和程序拥有者的权限关系，同时还有SUID和GUID这两个可以让程序拥有者把权限“借”给程序的运行者，例如/etc/passwd就是这样，虽然非root用户无法对其进行直接修改，但是却可以通过执行对应的验证程序，获得root权限从而读取/etc/passwd的信息。 chroot 一种沙盒机制这个可以将程序运行的作用范围控制在一定的距离内。 fork()这个可以返回子进程的进程号，同时子进程拥有父进程的所有上下文，也就是让程序运行到fork()前的状态，因此可以用child来区分当前进程是子进程还是父进程。 文件描述符文件描述符的前三个0,1,2总是stdin stdout 和stderr。大量的应用程序都依赖这个特性，虽然这个好像并不是标准。 比sleep更精确的计时器在unix中sleep的精确度为1s，而select(0,NULL,NULL,NULL,&amp;val)是一个更精确的计时器，可以以微秒为单位。 源码及注释下载地址spawn-fcgi.c","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"socket","slug":"socket","permalink":"http://yezhejack.github.io/tags/socket/"},{"name":"linux","slug":"linux","permalink":"http://yezhejack.github.io/tags/linux/"},{"name":"unix","slug":"unix","permalink":"http://yezhejack.github.io/tags/unix/"}]},{"title":"socket编程的一些问题","slug":"socket编程的一些问题","date":"2016-05-18T23:33:00.000Z","updated":"2016-05-26T08:24:28.000Z","comments":true,"path":"2016/05/19/socket编程的一些问题/","link":"","permalink":"http://yezhejack.github.io/2016/05/19/socket编程的一些问题/","excerpt":"","keywords":null,"text":"多个进程监听一个socket对于监听一个socket来说，多个进程同时在accept处阻塞，当有一个连接进入，多个进程同时被唤醒，但之间只有一个进程能成功accept，而不会同时有多个进程能拿到该连接对象，操作系统保证了进程操作这个连接的安全性。 扩展：上述过程，多个进程同时被唤醒，去抢占accept到的资源，这个现象叫“惊群”，而根据网上资料，Linux 内核2.6以下，accept响应时只有一个进程accept成功，其他都失败，重新阻塞，也就是说所有监听进程同时被内核调度唤醒，这当然会耗费一定的系统资源。 而2.6以上，则已经不存在惊群现象了，但是由于开发者开发程序时使用了如epoll等异步通知技术，仍然会造成惊群，如有需要更高性能要求，或许参考nginx的实现方案，这里就不详述了。 作者：云帆技术博客 这个方面可以从socket的accept函数的源码中得到解答，每个socket维护了一个deque来存储请求，可见是个FIFO的请求结构。","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"socket","slug":"socket","permalink":"http://yezhejack.github.io/tags/socket/"},{"name":"linux","slug":"linux","permalink":"http://yezhejack.github.io/tags/linux/"},{"name":"unix","slug":"unix","permalink":"http://yezhejack.github.io/tags/unix/"}]},{"title":"flask学习笔记","slug":"Flask学习笔记","date":"2016-05-16T12:56:00.000Z","updated":"2016-05-18T13:11:17.000Z","comments":true,"path":"2016/05/16/Flask学习笔记/","link":"","permalink":"http://yezhejack.github.io/2016/05/16/Flask学习笔记/","excerpt":"","keywords":null,"text":"Flask安装flask依赖两个外部库，一个是Werkzeug，一个提供WSGI支持，还有一个是Jinja2负责解析模版。 virtualenv它提供了分离运行环境的功能。 123makdir my projectcd myprojectvirtualenv venv 1. venv/bin/activate 最简单的应用1234from flask import Flask app = Flask(__name__) @app.route( / )def hello_world(): return Hello World! if __name__ == __main__ : app.run() 这个默认情况下是在debug模式下运行的，它只监听来自127.0.0.1的请求。想要关闭debug消息的话需要修改为下面的代码。 1234from flask import Flask app = Flask(__name__) @app.route( / )def hello_world(): return Hello World! if __name__ == __main__ : app.run(host='0.0.0.0') flask还提供了debug模式，就是不需要每次改完代码再重启服务。只要代码有修改那么只需要把代码改为 1234from flask import Flask app = Flask(__name__) @app.route( / )def hello_world(): return Hello World! if __name__ == __main__ : app.run(debug=True) 或者 12345from flask import Flask app = Flask(__name__) @app.route( / )def hello_world(): return Hello World! if __name__ == __main__ : app.debug=True app.run() 在多进程的环境下似乎会有问题。 路由在函数前加一个修饰，可以指定URL到这个函数中 1234567@app.route('/')def index(): return 'Index Page'@app.route('/hello')def hello(): return 'Hello World' 规则变量类似C＋＋中的重载，但在python中是根据变量的名字来找到对应的函数。 1234567@app.route('/user/&lt;username&gt;')def a(username): return 'hello'@app.route('/user/&lt;int:post_id&gt;')def a(post_id): return 'wolrd' 可以使用的用来分流的东西有int、float和path。 如果一个路径规则有一个/的话，当一个URL没有/的话，程序也会自动补上，但是反过来则无法匹配了。 URL生成1&gt;&gt;&gt;from flask import Flask, url_for &gt;&gt;&gt; app = Flask(__name__) &gt;&gt;&gt; @app.route( / ) ... def index(): pass ... &gt;&gt;&gt; @app.route( /login ) ... def login(): pass ... &gt;&gt;&gt; @app.route( /user/&lt;username&gt; ) ... def profile(username): pass ... &gt;&gt;&gt; with app.test_request_context(): ... print url_for( index ) ... print url_for( login ) ... print url_for( login , next= / ) ... print url_for( profile , username= John Doe ) ... / /login /login?next=/ /user/John%20Doe 4.3.3 HTTP方法默认情况下，flask app只对GET方法有回应，如果要接受POST方法的话 1234@app.route( /login , methods=[ GET , POST ]) def login(): if request.method == POST : do_the_login() else: show_the_login_form() 4.5 套用模版回复的时候为了可以套入模版，可以使用render_template方法。 需要将模版放在文件夹/templates/中。 case 1:a module: 123/application.py/templates /hello.html case 2:a package 1234/application /__init__.py /templates /hello.html 4.6 读取请求数据flask中使用的是全局的request object，但它本身是线程安全的。 4.6.1 Context Locals此处暂时没懂。 4.7 错误可以使用模版来自定义错误界面。 8.4 其它模块的日志当有其它模块的时候，为了让它的log也能写进来，可以使用下面的代码。 12345from logging import getLoggerloggers=[app.logger,getlogger('sqlalchemy'),getLogger('otherlibrary')]for logger in loggers: logger.addhandler(mail_handler) logger.addHandler(file_handler)","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"flask","slug":"flask","permalink":"http://yezhejack.github.io/tags/flask/"},{"name":"python","slug":"python","permalink":"http://yezhejack.github.io/tags/python/"},{"name":"framework","slug":"framework","permalink":"http://yezhejack.github.io/tags/framework/"}]},{"title":"Minimal Height Tree","slug":"Minimal Height Tree","date":"2016-05-10T07:51:00.000Z","updated":"2016-05-18T05:56:39.000Z","comments":true,"path":"2016/05/10/Minimal Height Tree/","link":"","permalink":"http://yezhejack.github.io/2016/05/10/Minimal Height Tree/","excerpt":"","keywords":null,"text":"Minimal Height Tree尝试了用暴力解法，无奈超时了，于是用了比较巧妙的解法。题目中提示答案的顶点数是有范围的，可以通过不断地删除度为1的点，调整每个节点的度，来求最后的答案。","raw":null,"content":null,"categories":[{"name":"Leetcode","slug":"Leetcode","permalink":"http://yezhejack.github.io/categories/Leetcode/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"http://yezhejack.github.io/tags/Leetcode/"},{"name":"题解","slug":"题解","permalink":"http://yezhejack.github.io/tags/题解/"}]},{"title":"如何在python中使用正则表达式","slug":"正则表达式","date":"2016-05-10T07:51:00.000Z","updated":"2016-05-18T05:56:28.000Z","comments":true,"path":"2016/05/10/正则表达式/","link":"","permalink":"http://yezhejack.github.io/2016/05/10/正则表达式/","excerpt":"","keywords":null,"text":".匹配除了换行符以外的任意字符。 *表示前面的内容可以连续重复任意次。 .*就表示匹配前面任意个除换行符外的字符组成的字符串。 简介略过。讲讲我的心路历程吧 正则表达式这个我真的是拖了很久才开始接触的东西，现在终于避不过去了，现在有个要探测字符串中是否含有QQ表情和emoji表情的任务。目标语言是python 简单模式2.1 匹配字符必须要掌握的是元符号metacharacter。这里有一个完整的元符号列表 . ^ $ * + ? { } [ ] \\ | ( ) 首先是[和]这两个符号。它们用来表示一组符号，你希望用这组符号之一来匹配，这里面也可以表示一个范围，使用-来连接两个符号，来表示范围。例如[abc]可以匹配a，b或c之一，这个相当于[a-c]。 其他元符号在字符组中不再是元字符的概念。例如[amk$]会匹配下面的任意一个字符a，k，m或$。 如果在字符组的第一个是一个^的话表示这个字符组会匹配不在这个字符组中的任何一个，如果是在外面的话就只是简单地匹配^这个字符。 \\是一个转义符，加在元字符前会让这个元字符不在是元字符，而是简单地匹配它本身。类似python中的字符串。同时它也和一些符号也可以组成一些特殊的符号，来表示一些预先定义好的字符组。 \\d相当于[0-9] \\D相当于[^0-9] \\s相当于[\\t\\n\\r\\f\\v]，匹配所有空白字符 \\S相当于[^\\t\\n\\r\\f\\v]，匹配所有非空白字符 \\w相当于[a-zA-Z0-9_] \\W相当于[^a-zA-Z0-9_] 这符号可以放在字符组中，例如[\\s,.]可以匹配任何的非空白字符以及,和.。 还有一个元字符.表示匹配除了回车符号的字符。 2.2 重复的东西元字符*并不匹配真正的符号*，相反的，它可以匹配让前面部分重复0至任意次数的字符串。遇到带元符号*的表达式，它会尽可能长地匹配子串。它会先尽量长地去先匹配，然后根据后面的字符来不断缩短自己的长度。 另一个可以重复的元字符是+，表示匹配一次或者多次。差别在于这个元字符至少得一次。 元字符?表示匹配一次或者零次。 最复杂的表示重复的元字符是{m,n}，m和n是两个整数，$m \\leq n$。这个表示对应的部分最少重复m次，最多重复n次。如果m缺了，会自动补上0，如果n缺了，会认为可以最多重复无限多次。当然说无限多次，依然是由机器、软件等限制的。 3 使用正则表达式首先要把正则表达式编译成一个object，然后在去match他们。 3.1 编译正则表达式p=re.compile(&#39;ab*&#39;,re.IGNORECASE)可以忽略大小写。 3.2 backslash灾难这里会产生很多的麻烦，而且让结果很难理解，因此可以使用字符的前缀r处理字符串，例如r&quot;\\n&quot;不再是一个回车了，而是两个字符\\和n。 3.3 匹配 match()如果没匹配，则返回None search()如果没匹配，则返回None findall()找到所有的匹配子串，然后返回一个list finditer()找到所有的匹配子串，然后返回一个迭代器 我们可以从match object里面通过下面四个函数获得匹配的信息 group()返回匹配的字符串 start()``end()返回开始和结束的位置 span()返回一个元组表示(start,end) 3.4 模块级别的函数可以使用不创建re的object的调用方法，例如re.match(r&#39;From\\s+&#39;,&#39;Fromage amk&#39;)，这个方法会在cache中创建object。之后使用相同的RE的话速度会快。 3.5 解释标志 DOTALL和S会让.匹配任何字符，包括换行符。 IGNORECASE和I会忽略大小写来进行匹配。 LOCALE和L这个会根据你的系统是哪个国家的，改变对应的字符集例如\\w的字符范围 MULTITLINE和M这个flag会影响到^和$等符号 UNICODE和U让\\w \\W \\b \\B \\d \\D \\s \\S依靠Unicode，这个还有待考察 VERBOSE和X这个可以在里面插入用#的注释 更多的模式更多的元符号 \\b是匹配一个词的边界 |是一个表示或的符号，但是它的优先级很低，这样可以让它的结果更合理，例如Crow|Servo表示的是匹配Crow和Servo中的其中一个，而不是w货S中的一个。 ^表示匹配字符串的开端，如果有MULTILINE的标志的话，它也会匹配每个换行，如果只是想匹配在句子首部的单词From的话，可以使用的正则表达式是^From。 $匹配的是句尾、行尾或者换行符号的后面的位置 \\A在没有MULTILINE的情况下和^一样，但是在有MULTILINE的情况下也只匹配字符串首部 \\Z无论什么情况下也都匹配字符串的尾部 \\b前面已经介绍过了，但是这里要抢到一个很严重的冲突，如果没有在正则表达式的字符串前加上r的话，python会把\\b处理成空格，这样就无法正常匹配了 \\B则是匹配不是词的边界 4.2 Grouping可以用(和)来讲许多字符当作一个整体，类似数学计算中的概念。","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yezhejack.github.io/tags/python/"},{"name":"正则表达式","slug":"正则表达式","permalink":"http://yezhejack.github.io/tags/正则表达式/"}]},{"title":"HTTP学习以及push机制","slug":"HTTP学习以及push机制","date":"2016-05-07T13:19:00.000Z","updated":"2016-05-07T13:40:28.000Z","comments":true,"path":"2016/05/07/HTTP学习以及push机制/","link":"","permalink":"http://yezhejack.github.io/2016/05/07/HTTP学习以及push机制/","excerpt":"","keywords":null,"text":"什么是长连接当客户端用TCP/IP协议从服务器上获取数据的时候，都需要一个连通客户端和服务器的连接，连接通过三次握手建立，通过四次握手释放。如果每次获取数据都创建一个独占的连接，并在数据传输完毕后释放，这种连接叫做短连接。而一个能够供多个请求多次传输数据，并在数据传输后保活一段时间的连接，我们称之为长连接。（转载自http://www.chanpin100.com/archives/58040）。 push机制利用的就是长连接，因为ios只要和官方的推送服务器发送心跳，因此很省电，但是国内的android无法使用google的推送服务器，因此各个app得自己建立推送管道，同时这些管道还不能被各种管家杀死。","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://yezhejack.github.io/tags/HTTP/"},{"name":"push","slug":"push","permalink":"http://yezhejack.github.io/tags/push/"},{"name":"长连接","slug":"长连接","permalink":"http://yezhejack.github.io/tags/长连接/"},{"name":"短连接","slug":"短连接","permalink":"http://yezhejack.github.io/tags/短连接/"}]},{"title":"马尔可夫模型","slug":"统计自然语言处理基础-第九章-马尔可夫模型","date":"2016-05-05T09:09:00.000Z","updated":"2016-05-08T03:18:36.000Z","comments":true,"path":"2016/05/05/统计自然语言处理基础-第九章-马尔可夫模型/","link":"","permalink":"http://yezhejack.github.io/2016/05/05/统计自然语言处理基础-第九章-马尔可夫模型/","excerpt":"","keywords":null,"text":"马尔可夫模型9.3 隐马尔可夫模型的三个基本问题 给出一个模型$\\mu=(A,B,\\pi)$，怎样有效地计算某个观测序列发生的概率，即$P(O|\\mu)$？ 给出观测序列$O$和模型$\\mu$，我们怎样选择一个状态序列$(X_{1},...,X_{T+1})$，以便能够最好地解释观测序列？ 给定观测序列$O$，以及通过改变模型$\\mu=(A,B,\\pi)$的参数而得到的模型空间，我们怎样才能找到一个最好地解释这个观测序列的模型。 这中间包含了三个量： 观测序列$O$ 模型$\\mu$ 状态序列$(X_{1},...,X_{T+1})$ 第一个是马尔可夫模型的正向用途，第二个是求出隐藏状态，第三个则是学习过程，即通过现象去学习一个模型。 9.3.1计算观测序列的概率每个状态到达观测序列$O$都有一定的概率，我们将所有状态到达观测状体的概率相加不就可以得到在给定模型$\\mu$的情况下，观测序列$O$出现的概率。 给定一个状态序列$(X_{1},...,X_{T+1})$，然后对应的到达$O$的概率就为 $$P(O|X,\\mu)=\\Pi_{t=1}^{T}P(O_t|X_t,X_t+1,\\mu) =b_{X_{1}X_{2}o_{1}} b_{X_{2}X_{3}o_{2}}...b_{X_{T}X_{T+1}o_{T}}$$ 出现这个状态序列的概率为 $P(X|\\mu)=\\pi_{x_1}a_{X_1 X_2}a_{X_2 X_3}...a_{X_TX_{T+1}}$ 因此 $P(O,X|\\mu)=P(O|X,\\mu)P(X|\\mu)$ 最终 $P(O|\\mu)=\\sum_X P(O|X,\\mu)P(X|\\mu)$ 分析一下这个算法，虽然说这个算法非常直观，推导也是初学者级别（没错就是我），但是首先给定一个$X$我们想要算出$P(O,X|\\mu)=P(O|X,\\mu)P(X|\\mu)$就需要$T-1+T+1$次的乘法，然后$X$还有$N^{T+1}$个组合，算出每个组合就需要$(2T)N^{T+1}$次乘法，再将这些组合的结果相乘，就得到了$(2T＋1)N^{T+1}$次乘法的需求。 直觉告诉我这里面有重复计算的结果，其实整个计算过程和动态规划很像，每条边都有代价 一个格子$(S_i,t)$存储了一些信息。 前向过程 $\\alpha_i(t)=P(o_1 o_2 ... o_{t-1},X_t=i|\\mu)$ $\\alpha_i(t)$存储在格路的$(S_i,t)$中，表示在$t$时刻已状态$S_i$结束的概率。 初始化 $\\alpha_i(1)=\\pi_i,1 \\leq i \\leq N$ 推导 $\\alpha_i(t+1)=\\sum_{t=1}^{N} \\alpha_i(t) a_{ij}b_{ijo_t},1\\leq t\\leq T,1\\leq j\\leq N$ 求和 $P(O|\\mu)=\\sum_{i=1}^N\\alpha_i(T+1)$ 用前向过程来计算观测序列的概率，可以只需要$2N^2T$次乘法就可以搞定了。 后向过程这个过程是和前向相对应的，所表示的意思是从当前t时刻开始，观测到$T$这样的观测序列，并且知道t时刻的隐藏状态是$X_{t}$。在给定的模型$\\mu$的情况下，能观察到之前的序列的概率就称为后向过程。之所以要引入后向概率是为了解决三个基本问题中的第三个问题。 给出的公式如下： $\\beta _{i}(t)=P(o_{t}...o_{T},X_{t},\\mu)$ 后向过程理解起来会有点难度，特别是像我这样直观先行的动物来说，对于公式总得翻译得直白一点才能想想。这个过程也就是当某一时刻$X_t=i$的情况下，开始转换的到最后能得出$o_{t}...o_{T}$这个观测序列的概率，也就是$\\beta _{i}(t)$，这样我们可以依赖于。如果我们知道$\\beta _{i}(t＋1)$的值的话，这个就很简单了，$\\beta _{i}(t)$想要到达最后的概率是通过各个$\\beta _{i}(t＋1),1 \\leq i \\leq N$的节点到达最后的概率只喝，因此它的推导公式就很好理解了。 初始化 $\\beta _{i}(T +1)=1, 1\\leq i\\leq N$ 推导 $\\beta _{i}(t)=\\sum_{j+1}^{N}a_{ij}b_{ijo_t}\\beta _{j}(t+1),1\\leq t\\leq T 1\\leq i \\leq N$ 求和 $P(O|\\mu)=\\sum_{i=1}^{N}\\pi_i\\beta_i(1)$ 前向后向的结合我们可以让前向后向公式结合来计算一个观测序列的概率 $P(O,X_{t}=i|\\mu)=\\alpha_{i}(t)\\beta_{i}(t)$ 有了这个中间公式之后，我们就可以计算 $P(O|\\mu)=\\sum_{i=1}^{N}\\alpha_{i}(t)\\beta_{i}(t),1\\leq t \\leq T +1$ 到此为止，我们解决了第一个问题了 9.3.2确定最佳状态序列这个过程通常被称为是一个译码，被人模糊地描述为“找到能够最好地解释观测值的状态序列”。存在两种方法： 对于每一个$t,1\\leq t \\leq T+1$，我们可以找到$X_t$，使得$P(X_t|O,\\mu)$最大 但这个方式实际上是最大化了奖杯正确猜测的状态的期望数目，所以最常用的方法是Viterbi算法。 Viterbi算法 我们希望找到一个状体序列$X$: $\\arg\\max_{X} P(X|O,\\mu)$ 我们可以比较容易得到在一个模型$\\mu$下，同时得到一个观测序列$O$和状态序列$X$的概率是多少，固定观测序列后，前面的问题的解等价于下面这个问题的解。 $\\arg\\max_{X} P(X,O|\\mu)$ 这个地方我需要解释一下，这个问题是如何转移的 $P(X,O|\\mu)=P(X|O,\\mu)P(O|\\mu)$ 然后由于$P(O|\\mu)=1$，因为它是给定的，所以概率当然为1啦。因此左右两边相等，因此前面的问题等价可以成立。 为了解决这个问题，定义： $\\delta_j(t)=\\max_{X_1...X_{t-1}} P(X_1 ... X_{t-1},o_1 ... o_{t-1},X_t=j|\\mu)$ 变量$\\psi_j(t)$记录了导致这条最可能路径的入弧节点。还是用动态规划的思想来解决。 初始化 $\\delta_j(t)=\\pi_j,1 \\leq j \\leq N$ 推导 $\\delta_j(t+1)=\\max_{1 \\leq i \\leq N}\\delta_i(t)a_{ij}b_{ijo_t},1\\leq j\\leq N$ 然后存储回溯路径 $\\psi_j(t+1)=\\arg\\max_{1 \\leq i \\leq N}\\delta_i(t)a_{ij}b_{ijo_t},1\\leq j\\leq N$ 终止以及路径读出 $$\\hat{X}_{T+1}=\\arg\\max_{1 \\leq i \\leq N}\\delta_i(T+1)\\\\ \\hat{X}_t=\\psi_{\\hat{X}_{T+1}}(t+1)\\\\ P(\\hat{X})=\\max_{1 \\leq i \\leq N}\\delta_i(T+1)$$ 9.3.3隐马尔可夫的参数估计问题这个问题可以看作是求解下面这个式子。这个已经可以算是机器学习的问题，对一个式子进行最优化，这里用的最优化方法是EM算法的一个特例。因为这个问题没有一个解析解，并不像之前的两个问题一样。这个方法叫做迭代爬山算法，也叫做Baum-Welch或前向后向算法。 $\\arg \\max_{\\mu} P(O_{training}|\\mu)$ 过程 $p_t(i,j)$是在给定观测序列$O$的情况下，在$t$时刻经过某条弧的概率。$$p_t(i,j)=P(X_t=i,X_{t+1}=j|O,\\mu)\\\\ =\\frac{P(X_t=i,X_{t+1}=j,O|\\mu)}{P(O|\\mu)}\\\\ =\\frac{\\alpha_i(t) a_{ij}b_{ijo_t} |beta_j(t+1)}{\\sum_{m=1}^N\\alpha_m(t)\\beta_m(t)}\\\\ =\\frac{\\alpha_i(t) a_{ij}b_{ijo_t} |beta_j(t+1)}{\\sum_{m=1}^N \\sum_{n=1}^N \\alpha_m(t) a_{mn}b_{mno_t}\\beta_n^{t+1}}$$ 求解上面这个式子是这个问题的关键，因为更新$\\pi$，$A$和$B$都需要这个式子 计算这个的时候需要先计算出前向过程和后向过程，上面这个式子实际上可能是有误导性的，因为虽然它把这个分的这么细，但实际上都需要前向和后向过程，所以可以直接获得$P(O|\\mu)$。然后分子就更好算了，其中四个量都是已知的。有了$p_t(i,j)$之后，$a_{ij}$就是可以更新为所有从i出发到达j的弧的概率除以从i出发的概率，$\\pi_i$则是在$t=1$时刻的从$i$出发的弧的个数除以$t=1$时刻出发的所有弧的个数。$b_{ijk}$则是所有从i到j的弧中，发射出状态k的概率。 这样之后一次参数更新就完成了，我们就有了新的模型，当观察序列的出现概率没有显著增长的话，算法就可以停止了。","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://yezhejack.github.io/tags/Natural-Language-Processing/"},{"name":"统计自然语言处理基础","slug":"统计自然语言处理基础","permalink":"http://yezhejack.github.io/tags/统计自然语言处理基础/"},{"name":"马尔科夫","slug":"马尔科夫","permalink":"http://yezhejack.github.io/tags/马尔科夫/"},{"name":"HMM","slug":"HMM","permalink":"http://yezhejack.github.io/tags/HMM/"}]},{"title":"基于hexo开源架构的GitHub博客维护","slug":"基于hexo开源架构的GitHub博客维护","date":"2016-05-01T16:00:00.000Z","updated":"2017-01-14T16:22:29.000Z","comments":true,"path":"2016/05/02/基于hexo开源架构的GitHub博客维护/","link":"","permalink":"http://yezhejack.github.io/2016/05/02/基于hexo开源架构的GitHub博客维护/","excerpt":"","keywords":null,"text":"添加博文 进入到hexo的根目录，然后在 source/_posts中建立一个新的以.md结尾的markdown文档。 文档开头需要填写yaml格式的描述，用于网站的存放 123456789title: 基于hexo开源架构的GitHub博客维护date: 2015-11-23 17:52:37tags:- 站点维护- hexo- githubcategories:- 维护description: 维护博客的工作流程 然后接下去输入编辑好的文档 打开终端输入下面的命令即可完成添加及更新网站123cd hexo根目录hexo ghexo deploy 更改主题 更改主题后整个添加博文的操作会有变化，变化的结果取决于主题的样式 我添加的一个是使用git clone把文件都放到theme/主题名的文件夹中，然后在_config.yml中更改相应属性 增加关于页面1hexo new page &lt;pagename&gt; 这里把&lt;pagename&gt;填成about就可以新建一个关于页面了。会在source/about/下生成一个index.md的文件，这个就是用来编辑这个页面的markdown文件了。 feature添加评论功能首先需要申请一个disqus的账号，然后针对你所建立的个人blog建立一个对应的站点，这里将会集合你的博客评论。然后在themes/maupassant/_config.yml中在对应的地方填上站点对应的shortname。这个shortname要注意。 最近评论功能因为对于前端开发不是很清楚，但是看了一下源码，似乎截止2016.5.2这天的maupassant是有问题存在的，其中recent_comment的地址模版似乎给错了，需要修改一下themes/maupassant/layout/_widget/recent_comments.jade文件。 修改后的样子 12345if theme.disqus .widget .widget-title i(class='fa fa-comment-o')= ' ' + __('recent_comments') script(type='text/javascript', src='http://#&#123;theme.disqus&#125;.disqus.com/recent_comments_widget.js?num_items=5&amp;num_items=5&amp;hide_mods=0&amp;hide_avatars=0&amp;avatar_size=32&amp;excerpt_length=100') 这样应该就可以了 修改网站的配置这里需要打开_config.yml。对里面的language和theme都要进行对应的修改。 swiftype 搜索功能的加入这里依然需要去相应网站去设置，主要问题在于给出的代码应该加在什么地方。新版本的maupassant里面单独有一个themes/maupassant/layout/_widget/search.jade。 12345678910if theme.swiftype .widget input.st-default-search-input(placeholder='Search' type='text') script. (function(w,d,t,u,n,s,e)&#123;w['SwiftypeObject']=n;w[n]=w[n]||function()&#123; (w[n].q=w[n].q||[]).push(arguments);&#125;;s=d.createElement(t); e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e); &#125;)(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st'); _st('install','Z-tgtsfWWX8ZNoWubjoC','2.0.0'); 当然在themes/maupassant/_config.yml中还需要填入key才行 插入图片 在source/img中存入图片，命名为avator.jpg。 在markdown中写入图片 1![头像](/img/avator.jpg) 数学公式hexo为数学公式提供了mathjax，这个按照网站介绍的来看是可以插入行内数学公式的。 1234567891011121314$$\\frac&#123;\\partial u&#125;&#123;\\partial t&#125;= h^2 \\left( \\frac&#123;\\partial^2 u&#125;&#123;\\partial x^2&#125; +\\frac&#123;\\partial^2 u&#125;&#123;\\partial y^2&#125; +\\frac&#123;\\partial^2 u&#125;&#123;\\partial z^2&#125;\\right)$$Simple inline $a = b + c$.&#123;% math %&#125;\\begin&#123;aligned&#125;\\dot&#123;x&#125; &amp; = \\sigma(y-x) \\\\\\dot&#123;y&#125; &amp; = \\rho x - y - xz \\\\\\dot&#123;z&#125; &amp; = -\\beta z + xy\\end&#123;aligned&#125;&#123;% endmath %&#125; $$\\frac{\\partial u}{\\partial t}= h^2 \\left( \\frac{\\partial^2 u}{\\partial x^2} +\\frac{\\partial^2 u}{\\partial y^2} +\\frac{\\partial^2 u}{\\partial z^2}\\right)$$ Simple inline $a = b + c$. $$\\begin{aligned} \\dot{x} &amp; = \\sigma(y-x) \\\\ \\dot{y} &amp; = \\rho x - y - xz \\\\ \\dot{z} &amp; = -\\beta z + xy \\end{aligned}$$","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"站点维护","slug":"站点维护","permalink":"http://yezhejack.github.io/tags/站点维护/"},{"name":"hexo","slug":"hexo","permalink":"http://yezhejack.github.io/tags/hexo/"},{"name":"github","slug":"github","permalink":"http://yezhejack.github.io/tags/github/"}]},{"title":"LibSVM使用","slug":"LibSVM使用","date":"2016-04-25T15:21:02.000Z","updated":"2017-04-01T10:46:00.000Z","comments":true,"path":"2016/04/25/LibSVM使用/","link":"","permalink":"http://yezhejack.github.io/2016/04/25/LibSVM使用/","excerpt":"","keywords":null,"text":"Install libsvm for ubuntu下载libsvm1wget http://www.csie.ntu.edu.tw/~cjlin/cgi-bin/libsvm.cgi?+http://www.csie.ntu.edu.tw/~cjlin/libsvm+tar.gz 解压这个文件1tar -xvf &lt;刚才下载的文件&gt;.tar.gz 编译进入到解压的文件夹中1234cd libsvm/pythonmakesudo cp svm.py svmutil.py /usr/lib/python2.7sudo cp ../libsvm.so.2 /usr/lib LIBSVM的使用背景毕设需要SVM来做二元分类，于是使用了LIBSVM。这里将先阅读LIBSVM的使用方法，而后再慢慢调整我的参数。参考文档就是LIBSVM官方给出的guide。 推荐使用过程 转换数据格式 对数据进行正规化，这一步很重要 通常先考虑RBF作为模型 使用cross-validation来选择参数 然后使用选择好的参数进行训练 测试","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"LIBSVM","slug":"LIBSVM","permalink":"http://yezhejack.github.io/tags/LIBSVM/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://yezhejack.github.io/tags/machine-learning/"}]},{"title":"机器翻译","slug":"统计自然语言处理基础-第十三章-机器翻译","date":"2016-04-17T06:22:00.000Z","updated":"2016-05-07T02:57:57.000Z","comments":true,"path":"2016/04/17/统计自然语言处理基础-第十三章-机器翻译/","link":"","permalink":"http://yezhejack.github.io/2016/04/17/统计自然语言处理基础-第十三章-机器翻译/","excerpt":"","keywords":null,"text":"第十三章 机器翻译存在问题 词的歧义 词序 句法歧义 几种常见的翻译模式 直接翻译法：词对词的对齐翻译方式。从源语言的表层句子出发，将词或固定词组直接置换成目标语言的对应成分，这种方式的最大缺陷就在于语言和语言之间可能不存在一一对应关系。同时词的歧义也是一个问题。这个需要参照上下文才能确定这个词改如何翻译。词序也有问题，句法转换可以解决这个问题，将其用手工定义的规则转换成一颗树，然后在这棵树上生成目标语言。这里面也存在句法歧义的问题。 语义转换方法：将原文转换为语义表示形式，然后在这个基础上生成目标语言。但通常会导致译文生涩难懂。 中间语言转换：中间语言独立于任何语言。这样在开发多语言翻译系统的时候是非常方便的，针对每个语言只需要开发一个分析模块和生成模块。但设计这样的中间语言是一件难度非常大的事情。 文本对齐可以拿多语言国家政府的官方文件作为平行语料库。 句子对齐河段落对齐简单地说句子对齐就是将源语言中的一组句子和目标语言中的一组句子进行对应的过程。每组句子可以为空，也可以额外加入对应源语言中不存在的句子或者删除原有的句子，我们称这两组对应的句子为一个句珠(bead)。 如何判定对齐：如果出现有个别词语对齐，则不能说它们是对齐的，但是如果是有任何子句对应出现，就可以判定句子之间的对齐关系。 有可能会出现交叉对应的情况，也就是一组的第一句话和另一组的第二句话有关系。因此有必要区分对齐（alignment）和对应（correspondence）之间的差别。对齐不允许交叉。 基于长度的对齐算法这个方法的基本思想是，假设源语言和目标语言的句子长度存在比例关系，即原文中过的短句对应于目标语言中的短句，长句对应于长句。 $\\arg\\max_A P(A|S,T)==\\arg\\max_A P(A,S,T)$ 目标是找出让这个概率值最大的A，其中A为对齐方式，S代表源语言，T代表目标语言。 这就转换成一个动态规划问题了，找出最小耗费函数。 因此我们需要找到每种对齐方式的耗费函数。 加强版的可以增加一些词锚（lexical anchor）概念，即利用一些有固定译法的单词或者短语结构，删除在语料中没有充分对齐的段落。 但是当对齐不同语系的文本的时候，算法的效果并不是很令人满意。 基于信号处理技术的偏移位置对齐算法大致思想基于信号处理方法的偏移位置对齐算法没有试图对齐句子，而是在平行文本中利用位置偏移量的概念，即源文本中一定位置的文本和目标语言中一定位置的文本是大致对齐的 Church(1993)的方法利用同源词信息（e.g.不同语言中的词由于借鉴或者来源于同一种祖先语言而具有的相同特征。因为OCR识别输出文本经常以后段落分割标记、标点符号以及诸如脚注、表等文本信息。 Church首先将源语言和目标语言连接起来，构建一个点阵图。坐标(x,y)处的点表示连接文本中位置x与位置y处的文本匹配，将4-gram作为匹配的最小单位。 Fung and McKeown(1994)适用范围 无需考虑句子的边界 双语文本只是局部对齐，即源语言和目标语言文本之间存在很多没有对齐的段落 与具体的语言无关 做法： 先导出一个小规模的双语词典，词典提供一些对齐的基点。每个词对应一个信号，用到达向量表示，标记了词的不同出现位置之间的词数。例如一个词出现了4次，位置向量为（1，263，267，519），那么词的到达向量事（262，4，252）。 动态时间伸缩（Dynamic Time Warping）。","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://yezhejack.github.io/tags/Natural-Language-Processing/"},{"name":"机器翻译","slug":"机器翻译","permalink":"http://yezhejack.github.io/tags/机器翻译/"},{"name":"统计自然语言处理基础","slug":"统计自然语言处理基础","permalink":"http://yezhejack.github.io/tags/统计自然语言处理基础/"}]},{"title":"NLP入门","slug":"NLP入门","date":"2016-04-17T05:37:47.000Z","updated":"2017-03-21T03:04:01.000Z","comments":true,"path":"2016/04/17/NLP入门/","link":"","permalink":"http://yezhejack.github.io/2016/04/17/NLP入门/","excerpt":"","keywords":null,"text":"入门书籍比较古老的两本 统计自然语言处理基础 自然语言处理基础 第一版 自然语言处理基础 第二版 这个是王威廉在微博上推荐的书籍，图侵删 前两本都极其古老，但是据说不错，在看的是统计自然语言处理基础。 论文集ACL","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"http://yezhejack.github.io/categories/NLP/"}],"tags":[{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://yezhejack.github.io/tags/Natural-Language-Processing/"},{"name":"入门计划","slug":"入门计划","permalink":"http://yezhejack.github.io/tags/入门计划/"}]},{"title":"STL学习笔记-4-序列式容器","slug":"STL学习笔记-4-序列式容器","date":"2016-04-16T13:32:58.000Z","updated":"2016-05-04T14:20:03.000Z","comments":true,"path":"2016/04/16/STL学习笔记-4-序列式容器/","link":"","permalink":"http://yezhejack.github.io/2016/04/16/STL学习笔记-4-序列式容器/","excerpt":"","keywords":null,"text":"vectorvector实际上是一个大小不定的线性空间。 vector提供的是Random Access Iterators。 如果加入新的元素时，空间不足以容纳，就会去请求更大的空间，来容纳。 12345678void push_back(const T&amp; x) &#123; if (finish != end_of_storage) &#123; construct(finish, x); ++finish; &#125; else insert_aux(end(), x); &#125; 1234567891011121314151617181920212223242526272829303132333435template &lt;class T, class Alloc&gt;void vector&lt;T, Alloc&gt;::insert_aux(iterator position, const T&amp; x) &#123; if (finish != end_of_storage) &#123; construct(finish, *(finish - 1)); ++finish; T x_copy = x; copy_backward(position, finish - 2, finish - 1); *position = x_copy; &#125; else &#123; const size_type old_size = size(); const size_type len = old_size != 0 ? 2 * old_size : 1; iterator new_start = data_allocator::allocate(len); iterator new_finish = new_start; __STL_TRY &#123; new_finish = uninitialized_copy(start, position, new_start); construct(new_finish, x); ++new_finish; new_finish = uninitialized_copy(position, finish, new_finish); &#125;# ifdef __STL_USE_EXCEPTIONS catch(...) &#123; destroy(new_start, new_finish); data_allocator::deallocate(new_start, len); throw; &#125;# endif /* __STL_USE_EXCEPTIONS */ destroy(begin(), end()); deallocate(); start = new_start; finish = new_finish; end_of_storage = new_start + len; &#125;&#125; listlist是一种链表。 list.sort()STL为list设计的sort算法速度及其高，但是占用内存还挺多的。因为它建立65个空的list来作为中间介质。 1234567891011121314151617181920template &lt;class T, class Alloc&gt;void list&lt;T, Alloc&gt;::sort() &#123; if (node-&gt;next == node || link_type(node-&gt;next)-&gt;next == node) return; list&lt;T, Alloc&gt; carry; list&lt;T, Alloc&gt; counter[64]; int fill = 0; while (!empty()) &#123; carry.splice(carry.begin(), *this, begin()); int i = 0; while(i &lt; fill &amp;&amp; !counter[i].empty()) &#123; counter[i].merge(carry); carry.swap(counter[i++]); &#125; carry.swap(counter[i]); if (i == fill) ++fill; &#125; for (int i = 1; i &lt; fill; ++i) counter[i].merge(counter[i-1]); swap(counter[fill-1]);&#125; 对应的counter[i]在使用的时候会存储2^i个元素，否则就存储0个元素。大量使用了merge，速度极快。原书中作者注释有误，此处应该是归并排序而非quick sort（快排）。 dequedeque从逻辑上来看，是连续空间的，同vector不同的在于它可以在连续空间的两端进行操作，而且deque在空间需要增长的时候不像vector需要大量的操作，因为它的空间连续性是个假象。因此，deque的最大任务便是在这些分段的定量连续空间上，维护其整体连续的假象，并提供随机存取的接口。避开了“重新配置、复制、释放”的轮回，代价则是复杂的迭代器架构。 deque采用一块所谓的map作为主控，这里所谓的map是一小块连续空间，其中每个元素都是一个指针，指向另一段连续线性空间，称为缓冲区。缓冲区才是deque的储存空间主体。SGI STL允许我们指定缓冲区大小，默认值0表示将使用512bytes缓冲区。","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://yezhejack.github.io/tags/C/"},{"name":"STL","slug":"STL","permalink":"http://yezhejack.github.io/tags/STL/"},{"name":"学习笔记","slug":"学习笔记","permalink":"http://yezhejack.github.io/tags/学习笔记/"}]},{"title":"python的web server","slug":"python的web server","date":"2016-04-13T11:21:58.000Z","updated":"2016-05-04T14:18:06.000Z","comments":true,"path":"2016/04/13/python的web server/","link":"","permalink":"http://yezhejack.github.io/2016/04/13/python的web server/","excerpt":"","keywords":null,"text":"shebang这个就是放在python代码的第一句。 1#!/usr/bin/env python 这个是为了让程序找到python的位置，如果这句话不起作用的话，可以直接使用完整的python路径。 CGI(Common Gateway Interface)服务器接收到动态请求的时候，请求CGI脚本，然后启动python程序，将URL请求转换为python的标准输入，然后从python程序的标准输出中获得返回内容。对于CGI来说，它是将python解释器嵌入到服务器本身中。 FastCGI &amp; SCGI这两种都是通过服务器本身和后台进程的交流来实现动态内容请求。SCGI可以理解为是一种simpler FastCGI。鉴于现在大多数的Web Server对其支持的缺乏，大家更倾向于使用FastCGI。 WSGIWSGI实际上是一个类似标准的东西，是对一种中间件的描述，当HTTP Server按照这个标准提供支持，应用本身也根据这个标准提供支持后，这两者就可以通过WSGI来进行沟通。需要强调的是这个是为python定制的标准。","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yezhejack.github.io/tags/python/"},{"name":"Web Server","slug":"Web-Server","permalink":"http://yezhejack.github.io/tags/Web-Server/"}]},{"title":"STL学习笔记-3－迭代器(iterators)概念与traits编程技法","slug":"STL学习笔记-3-1~3-3","date":"2016-04-13T07:04:55.000Z","updated":"2016-05-04T14:19:46.000Z","comments":true,"path":"2016/04/13/STL学习笔记-3-1~3-3/","link":"","permalink":"http://yezhejack.github.io/2016/04/13/STL学习笔记-3-1~3-3/","excerpt":"","keywords":null,"text":"auto_ptr1void remodel(string &amp; str) &#123; string * ps = new string(str); str = ps; return; &#125; 这段代码会造成内存泄漏，我们会想说在函数return之前记得delete ps即可避免内存泄漏。 123void remodel(string &amp; str) &#123; string * ps = new string(str); if (weird_thing()) throw exception(); str = *ps; delete ps; return; &#125; 但是这段代码在exception被抛出之后，delete这个句子不会被执行，也就造成了内存泄漏。 使用一个auto_ptr类型的话，在函数返回时，因为auto_ptr有自己的析构函数，所以会自己释放自己指向的内存空间。auto_ptr不能用于数组。 You should use an auto_ptr object only for memory allocated by new, not for memory allocated by new [] or by simply declaring a variable. 赋值操作在正常的指针操作中，指针指间的相互赋值实际上是让多个指针同时指向一个内存空间。但是这个应用到auto_ptr上时就会有问题了，但删除两个auto_ptr时会导致程序对同一个内存空间进行了多次的delete操作。 为了避免这一个问题，可以用下面几个策略来避免： 让赋值操作变成deep copy，这会让两个指针指向不同的内存空间，其中一个将作为另一个的拷贝。 构建ownership概念，也就是拥有者概念。一个对象只能有一个智能指针拥有，只有拥有这个对象的智能指针能析构这个对象。赋值的同时转换拥有权。 创建一个更加智能的指针，对指向特定对象的指针进行跟踪。这个叫做reference counting。 当一个指针放弃了自己的拥有权之后，它可能会转变为不可用的状态。 iterator 是一种smart pointerexplicit在类声明中使用这个可以避免隐形的转换。例如 123456789class Star&#123;...public: explicit Start(const char*);...&#125;; Star north;north=\"polaris\";//错误的使用方式north=Star(\"polaris\");//正确的使用方式 偏特化两个说法 提供一份template定义式，其本身仍然是templaized。 针对任何template参数更进一步的条件限制所设计出来的一个特化版本。 例如下面的代码接受任意类型 12template &lt;typename T&gt;class C&#123;...&#125; 而下面的代码只接受原生指针 12template &lt;typename T&gt;class C&lt;T&gt; &#123;...&#125; 下面是一段用于类型萃取的代码 1234template &lt;class I&gt;struct iterator_traits&#123; //traits意为特性 typedef typename I::value_type value_type&#125; 如果class I有自己的value_type的话 12345template&lt;class I&gt;typename iterator_traits&lt;I&gt;::valuetype //函数的返回类型func(I ite)&#123; return *ite;&#125; 但是这个萃取器对于原生指针，像int* I这样的类型并不起作用，因为它并不是由用户定义的，因此并没有在内部定义一个value_type。因此需要一个特化版本的iterator_traits。如下所示 1234template&lt;typename T&gt;struct iterator_traits&lt;T*&gt;&#123; typedef T value_type;&#125; 即使有这两个萃取器，对于像指向const int的指针类型的时候，我们只能获得一个无法更改的返回值。因此我们还需要另一个特化的萃取器。 1234template&lt;typename T&gt;struct iterator_traits&lt;const T*&gt;&#123; typedef T value_type;&#125; 最常用的迭代器(iterator)类型有物种: value type difference type pointer reference iterator catagoly 3.4.1 value type任何一个打算与STL配合的class都需要定义自己的value type内嵌型别。 3.4.2 difference typedifference type表示两个迭代器之间的距离。因此它可以用来表示一个容器的最大容量。如果需要提供一个计数功能count()的话，返回值就需要是difference type 12345678910template&lt;class I,class T&gt;typename iterator_traits&lt;I&gt;::difference_typecount(I first,I last,const T&amp; value)&#123; typename iterator_traist&lt;I&gt;::difference_type n=0; for (;first!=last;first++)&#123; if (*first==value) ++n; return n; &#125;&#125; 接着可以给出一个泛化版本的和两个特化版本的difference type的萃取器。 12345//泛化版本template &lt;class I&gt;struct iterator_traits&#123; typedef typename I::differene_type difference_type;&#125; 12345//为原生指针特化的版本template &lt;class T&gt;struct iterator_traits&lt;T*&gt;&#123; typedef ptrddif_t difference_type;&#125; 12345//为pointer to const偏特化的template &lt;class T&gt;struct iterator_traits&lt;const T*&gt;&#123; typedef ptrddif_t difference_type;&#125; 有了这三个萃取器，我们想要任何迭代器的difference type的时候我们只需要一下代码: 1typename iterator_traits&lt;class I&gt;::difference_type; reference type从迭代器所指之物的内容是否允许改变来看，可以将迭代器分为两种: 不允许改变所指对象之内容，称为constant iterators。例如const int* pic。 允许改变所指对象之内容，称为mutable iterators。例如int* pi。 当p是一个mutable iterators时，如果其value type是T，那么*p的型别不应是T，而应该是&amp;T。以此类推，如果p是一个constant iter，其value type是T，那么*p的型别应该是const T&amp;。这里所讨论的*p就是所谓的reference type。 pointer type12Item&amp; operator* () const&#123; return *ptr;&#125;Item* operator-&gt;() const &#123; return ptr;&#125; Item&amp;便是reference type，而Item*便是其pointer type。 直接献上完整的代码(Orz): 1234567891011121314151617181920212223242526template &lt;class Iterator&gt;struct iterator_traits &#123; typedef typename Iterator::iterator_category iterator_category; typedef typename Iterator::value_type value_type; typedef typename Iterator::difference_type difference_type; typedef typename Iterator::pointer pointer; typedef typename Iterator::reference reference;&#125;;template &lt;class T&gt;struct iterator_traits&lt;T*&gt; &#123; typedef random_access_iterator_tag iterator_category; typedef T value_type; typedef ptrdiff_t difference_type; typedef T* pointer; typedef T&amp; reference;&#125;;template &lt;class T&gt;struct iterator_traits&lt;const T*&gt; &#123; typedef random_access_iterator_tag iterator_category; typedef T value_type; typedef ptrdiff_t difference_type; typedef const T* pointer; typedef const T&amp; reference;&#125;; iterator_category根据移动特性与施行操作，迭代器被分为五类: input iterator:这种迭代器所指的对象，不允许外界改变。只读(read only)。 output iterator:write only。 forward iterator:允许“写入型”算法（例如replace()）在此迭代器所形成的区间上进行读写操作。 bidirectional iterator:可双向移动。某些算法需要逆向走访某个迭代器区间（例如逆向拷贝某范围内的元素），可以使用这种迭代器。 random access iterator:前四种迭代器都只供应一部分指针算数能力（前三种支持operator++，第四种再加上operator–），第五种则涵盖所有指针算数能力，包括p+n``p-n``p[n]``p1-p2``p1&lt;p2。 advance()这是许多算法内部常用的一个函数，该函数有两个参数p和n。表示函数将p累进n次。下面有三份定义，分别针对Input Iterator``Bidirectional Iterator``Random Access Iterator。而对ForwardIterator来说，它的版本与Input Iterator的版本一致。 123456template &lt;class InputIterator, class Distance&gt;void advance_II(InputIterator&amp; i,Distance n)&#123; //单向，逐一前进 while(n--) ++i;&#125; 123456789template &lt;class BidirectionalIterator, class Distance&gt;void advance_BI(BidirectionalIterator&amp; i,Distance n)&#123; //双向，逐一前进 if (n&gt;=0) while (n--) ++i; else while (n++) --i;&#125; 123456template &lt;class RandomAccessIterator, class Distance&gt;void advance_RAI(RandomAccessIterator&amp; i,Distance n)&#123; //双向，跳跃前进 i+=n;&#125; 但是这样的话我们还需要一个函数来判断迭代器的类型然后调用相应的函数。 定义五个classes 12345struct input_iterator_tag &#123;&#125;;struct output_iterator_tag &#123;&#125;;struct forward_iterator_tag : public input_iterator_tag &#123;&#125;;struct bidirectional_iterator_tag : public forward_iterator_tag &#123;&#125;;struct random_access_iterator_tag : public bidirectional_iterator_tag &#123;&#125;; 设计五个用于内部调用的函数，最后一个参数只是为了激活函数的重载。 1234template &lt;class InputIterator, class Distance&gt;inline void __advance(InputIterator&amp; i, Distance n, input_iterator_tag) &#123; while (n--) ++i;&#125; 12345678template &lt;class BidirectionalIterator, class Distance&gt;inline void __advance(BidirectionalIterator&amp; i, Distance n, bidirectional_iterator_tag) &#123; if (n &gt;= 0) while (n--) ++i; else while (n++) --i;&#125; 12345template &lt;class RandomAccessIterator, class Distance&gt;inline void __advance(RandomAccessIterator&amp; i, Distance n, random_access_iterator_tag) &#123; i += n;&#125; 这里使用的方法是添加一个单纯的调用函数advance()，这个函数只接受两个参数，当它将工作转向__advance()的时候，自行通过traits机制加上第三个参数，也就是前面定义的五个classes。 1234template &lt;class InputIterator, class Distance&gt;inline void advance(InputIterator&amp; i, Distance n) &#123; __advance(i, n, iterator_category(i));&#125; 然后再定义函数iterator_category()。 123456template &lt;class Iterator&gt;inline typename iterator_traits&lt;Iterator&gt;::iterator_categoryiterator_category(const Iterator&amp;) &#123; typedef typename iterator_traits&lt;Iterator&gt;::iterator_category category; return category();&#125; 可以注意到我们在列举__advance()函数的时候，少列举了两种，一种是OutputIterator，另一种是ForwardIterator。前者是因为可以同InputIterator共用，后者是因为可以消除单纯只做传递调用的函数，所以就没有对应的__advance()函数了。 __types_traits这个技巧类似于前面的iterator_traits，都是用于解析特征。这里需要解析看这个类型能否使用更快速的内存上的操作进行操作，避免使用高层次的函数。 使用场景：uninitialized_fill_n()12345template &lt;class ForwardIterator, class Size, class T&gt;inline ForwardIterator uninitialized_fill_n(ForwardIterator first, Size n, const T&amp; x) &#123; return __uninitialized_fill_n(first, n, x, value_type(first));&#125; 上面这个函数以x为蓝本，自迭代器first开始构造n个元素。为求取最大效率，首先以value_type()萃取出迭代器first的value type，再利用__type_traits判断该型别是否为POD型别： 1234567template &lt;class ForwardIterator, class Size, class T, class T1&gt;inline ForwardIterator __uninitialized_fill_n(ForwardIterator first, Size n, const T&amp; x, T1*) &#123; typedef typename __type_traits&lt;T1&gt;::is_POD_type is_POD; return __uninitialized_fill_n_aux(first, n, x, is_POD()); &#125; 接着调用了上面这个函数，其中is_POD()是一个struct，然后调__uninitialized_fill_n_aux函数，其中编译器会根据is_POD()来重载函数，分配到是POD和不是POD对应的函数中。","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://yezhejack.github.io/tags/C/"},{"name":"STL","slug":"STL","permalink":"http://yezhejack.github.io/tags/STL/"},{"name":"学习笔记","slug":"学习笔记","permalink":"http://yezhejack.github.io/tags/学习笔记/"}]},{"title":"STL学习笔记-2.3内存基本处理工具","slug":"STL学习笔记-2-3内存基本处理工具","date":"2016-04-12T08:15:20.000Z","updated":"2016-05-04T14:19:33.000Z","comments":true,"path":"2016/04/12/STL学习笔记-2-3内存基本处理工具/","link":"","permalink":"http://yezhejack.github.io/2016/04/12/STL学习笔记-2-3内存基本处理工具/","excerpt":"","keywords":null,"text":"2.3 内存基本处理工具这里面主要包括了三个函数 uninitialized_copy() uninitialized_fill() uninitialized_fill_n() 对应着高层次的函数 copy() fill() fill_n() 这些都是STL算法。 uninitialized_copy()讲某一区间的内容复制到另一个区间： 配置内存区块，足以包含范围内的所有元素 使用该函数，在该内存区块上构造元素 C++标准中要求这个函数具有“commit or rollback”的语义，其实也就是让这个操作原子话，当某个copy constructor失败时回滚所有已发生的操作。 接受三个参数: 迭代器first指向输入端的起始位置 迭代器last指向输入端的结束位置（前闭后开区间）。 迭代器result指向输出端（欲初始化空间）的起始处。 uninitialized_fill()对于范围内的所有区块，都构造一个给定的内存内容。同样要求有原子性。 uninitialized_fill_n()类似前者，从first位置开始，构造n个第三参数的复制品。同样也在标准中要求原子性 本函数接受三个参数: 迭代器first指向欲初始化空间的起始处。 n表示欲初始化空间的大小 x表示初值 12345template &lt;class ForwardIterator, class Size, class T&gt;inline ForwardIterator uninitialized_fill_n(ForwardIterator first, Size n, const T&amp; x) &#123; return __uninitialized_fill_n(first, n, x, value_type(first));&#125; 这段代码先取出迭代器first的value type，然后判断该类别是否为POD类别。POD也就是Plain Old Data也就是标量型或传统的C struct类别。POD类别拥有trivial ctor／dtor／copy／assignment函数，因此对POD类别采用最有效率的初值填写手法，而对非POD类别采用最保险安全的做法。","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://yezhejack.github.io/tags/C/"},{"name":"STL","slug":"STL","permalink":"http://yezhejack.github.io/tags/STL/"},{"name":"学习笔记","slug":"学习笔记","permalink":"http://yezhejack.github.io/tags/学习笔记/"}]},{"title":"STL学习笔记-1.1~2.2","slug":"STL学习笔记-1.1~2.2","date":"2016-04-09T14:20:32.000Z","updated":"2016-05-04T14:19:05.000Z","comments":true,"path":"2016/04/09/STL学习笔记-1.1~2.2/","link":"","permalink":"http://yezhejack.github.io/2016/04/09/STL学习笔记-1.1~2.2/","excerpt":"","keywords":null,"text":"笔记p45出现了::operator new和::operator delete，这个地方比较难理解。首先，先把new和delete当作一个操作符，因此要重载它的时候需要加个operator。 1::operator new 这个语句的意思是在全局命名空间下的new操作符。 重要概念模版特化函数模版特化比如设置了一个带有模版的函数，但是我想对其中的某种类型参数的函数进行单独定义。如果不含有这个机制的话，编译器会将其匹配到原始的模版函数中。例如 12345template &lt;class T&gt;T max(T a,T b)&#123; return a&lt;b ? b:a;&#125; 如果我想使用这个模版函数来处理字符串 123char* p1=\"hello\";char* p2=\"world\";char* p3=max(p1,p2); 这时候会单纯地比较p1和p2两个指针数值本身的大小，而非对应的字符串。所以我们需要特化。 12345template&lt;&gt;char* max(char* a,char* b)&#123; returan (strcmp(a,b)&lt;0)?b:a;&#125; 这样才能让程序正确地被编译出来。 类模版特化1234template &lt;class T&gt;class stack &#123;&#125;;template &lt; &gt;class stack&lt;bool&gt; &#123; //…// &#125;; 偏特化偏特化的意思就是对某些参数给定类型的时候给出特化方案。 类模版特化 1234template &lt;class T, class Allocator&gt;class vector &#123; // … // &#125;;template &lt;class Allocator&gt;class vector&lt;bool, Allocator&gt; &#123; //…//&#125;; 函数模版特化 严格的来说，函数模板并不支持偏特化，但由于可以对函数进行重载，所以可以达到类似于类模板偏特化的效果。 1template &lt;class T&gt; void f(T); (a) 根据重载规则，对（a）进行重载 1template &lt; class T&gt; void f(T*); (b) 如果将（a）称为基模板，那么（b）称为对基模板（a）的重载，而非对（a）的偏特化。C++的标准委员会仍在对下一个版本中是否允许函数模板的偏特化进行讨论。 memorystd::allocator这个文件是旧式做法的配置器。不建议使用它，所有的SGI STL头文件都没有包含它。 SGI的std::allocator只是对::operator new和::operator delete做了一层薄薄的包装。 stl_alloc.h 负责内存空间的配置与释放 包含了一二级的配置器，彼此合作。 设计哲学 向system heap要求空间 考虑多线程（multi-threads)状态。 考虑内存不足时的应变措施。 考虑过多的“小型区块”可能造成的内存碎片（fragment）问题。 当需要的区块大于128 Bytes的时候，会使用一级配置器来分配内存，如果是小于的话，则利用二级配置器，从free list中获取。 SGI STL缺省使用的空间配置器并不是标准的allocator，而是alloc。它在所有的需要空间配置器的类中使用的都是alloc。 例如 12template &lt;class T,class Alloc=alloc&gt;class vector &#123;...&#125;; 一级配置器:__malloc_alloc_template 二级配置器:__default_alloc_template 无论是一级还是二级配置器，SGI都为其包装了一层薄薄的借口，使之能够符合STL的标准: 12345678910111213template&lt;class T, class Alloc&gt;class simple_alloc &#123;public: static T *allocate(size_t n) &#123; return 0 == n? 0 : (T*) Alloc::allocate(n * sizeof (T)); &#125; static T *allocate(void) &#123; return (T*) Alloc::allocate(sizeof (T)); &#125; static void deallocate(T *p, size_t n) &#123; if (0 != n) Alloc::deallocate(p, n * sizeof (T)); &#125; static void deallocate(T *p) &#123; Alloc::deallocate(p, sizeof (T)); &#125;&#125;; free list这个结构存储这个多个区块，同时拥有一个节点的数据结构 1234union obj &#123; union obj * free_list_link; char client_data[1]; /* The client sees this. */ &#125;; 这个结构的就只包含一个指针，在64位系统中，这个指针的大小位8个字节。因此在free list中，如果想要存储小于8个字节的区块时是会失败的。每个区块实际上都存着一个obj结构。这里的设计的妙处在于，我们通常都会用一个节点结构，这个节点结构后面再挂着实际区块。例如 1234struct obj&#123; struct obj* next_obj; char* addr;&#125;; chunk_alloc()这样我们就需要16个字节来表示一个区块，而且这16个字节还不是区块本身的字节。因此孰优孰劣，一目了然。 stl_construct.h负责对象内容的构造与析构 其中的__type_traits&lt;&gt; 将在3.7节中有详细介绍。","raw":null,"content":null,"categories":[{"name":"Learning","slug":"Learning","permalink":"http://yezhejack.github.io/categories/Learning/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://yezhejack.github.io/tags/C/"},{"name":"STL","slug":"STL","permalink":"http://yezhejack.github.io/tags/STL/"}]},{"title":"安装libsvm和gnuplot","slug":"安装libsvm和gnuplot","date":"2016-03-28T09:07:22.000Z","updated":"2016-05-04T14:10:32.000Z","comments":true,"path":"2016/03/28/安装libsvm和gnuplot/","link":"","permalink":"http://yezhejack.github.io/2016/03/28/安装libsvm和gnuplot/","excerpt":"","keywords":null,"text":"安装libsvm安装 直接下载源码，然后进入目录make 安装gnuplot安装 下载源码 1234cd 目录./configuremakesudo make install 可能遇到的问题 我是在使用libsvm的easy.py来测试的时候遇到无法检测出gnuplot存在，查其原因，可能是因为在/usr/bin目录下没有看到gnuplot，而是在/usr/local/bin中有。因此需要找到方法让别的程序能找到安装的gnuplot。 解决方案：将/usr/local/bin/gnuplot拷贝到/usr/bin下。或者，将easy.py中的关于gnuplot的路径改为/usr/local/bin。","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"libsvm","slug":"libsvm","permalink":"http://yezhejack.github.io/tags/libsvm/"},{"name":"gnuplot","slug":"gnuplot","permalink":"http://yezhejack.github.io/tags/gnuplot/"}]},{"title":"nginx","slug":"nginx","date":"2015-12-21T16:26:18.000Z","updated":"2016-07-21T08:39:03.000Z","comments":true,"path":"2015/12/22/nginx/","link":"","permalink":"http://yezhejack.github.io/2015/12/22/nginx/","excerpt":"","keywords":null,"text":"123456789101112131415161718+ using PCRE library: /mnt/application/pcre-8.32 + OpenSSL library is not used + using builtin md5 code + sha1 library is not found + using zlib library: /mnt/application/zlib-1.2.8 nginx path prefix: \"/usr/local/nginx\" nginx binary file: \"/usr/local/nginx/sbin/nginx\" nginx configuration prefix: \"/usr/local/nginx/conf\" nginx configuration file: \"/usr/local/nginx/conf/nginx.conf\" nginx pid file: \"/usr/local/nginx/logs/nginx.pid\" nginx error log file: \"/usr/local/nginx/logs/error.log\" nginx http access log file: \"/usr/local/nginx/logs/access.log\" nginx http client request body temporary files: \"client_body_temp\" nginx http proxy temporary files: \"proxy_temp\" nginx http fastcgi temporary files: \"fastcgi_temp\" nginx http uwsgi temporary files: \"uwsgi_temp\" nginx http scgi temporary files: \"scgi_temp\" 从源码安装nginx拷贝nginx 到sbin分析access.log 访问量access.log是记录了所有的nginx的访问链接，并且纪录了这些访问的结果。可以使用python中的正则表达式来对其进行解析。这里需要python的re模块 123456789101112131415161718import reip = r\"?P&lt;ip&gt;[\\d.]*\"date = r\"?P&lt;date&gt;\\d+\"month = r\"?P&lt;month&gt;\\w+\"year = r\"?P&lt;year&gt;\\d+\"log_time = r\"?P&lt;time&gt;\\S+\"method = r\"?P&lt;method&gt;\\S+\"request = r\"?P&lt;request&gt;\\S+\"status = r\"?P&lt;status&gt;\\d+\"bodyBytesSent = r\"?P&lt;bodyBytesSent&gt;\\d+\"refer = r\"\"\"?P&lt;refer&gt; [^\\\"]* \"\"\"userAgent=r\"\"\"?P&lt;userAgent&gt; .* \"\"\"p = re.compile(r\"(%s)\\ -\\ -\\ \\[(%s)/(%s)/(%s)\\:(%s)\\ [\\S]+\\]\\ \\\"(%s)?[\\s]?(%s)?.*?\\\"\\ (%s)\\ (%s)\\ \\\"(%s)\\\"\\ \\\"(%s).*?\\\"\" %( ip, date, month, year, log_time, method, request, status, bodyBytesSent, refer, userAgent ), re.VERBOSE)m = re.findall(p, line) 分割access.log安装方法1以ubuntu 14.04为例，其中的codename是trusty因此需要把下面的代码追加到12 deb http://nginx.org/packages/ubuntu/ trusty nginxdeb-src http://nginx.org/packages/ubuntu/ trusty nginx12然后运行 sudo apt-get updatesudo apt-get install nginx123456这个安装方式有问题的### 安装方法2 ####### 安装pcre #### cd /usr/local/srcsudo wget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-8.37.tar.gzsudo tar -zxvf pcre-8.37.tar.gzcd pcre-8.34sudo ./configuresudo makesudo make install12#### 安装zlib #### cd /usr/local/srcsudo wget http://zlib.net/zlib-1.2.8.tar.gzsudo tar -zxvf zlib-1.2.8.tar.gzcd zlib-1.2.8/sudo ./configuresudo makesudo make install12#### 安装ssl #### cd /usr/local/srcsudo wget https://www.openssl.org/source/openssl-1.0.1t.tar.gzsudo tar -zxvf openssl-1.0.1t.tar.gz12#### 安装 nginx #### cd /usr/local/srcwget http://nginx.org/download/nginx-1.8.1.tar.gztar -zxvf nginx-1.8.1.tar.gzcd nginx-1.8.1.tar.gzsudo ./configure –sbin-path=/usr/local/nginx/nginx –conf-path=/usr/local/nginx/nginx.conf –pid-path=/usr/local/nginx/nginx.pid –with-http_ssl_module –with-pcre=/usr/local/src/pcre-8.37 –with-zlib=/usr/local/src/zlib-1.2.8 –with-openssl=/usr/local/src/openssl-1.0.1tsudo makesudo make install```","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"Server","slug":"Server","permalink":"http://yezhejack.github.io/tags/Server/"},{"name":"Nginx","slug":"Nginx","permalink":"http://yezhejack.github.io/tags/Nginx/"}]},{"title":"redis","slug":"redis","date":"2015-12-20T08:23:15.000Z","updated":"2016-05-27T06:01:13.000Z","comments":true,"path":"2015/12/20/redis/","link":"","permalink":"http://yezhejack.github.io/2015/12/20/redis/","excerpt":"","keywords":null,"text":"安装1234wget http://download.redis.io/releases/redis-3.0.6.tar.gztar xzf redis-3.0.6.tar.gzcd redis-3.0.6make 启动redis1src/redis-server 会在前台启动，然后再启动客户端来进行测试1src/redis-cli 可以在测试端里写1set foo bar 然后再写1get foo bar 分别对应着存取操作 安装redis-py1sudo easy_install redis 安装hiredis1pip install hiredis 这个parser的效率比较高 后台启动redis修改redis目录下的redis.conf文件，将其中的1daemonize no 修改为1daemonize yes 然后关闭刚才在前台启动的redis-server，进入到redis的目录1src/redis-server ./redis.conf 即可启动redis 关闭后台redis打开redis-cli，输入1shutdown save 即可安全退出redis，后面的save表示会保存之后再退出，如果是nosace则是马上退出，有可能会存在数据丢失 配置redis安全选项12rename-command FLUSHALL BENBENREDISFLUSHALLrename-command FLUSHDB BENBENREDISFLUSHDB 将FLUSHALL操作变为BENBENREDISFLUSHALL,将FLUSHDB变为BENBENREDISFLUSHDB,以降低开发过程中不小心将数据全部清空的概率 python 操作初始化123import redisr=redis.StrictRedis(host='localhost',port=6379,db=0) listlpush &amp;&amp; rpushlpush对应在list头加入元素，rpush对应在list尾加入元素lrange可以取出对应范围的元素比如要往一个叫做alist的list里append一个数据，可以直接用1r.rpush('alist',value1,value2...) 后面跟至少一个参数运行1r.lrange('alist',0,-1) 取出alist中的所有数据，返回类型是一个string类型的list1r.lindex('alist',pos) 等价于alist[pos] dicthashpython中的dict类型对应在redis中的类型是hash hmset这是可以直接把python中的dict一次性存入redis的命令12dict_a=&#123;'a':1,'b':2&#125;r.hmset('dict_a',dict_a) 这就直接把一个叫做dict_a的dict存入了redis中，并且这个名字保持一致 hgetall想要一次性把dict_a全部取出来1dict_a=r.hgetall('dict_a') hget读取key对应的value1value=r.hget('dict_a','a') hdel用来删除dict中对应的键值1r.hdel('dict_a','key') mset mget```r.mset({‘name1’:value1,’name2’:value2})r.mget(‘name1’,’name2’)","raw":null,"content":null,"categories":[{"name":"Programmer","slug":"Programmer","permalink":"http://yezhejack.github.io/categories/Programmer/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yezhejack.github.io/tags/python/"},{"name":"redis","slug":"redis","permalink":"http://yezhejack.github.io/tags/redis/"},{"name":"内存","slug":"内存","permalink":"http://yezhejack.github.io/tags/内存/"},{"name":"内存型数据库","slug":"内存型数据库","permalink":"http://yezhejack.github.io/tags/内存型数据库/"}]},{"title":"linux常见操作","slug":"linux常见操作","date":"2015-12-20T08:23:15.000Z","updated":"2017-07-01T17:45:12.000Z","comments":true,"path":"2015/12/20/linux常见操作/","link":"","permalink":"http://yezhejack.github.io/2015/12/20/linux常见操作/","excerpt":"","keywords":null,"text":"用户管理增加一个用户1useradd -g &lt;初始组名称&gt; -G &lt;附属组名称&gt; -m -d /home/&lt;username&gt; -s /bin/bash &lt;username&gt; 12useradd -g benben -G sudo -m -d /home/zhangsan -s /bin/bash zhangsanpasswd zhangsan 上面新建了一个用户，属于笨笨组的，同时也属于sudo组，拥有sudo权限，是个拥有很高权限的管理员，仅次于root。同时他的目录定义在了/home/zhangsan，同时将shell指定为/bin/bash，第二句话会设置一个密码。 如果只是一个普通的用户，不希望让他拥有sudo权限的话，将-G参数去掉，只让他属于benben组即可。这样这个帐户就无法安装东西，必须联系管理员才能进行安装。 公共工作空间为了让大家共同合作，可以利用管理员账户建立一个目录，这个目录的组别属于benben，也就是需要合作的人的共同组别。然后将这个目录的属于组修改为benben。 1chgrp -R benben /mnt/benben 然后再修改这个目录的权限为775，也就是rwx rwx r_x，拥有者和组成员拥有读、写、执行权限，而其他人只拥有读、执行权限。 1chmod 775 /mnt/benben 为用户增加sudo权限登陆到root账号，然后输入下面的命令，以zhangsan这个用户为例 1usermod -G sudo zhangsan 也可以在/etc/group中在sudo后面加上zhangsan 增加用户到另一个用户组中我新建了一个用户yezhe，然后想要把它增加到用户组sudo中，以便于使用sudo命令，这里需要使用root用户 1usermod -G sudo yezhe 更改用户的工作目录需要用root权限 1usermod -d /home/yezhe yezhe 如果出现一个用户在命令行无法显示名字和路径这个情况下我在更改了/etc/passwd之后就可以了，也就是给它添加一个shell，一般默认是和root一样用同一个shell:/bin/bash screen设置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# Set default encoding using utf8defutf8 on## 解决中文乱码,这个要按需配置defencoding utf8encoding utf8 utf8 #兼容shell 使得.bashrc .profile /etc/profile等里面的别名等设置生效shell -$SHELL#set the startup messagestartup_message offterm linux## 解决无法滚动termcapinfo xterm|xterms|xs ti@:te=\\E[2J # 屏幕缓冲区行数defscrollback 10000 # 下标签设置hardstatus oncaption always \"%&#123;= kw&#125;%-w%&#123;= kG&#125;%&#123;+b&#125;[%n %t]%&#123;-b&#125;%&#123;= kw&#125;%+w %=%d %M %0c %&#123;g&#125;%H%&#123;-&#125;\" #关闭闪屏vbell off #Keboard binding# bind Alt+z to move to previous windowbindkey ^[z prev# bind Alt+x to move to next windowbindkey ^[x next# bind Alt`~= to screen0~12bindkey \"^[`\" select 0bindkey \"^[1\" select 1bindkey \"^[2\" select 2bindkey \"^[3\" select 3bindkey \"^[4\" select 4bindkey \"^[5\" select 5bindkey \"^[6\" select 6bindkey \"^[7\" select 7bindkey \"^[8\" select 8bindkey \"^[9\" select 9bindkey \"^[0\" select 10bindkey \"^[-\" select 11bindkey \"^[=\" select 12# bind F5 to create a new screenbindkey -k k5 screen# bind F6 to detach screen session (to background)bindkey -k k6 detach# bind F7 to kill current screen windowbindkey -k k7 kill``# bind F8 to rename current screen windowbindkey -k k8 title 在screen中，一个session中可以包含多个windows C-a A 用于修改当前标签的名字 C-a k 关闭当前windows 运维方案 这个方案以ubuntu 14.04 为例，并且是使用vultr的服务器。 修改root账号的密码 登录root账号 使用 passwd root命令，即可根据提示修改密码 增加个人用户，并使其拥有sudo权限123useradd -d /home/zhangsan -m zhangsanpasswd zhangsanusermod -G sudo zhangsan 第一行的命令同时为zhangsan这个账户增加了一个工作目录，第二行为修改密码，第三行为修改这个账户的权限，将其加入到sudo的组中，到此为止一个简单的linxu远程服务器的用户部分就已经配置好了。 查看进程1ps aux|grep &lt;name&gt; 查看主机最近一次的启动时间1who -b 在高性能服务器上跑训练标准输出重定向1command &gt; out.put 2&gt;&amp;1 command就是我们要跑的程序。out.put就是我们希望把屏幕上输出的内容存入的文件，比如我希望明天一早起来看到一个程序的结尾，但是我又不想改这个程序让其把最后的算法结果存入某个程序（其实就是懒）。2表示标准输出，而1表示标准错误输出。另外提一句0就是表示标准输入，这些都是一个进程的默认的文件描述符，unix中一切皆文件。 如果中间的符号改为&gt;&gt;的话，是以追加的方式写文件。 查看cpu信息1cat /proc/cpuinfo 使用nologin运行程序1sudo -u nologin &lt;cmd&gt; top命令改变RES（实际占用的物理内存大小）12e 是切换任务区域的E 切换总结区域的 查看文件夹大小1du -h --max-depth=1 公钥登录生成本地的公钥1ssh-keygen 然后将其复制到远程主机上1ssh-copy-id -p port username@remotehost 如果有问题的话，可能是.ssh和authorized_keys这两个东西的权限的有问题，正常情况下分别是700和600的权限。 可以在linux远程主机上的~/.profile里加上一句话screen -r &lt;name&gt;这样子可以一登录就重启之前启动好的screen。千万不能变成screen -R &lt;name&gt;。否则会陷入死循环。 alias 命令别名在实际使用中，需要登录的机器的名字太长了，所以可以设置一些别名，用这些别名来代替真正的命令行。 1vim ~/.bash_profile 然后输入 1alias to_host=\"ssh yezhe@remotehost\" 保存退出后让其生效 1source ~/.bash_profile 这时候使用1to_host 就可以直接登录了，而且还支持补全代码，也就是tab。 locale 设置生成空白文件1dd if=/dev/zero of=hello.txt bs=100M count=1 supervisorsupervisor是一个进程管理，在我们远程登录server的时候会启动一个bash，但是这个bash我们退出ssh之后就消失了，里面的进程也就消失了。于是我们需要一个进程管理，我们将想要启动的进程扔给它，由它来管理即可。screen可以是一个不错的进程管理，可以用来debug，因为它可以前端显示，但是本质也还是后台启动。 但是最近在使用一个开源工具，这个工具的crash的频率比较高，因此需要一个能够自动重启carsh进程的进程管理，那就是supervisor 安装1easy_install supervisor 这个命令可以在屏幕上打印出配置信息 1echo_supervisord_conf 然后将配置输入到/etc路径中，这个需要root权限1echo_supservisord_conf &gt; /etc/supervisord.conf 启动如果supdervirod的二进制文件是安装在系统的PATH下的话，是可以直接启动的 新建一个文件夹/etc/supervisor/然后在/etc/supervisord.conf中的include项中加入 12[include]files = /etc/supervisor/*.conf 然后填写一个需要启动的程序的配置文件12345678910111213141516[program:phantomjs]directory = /data/pyspider ; 程序的启动目录command = pyspider -c ./config/pyspider.json phantomjs ; 启动命令，可以看出与手动在命令行启动的命令是一样的autostart = true ; 在 supervisord 启动的时候也自动启动startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了autorestart = true ; 程序异常退出后自动重启startretries = 3 ; 启动失败自动重试次数，默认是 3user = yezhe ; 用哪个用户启动redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 falsestdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MBstdout_logfile_backups = 20 ; stdout 日志文件备份数; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件）stdout_logfile = /data/pyspider/log/phantomjs.log; 可以通过 environment 来添加需要的环境变量，一种常见的用法是修改 PYTHONPATH; environment=PYTHONPATH=$PYTHONPATH:/path/to/somewhere 输出日期12echo `date`echo `date +%Y-%m-%d-%H-%M-%S` cron command not found这个问题的原因在于cron在运行时的环境变量不一样，其中的$PATH会被 查看当前文件夹下的文件个数和文件夹个数1ls -l |grep \"^-\"|wc -l 转载自http://blog.sina.com.cn/s/blog_464f6dba01012vwv.html统计某文件夹下文件的个数 1ls -l |grep \"^-\"|wc -l 统计某文件夹下目录的个数 1ls -l |grep \"^ｄ\"|wc -l 统计文件夹下文件的个数，包括子文件夹里的 1ls -lR|grep \"^-\"|wc -l 如统计/home/han目录(包含子目录)下的所有js文件则： 1ls -lR /home/han|grep js|wc -l 或 ls -l \"/home/han\"|grep \"js\"|wc -l 统计文件夹下目录的个数，包括子文件夹里的 1ls -lR|grep \"^d\"|wc -l 说明： 1ls -lR 长列表输出该目录下文件信息(R代表子目录注意这里的文件，不同于一般的文件，可能是目录、链接、设备文件等) 1grep \"^-\" 这里将长列表输出信息过滤一部分，只保留一般文件，如果只保留目录就是 ^d 1wc -l 统计输出信息的行数，因为已经过滤得只剩一般文件了，所以统计结果就是一般文件信息的行数，又由于一行信息对应一个文件，所以也就是文件的个数。 ＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝如果只查看文件夹ls -d 只能显示一个.find -type d 可以看到子文件夹ls -lF |grep / 或 ls -l |grep &#39;^d&#39; 只看当前目录下的文件夹，不包括往下的文件夹 sudo无法解析主机名字修改/etc/hosts中的一行 先打开/etc/hostname，其中包括一个主机名字 然后在/etc/hosts中保证有一行 1127.0.1.1 &lt;主机名字&gt; 查看正在使用的端口1sudo netstat -anltp | grep \"LISTEN\" 安装nvidia GTX1080驱动1234sudo apt-get purge nvidia-*sudo add-apt-repository ppa:graphics-drivers/ppasudo apt-get updatesudo apt-get install nvidia-375 cuda 8.0 安装1234wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-debsudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.debsudo apt-get updatesudo apt-get install cuda 安装完成之后还需要添加环境变量1vim ~/.bashrc 在文件的最后添加两行 12export PATH=/usr/local/cuda-8.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;export LD_LIBRARY_PATH=/usr/local/cuda-8.0.61/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125; 然后在当前的bash中执行会让修改生效，或者可以打开新的bash1source ~/.bashrc cuda的下载链接cuda 7.5 for ubuntu 14.041http://developer.download.nvidia.com/compute/cuda/7.5/Prod/local_installers/cuda-repo-ubuntu1404-7-5-local_7.5-18_amd64.deb cudnn 5.0 for cuda 7.51http://developer.download.nvidia.com/compute/machine-learning/cudnn/secure/v5/prod/cudnn-7.5-linux-x64-v5.0-ga.tgz?autho=1491911109_39b0ac86b2d9e816a24be1fb8148a31c&amp;file=cudnn-7.5-linux-x64-v5.0-ga.tgz cudnn 5.1 for cude 8.01http://developer.download.nvidia.com/compute/machine-learning/cudnn/secure/v5.1/prod_20161129/8.0/cudnn-8.0-linux-x64-v5.1.tgz?autho=1491911192_c4362e4d2c81a2d3c463a819201b25e6&amp;file=cudnn-8.0-linux-x64-v5.1.tgz 错误如果出现了以下错误1/sbin/ldconfig.real: /usr/lib32/nvidia-375/libEGL.so.1 is not a symbolic link 可以使用下面的命令来尝试解决1234sudo mv /usr/lib/nvidia-375/libEGL.so.1 /usr/lib/nvidia-375/libEGL.so.1.orgsudo mv /usr/lib32/nvidia-375/libEGL.so.1 /usr/lib32/nvidia-375/libEGL.so.1.orgsudo ln -s /usr/lib/nvidia-375/libEGL.so.375.39 /usr/lib/nvidia-375/libEGL.so.1sudo ln -s /usr/lib32/nvidia-375/libEGL.so.375.39 /usr/lib32/nvidia-375/libEGL.so.1 建立http代理服务器1sudo pip install shadowsocks 然后建立新的json文件12345678&#123; \"server\":\"my_server_ip\", \"server_port\":8388, \"local_port\":1080, \"password\":\"barfoo!\", \"timeout\":600, \"method\":\"rc4-md5\"&#125; 启动sslocal1sudo sslocal -c yezhe.json -d start 123sudo apt-get install poliposudo service polipo stopsudo polipo socksParentProxy=localhost:1080 然后就可以在命令前面加上代理的信息http_proxy=http://localhost:81231http_proxy=http://localhost:8123 apt-get update 解压 .gz1gzip -d &lt;input&gt;","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yezhejack.github.io/tags/Linux/"},{"name":"服务器","slug":"服务器","permalink":"http://yezhejack.github.io/tags/服务器/"}]},{"title":"mpv播放器使用指南","slug":"mpv播放器使用指南","date":"2015-12-20T08:23:15.000Z","updated":"2016-08-28T13:46:54.000Z","comments":true,"path":"2015/12/20/mpv播放器使用指南/","link":"","permalink":"http://yezhejack.github.io/2015/12/20/mpv播放器使用指南/","excerpt":"","keywords":null,"text":"mpv播放器这是我觉得在mac os x平台上最好用的播放器了，耗电量正常情况下都很好。 homebrew安装解决字幕乱码在~/.config/mpv.conf中增加1subcp=enca:zh:cp936","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"mpv","slug":"mpv","permalink":"http://yezhejack.github.io/tags/mpv/"},{"name":"视频播放器","slug":"视频播放器","permalink":"http://yezhejack.github.io/tags/视频播放器/"}]},{"title":"Git以及GitHub的用法","slug":"git以及github的用法","date":"2015-12-10T09:48:00.000Z","updated":"2016-05-04T15:54:35.000Z","comments":true,"path":"2015/12/10/git以及github的用法/","link":"","permalink":"http://yezhejack.github.io/2015/12/10/git以及github的用法/","excerpt":"","keywords":null,"text":"删除远程分支当你刚刚在本地删除了一个分支，你想要让这个变化反应在远程的repo中的话1git push origin :&lt;branch name&gt; 假设你的branch的名字是simple那么需要的命令就是1git push origin :simple 推送新建分支到远程仓库在本地新建了一个分支，然后觉得这个分支写得好于是就可以推送到远程的仓库中。1git push origin &lt;branch name&gt; 从git中删除文件，但是保存在硬盘上1git rm --cached &lt;filename&gt; 重命名一个分支并且让这个作用到远程分支上123git branch -m simple devgit push origingit push origin :simple","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"版本管理","slug":"版本管理","permalink":"http://yezhejack.github.io/tags/版本管理/"},{"name":"Git","slug":"Git","permalink":"http://yezhejack.github.io/tags/Git/"},{"name":"GitHub","slug":"GitHub","permalink":"http://yezhejack.github.io/tags/GitHub/"}]},{"title":"python学习","slug":"python学习","date":"2015-12-10T09:22:36.000Z","updated":"2017-04-16T11:55:24.000Z","comments":true,"path":"2015/12/10/python学习/","link":"","permalink":"http://yezhejack.github.io/2015/12/10/python学习/","excerpt":"","keywords":null,"text":"xrangexrange和range函数的区别只在于节省空间，而且xrange是一个不透明的序列结构，range在使用的时候就已经创建了所有的序列，而xrange在使用到的时候才会有真正的值，常用于loop结构。它实际上只支持了索引，迭代和长度函数。 stripstring类型对应的函数，用于去除一些特殊符号，例如 \\n \\t 等，没有参数的话会去除多个，如果有参数的话则会根据参数去除。但是按照2.7版本的官方文档来说只是去除了空格，但是实际上还去除了换行符等，不知道是不是版本的问题。 argument parse getopt(C stytle) argparse 初始化 1234567import argparse#新建一个解析器（应该是这么翻译的）parser=argparse.ArgumentParser()#增加一个开关变量parser.add_argument(\"-v\",\"--verbose\",help=\"increate output verbosity\",action=\"store_true\")#获得变量args=parser.parse_args() 这个变量有一个缩写的名字-v，完整的名字是–verbose,并且是个开关变量，即标志，这里因为’action=”store_true”‘，因此有这个参数出现的时候，就代表了args.vebosity=True,当’action=count’时，会是一个计数器 使用args是一个带有所有参数变量的结构体（类） 限定输入参数的类型 1parser.add_argument(\"-v\",\"--verbosity\",type=int,help=\"balabalabala\") 位置参数和可选参数暂时还没搞懂这个区别，甚至是翻译是否正确也不知道(positional argument and optional argument) 默认参数可以用一个默认的和args相同尺寸的结构体来代替 12345parser=argparse.ArgumentParser(argument_default=argparse.SUPPRESS)parser.add_argument('--foo')parser.add_argument('bar',nargs='?')parser.parse_args(['--fool','1','BAR'])parser.parse_args([]) 还没实验，有问题还有另外一个方法，就是在parser.add_argument的最后加一个dafault=默认值 获得路径 获得当前工作路径12import osos.getcwd() 编码错误如果碰到这个问题1'ascii' codec can't encode characters in position 0-15: ordinal not in range(128) 可以用下面的代码来解决123import sysreload(sys)sys.setdefaultencoding('utf8') 修改开源组件因为项目需要加打log，所以需要修改开源组件webpy来进行纪录log。默认情况下webpy会被安装在’/usr/local/lib/python2.7/dist-packages’这个目录下，你需要删除webpy相关的几个东西，看名字就可以知道了，如果只是重新编译并安装的话是没有用的，有些地方无法覆盖 内存分析工具meliae安装123sudo apt-get install python-pipsudo apt-get install Cythonsudo pip install meliae dump内存12from meliae import scannerscanner.dump_all_objects('/opt/log/dump.txt') 分析内存12345from meliae import loaderom = loader.load('/opt/log/dump.txt')om.compute_parents()om.collapse_instance_dicts()om.summarize() 排序算法文件锁最近遇到一个问题，我需要跑N个进程的训练程序，然后我想有序地输出结果到文件中，并且是以追加的形式，因此我要在打开文件之前就 非root安装third party modules1pip install --user &lt;包名&gt; 和*的用法这是两种方便的做法，比如 123456def foo(a,b): print(a) print(b)a=[1,2]foo(*a) 这里把a中的两个分开，分别给了a和b。 而**则可以把dict中的value赋给key，然后传到函数中。","raw":null,"content":null,"categories":[{"name":"Programmer","slug":"Programmer","permalink":"http://yezhejack.github.io/categories/Programmer/"}],"tags":[{"name":"编程语言","slug":"编程语言","permalink":"http://yezhejack.github.io/tags/编程语言/"},{"name":"python","slug":"python","permalink":"http://yezhejack.github.io/tags/python/"}]},{"title":"OS_X中安装虚拟linux服务器","slug":"OS-X中安装虚拟linux服务器","date":"2015-11-28T10:35:19.000Z","updated":"2016-05-05T00:43:26.000Z","comments":true,"path":"2015/11/28/OS-X中安装虚拟linux服务器/","link":"","permalink":"http://yezhejack.github.io/2015/11/28/OS-X中安装虚拟linux服务器/","excerpt":"","keywords":null,"text":"安装virtualbox安装Ubuntu这里使用14.04 LTS版本，因为主要是为乐运行一些程序，所以用的比较稳定的版本。设置VirtualBox的网络设置为桥接（bridge）模式，这样就能用宿主机的所在网段地址去用ssh访问，因为我希望这个server能够是透明的，这样我就可以肆无忌惮地用OS X的终端来直接使用。 设置SSH使用以下的命令12sudo apt-get updatesudo apt-get install openssh-server 输入1sudo ps -e|grep ssh 即可查看ssh服务是否启动了，如果有sshd，则说明服务已经启动了如果没有启动的话，输入1sudo service ssh start 查看ip地址1ifconfig 想要编译源代码通常想要编译源代码的话，需要安装一个build-essential的编译包，看名字就知道是用于编译的1sudo apt-get install build-essential 这个操作之后应该是能够帮你把常见的相关包安装上","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"OS X","slug":"OS-X","permalink":"http://yezhejack.github.io/tags/OS-X/"},{"name":"虚拟机","slug":"虚拟机","permalink":"http://yezhejack.github.io/tags/虚拟机/"},{"name":"MAC","slug":"MAC","permalink":"http://yezhejack.github.io/tags/MAC/"},{"name":"Linux","slug":"Linux","permalink":"http://yezhejack.github.io/tags/Linux/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://yezhejack.github.io/tags/Ubuntu/"}]},{"title":"vimrc设置方法","slug":"vimrc设置方法","date":"2015-11-24T08:26:22.000Z","updated":"2016-05-05T00:43:35.000Z","comments":true,"path":"2015/11/24/vimrc设置方法/","link":"","permalink":"http://yezhejack.github.io/2015/11/24/vimrc设置方法/","excerpt":"","keywords":null,"text":".vimrc的位置OS X和Linux的系统中，全局的vimrc的位置是/usr/share/vim/.vimrc，这个是不用更改的，可能也不能更改，但是我们可以将其复制到~/目录下，也就是//目录下，然后再进行更改。因为这个目录下的.vimrc的优先级比前者高，所以会优先使用这个。 .vimrc的配置从网络上搜刮来的现成配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" =&gt; General\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" Sets how many lines of history VIM has to rememberset history=700\" Enable filetype pluginsfiletype plugin onfiletype indent on\" Set to auto read when a file is changed from the outsideset autoread\" With a map leader it's possible to do extra key combinations\" like &lt;leader&gt;w saves the current filelet mapleader = \",\"let g:mapleader = \",\"\" Fast savingnmap &lt;leader&gt;w :w!&lt;cr&gt;\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" =&gt; VIM user interface\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" Set 7 lines to the cursor - when moving vertically using j/kset so=7\" Turn on the WiLd menuset wildmenu\" Ignore compiled filesset wildignore=*.o,*~,*.pyc\"Always show current positionset ruler\" Height of the command barset cmdheight=2\" A buffer becomes hidden when it is abandonedset hid\" Configure backspace so it acts as it should actset backspace=eol,start,indentset whichwrap+=&lt;,&gt;,h,l\" Ignore case when searchingset ignorecase\" When searching try to be smart about cases set smartcase\" Highlight search resultsset hlsearch\" Makes search act like search in modern browsersset incsearch\" Don't redraw while executing macros (good performance config)set lazyredraw\" For regular expressions turn magic onset magic\" Show matching brackets when text indicator is over themset showmatch\" How many tenths of a second to blink when matching bracketsset mat=2\" No annoying sound on errorsset noerrorbellsset novisualbellset t_vb=set tm=500set number\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" =&gt; Colors and Fonts\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" Enable syntax highlightingsyntax enablecolorscheme desertset background=dark\" Set extra options when running in GUI modeif has(\"gui_running\") set guioptions-=T set guioptions+=e set t_Co=256 set guitablabel=%M\\ %tendif\" Set utf8 as standard encoding and en_US as the standard languageset encoding=utf8\" Use Unix as the standard file typeset ffs=unix,dos,mac\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" =&gt; Files, backups and undo\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" Turn backup off, since most stuff is in SVN, git et.c anyway...set nobackupset nowbset noswapfile\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" =&gt; Text, tab and indent related\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" Use spaces instead of tabsset expandtab\" Be smart when using tabs ;)set smarttab\" 1 tab == 4 spacesset shiftwidth=4set tabstop=4\" Linebreak on 500 charactersset lbrset tw=500set ai \"Auto indentset si \"Smart indentset wrap \"Wrap lines\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" =&gt; Visual mode related\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" Visual mode pressing * or # searches for the current selection\" Super useful! From an idea by Michael Naumannvnoremap &lt;silent&gt; * :call VisualSelection('f')&lt;CR&gt;vnoremap &lt;silent&gt; # :call VisualSelection('b')&lt;CR&gt;\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" =&gt; Moving around, tabs, windows and buffers\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" Treat long lines as break lines (useful when moving around in them)map j gjmap k gk\" Map &lt;Space&gt; to / (search) and Ctrl-&lt;Space&gt; to ? (backwards search)map &lt;space&gt; /map &lt;c-space&gt; ?\" Disable highlight when &lt;leader&gt;&lt;cr&gt; is pressedmap &lt;silent&gt; &lt;leader&gt;&lt;cr&gt; :noh&lt;cr&gt;\" Smart way to move between windowsmap &lt;C-j&gt; &lt;C-W&gt;jmap &lt;C-k&gt; &lt;C-W&gt;kmap &lt;C-h&gt; &lt;C-W&gt;hmap &lt;C-l&gt; &lt;C-W&gt;l\" Close the current buffermap &lt;leader&gt;bd :Bclose&lt;cr&gt;\" Close all the buffersmap &lt;leader&gt;ba :1,1000 bd!&lt;cr&gt;\" Useful mappings for managing tabsmap &lt;leader&gt;tn :tabnew&lt;cr&gt;map &lt;leader&gt;to :tabonly&lt;cr&gt;map &lt;leader&gt;tc :tabclose&lt;cr&gt;map &lt;leader&gt;tm :tabmove\" Opens a new tab with the current buffer's path\" Super useful when editing files in the same directorymap &lt;leader&gt;te :tabedit &lt;c-r&gt;=expand(\"%:p:h\")&lt;cr&gt;/\" Switch CWD to the directory of the open buffermap &lt;leader&gt;cd :cd %:p:h&lt;cr&gt;:pwd&lt;cr&gt;\" Specify the behavior when switching between buffers try set switchbuf=useopen,usetab,newtab set stal=2catchendtry\" Return to last edit position when opening files (You want this!)autocmd BufReadPost * \\ if line(\"'\\\"\") &gt; 0 &amp;&amp; line(\"'\\\"\") &lt;= line(\"$\") | \\ exe \"normal! g`\\\"\" | \\ endif\" Remember info about open buffers on closeset viminfo^=%\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" =&gt; Status line\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" Always show the status lineset laststatus=2\" Format the status lineset statusline=\\ %&#123;HasPaste()&#125;%F%m%r%h\\ %w\\ \\ CWD:\\ %r%&#123;getcwd()&#125;%h\\ \\ \\ Line:\\ %l\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" =&gt; Editing mappings\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" Remap VIM 0 to first non-blank charactermap 0 ^\" Move a line of text using ALT+[jk] or Comamnd+[jk] on macnmap &lt;M-j&gt; mz:m+&lt;cr&gt;`znmap &lt;M-k&gt; mz:m-2&lt;cr&gt;`zvmap &lt;M-j&gt; :m'&gt;+&lt;cr&gt;`&lt;my`&gt;mzgv`yo`zvmap &lt;M-k&gt; :m'&lt;-2&lt;cr&gt;`&gt;my`&lt;mzgv`yo`zif has(\"mac\") || has(\"macunix\") nmap &lt;D-j&gt; &lt;M-j&gt; nmap &lt;D-k&gt; &lt;M-k&gt; vmap &lt;D-j&gt; &lt;M-j&gt; vmap &lt;D-k&gt; &lt;M-k&gt;endif\" Delete trailing white space on save, useful for Python and CoffeeScript ;)func! DeleteTrailingWS() exe \"normal mz\" %s/\\s\\+$//ge exe \"normal `z\"endfuncautocmd BufWrite *.py :call DeleteTrailingWS()autocmd BufWrite *.coffee :call DeleteTrailingWS()\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" =&gt; vimgrep searching and cope displaying\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" When you press gv you vimgrep after the selected textvnoremap &lt;silent&gt; gv :call VisualSelection('gv')&lt;CR&gt;\" Open vimgrep and put the cursor in the right positionmap &lt;leader&gt;g :vimgrep // **/*.&lt;left&gt;&lt;left&gt;&lt;left&gt;&lt;left&gt;&lt;left&gt;&lt;left&gt;&lt;left&gt;\" Vimgreps in the current filemap &lt;leader&gt;&lt;space&gt; :vimgrep // &lt;C-R&gt;%&lt;C-A&gt;&lt;right&gt;&lt;right&gt;&lt;right&gt;&lt;right&gt;&lt;right&gt;&lt;right&gt;&lt;right&gt;&lt;right&gt;&lt;right&gt;\" When you press &lt;leader&gt;r you can search and replace the selected textvnoremap &lt;silent&gt; &lt;leader&gt;r :call VisualSelection('replace')&lt;CR&gt;\" Do :help cope if you are unsure what cope is. It's super useful!\"\" When you search with vimgrep, display your results in cope by doing:\" &lt;leader&gt;cc\"\" To go to the next search result do:\" &lt;leader&gt;n\"\" To go to the previous search results do:\" &lt;leader&gt;p\"map &lt;leader&gt;cc :botright cope&lt;cr&gt;map &lt;leader&gt;co ggVGy:tabnew&lt;cr&gt;:set syntax=qf&lt;cr&gt;pggmap &lt;leader&gt;n :cn&lt;cr&gt;map &lt;leader&gt;p :cp&lt;cr&gt;\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" =&gt; Spell checking\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" Pressing ,ss will toggle and untoggle spell checkingmap &lt;leader&gt;ss :setlocal spell!&lt;cr&gt;\" Shortcuts using &lt;leader&gt;map &lt;leader&gt;sn ]smap &lt;leader&gt;sp [smap &lt;leader&gt;sa zgmap &lt;leader&gt;s? z=\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" =&gt; Misc\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" Remove the Windows ^M - when the encodings gets messed upnoremap &lt;Leader&gt;m mmHmt:%s/&lt;C-V&gt;&lt;cr&gt;//ge&lt;cr&gt;'tzt'm\" Quickly open a buffer for scripbblemap &lt;leader&gt;q :e ~/buffer&lt;cr&gt;\" Toggle paste mode on and offmap &lt;leader&gt;pp :setlocal paste!&lt;cr&gt;\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" =&gt; Helper functions\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"function! CmdLine(str) exe \"menu Foo.Bar :\" . a:str emenu Foo.Bar unmenu Fooendfunctionfunction! VisualSelection(direction) range let l:saved_reg = @\" execute \"normal! vgvy\" let l:pattern = escape(@\", '\\\\/.*$^~[]') let l:pattern = substitute(l:pattern, \"\\n$\", \"\", \"\") if a:direction == 'b' execute \"normal ?\" . l:pattern . \"^M\" elseif a:direction == 'gv' call CmdLine(\"vimgrep \" . '/'. l:pattern . '/' . ' **/*.') elseif a:direction == 'replace' call CmdLine(\"%s\" . '/'. l:pattern . '/') elseif a:direction == 'f' execute \"normal /\" . l:pattern . \"^M\" endif let @/ = l:pattern let @\" = l:saved_regendfunction\" Returns true if paste mode is enabledfunction! HasPaste() if &amp;paste return 'PASTE MODE ' en return ''endfunction\" Don't close window, when deleting a buffercommand! Bclose call &lt;SID&gt;BufcloseCloseIt()function! &lt;SID&gt;BufcloseCloseIt() let l:currentBufNum = bufnr(\"%\") let l:alternateBufNum = bufnr(\"#\") if buflisted(l:alternateBufNum) buffer # else bnext endif if bufnr(\"%\") == l:currentBufNum new endif if buflisted(l:currentBufNum) execute(\"bdelete! \".l:currentBufNum) endifendfunction","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yezhejack.github.io/tags/Linux/"},{"name":"vim","slug":"vim","permalink":"http://yezhejack.github.io/tags/vim/"}]},{"title":"sublime使用方法","slug":"sublime使用方法","date":"2015-11-24T02:24:12.000Z","updated":"2016-05-04T14:20:47.000Z","comments":true,"path":"2015/11/24/sublime使用方法/","link":"","permalink":"http://yezhejack.github.io/2015/11/24/sublime使用方法/","excerpt":"","keywords":null,"text":"sublime多行插入操作按住cmd然后用鼠标选择需要加入光标的位置","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"sublime","slug":"sublime","permalink":"http://yezhejack.github.io/tags/sublime/"},{"name":"编辑器","slug":"编辑器","permalink":"http://yezhejack.github.io/tags/编辑器/"}]},{"title":"Go语言的方法以及结构体  接口","slug":"Go语言的方法以及结构体","date":"2015-07-29T06:29:53.000Z","updated":"2016-05-04T14:13:53.000Z","comments":true,"path":"2015/07/29/Go语言的方法以及结构体/","link":"","permalink":"http://yezhejack.github.io/2015/07/29/Go语言的方法以及结构体/","excerpt":"","keywords":null,"text":"编译Golang Project 每次创建工程的时候都需要使用设置环境变量GOPATH来控制代码的位置1export GOPATH=&lt;path&gt; 可以设置路径 go build和go install的区别go build会生成一个输出文件，只能针对包含main函数的文件使用，会自动生成依赖go install会将二进制文件包都存在固定形式的目录下 方法 结构体的构造函数Go语言中没有构造函数这种东西，有别于C/C++。但是可以构造一个函数返回结构体指针，统一命名为New。这个称作Factory Method。 结构体的长度 1size := unsafe.Sizeof(T&#123;&#125;) 可以获得名字为T的结构体的长度 强制结构体用Factory Method来初始化将Struct Name的首字母小写，这样就导致其变为私有的（private），因此在其他包中都必须使用Factory Method来初始化结构体了 结构体的标志只能用reflect包内的内容来访问 匿名域和内嵌结构体这个和传统的OO编程中的继承很类似，可以实现一些继承的功能。直接使用类型名字来访问匿名域内嵌结构体可以直接实现继承域的功能 结构体的方法结构体的方法实际上是由函数加上一个接收者来表示这个方法是属于谁的。结构体的方法和结构体的定义都必须在同一个包里。通常接收者是一个指针 方法的继承对于多个继承关系的话，直接添加进去就好了 接口 接口定义结构1234type Namer interface &#123; Method1(param_list) return_type Method2(param_list) return_type ...&#125; 注意接口的名字是需要加上(e)r的，也就是比如一个接口的功能上我们将其命名为Select，那么接口的名字就要改为SelecterGo语言中的接口可以实例化，对应着的是一个多字的数据结构指针，未初始化时是一个nil指针 接口和实现可以放在不同的包里 利用接口实现多态将一个接口用多个结构体来实现，然后这些结构体中实现同一个接口的，然后实例化一个接口，这个接口可以用前面这些多个结构体赋值，从而实现了多态性和重载一个例子在Go的标准库io中，有定义了一个接口Reader123type Reader interface&#123; Read(p []byte)(n int, err error)&#125; 然后一下这些代码都是可行的123456var r io.Readerr=os.stdinr=bufio.NewReader(r)r = new(bytes.Buffer)f, _ := os.Open(“test.txt”)r = bufio.NewReader(f) 因为从第二行开始的右边的结构体中都实现了Reader这个接口，里面都有一个Read的函数与Reader中的对应","raw":null,"content":null,"categories":[{"name":"Programmer","slug":"Programmer","permalink":"http://yezhejack.github.io/categories/Programmer/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://yezhejack.github.io/tags/Golang/"},{"name":"编程语言","slug":"编程语言","permalink":"http://yezhejack.github.io/tags/编程语言/"}]},{"title":"goroutines以及channel","slug":"goroutines以及channel","date":"2015-07-28T07:20:46.000Z","updated":"2016-05-04T14:14:21.000Z","comments":true,"path":"2015/07/28/goroutines以及channel/","link":"","permalink":"http://yezhejack.github.io/2015/07/28/goroutines以及channel/","excerpt":"","keywords":null,"text":"goroutinesgoroutines并不对应着操作系统中的线程，它可以由多个线程来执行 gorouinte 之间又不像进程之间那样，使用独立的内存空间，它们使用共享的内存空间，因此它们存在读写同步的问题，使用go的channel。 gc-compiler对应着是真正的goroutines，每个都对应着一个或若干个线程，而gccgo则是为每一个goroutine建立一个线程。不要试图用print语句来显示多个进程间的真实顺序，因为print的延迟会导致显示的不是真正的顺序 channel 非缓存的channelchannel是无法暂时混存数据，因此发送端会被阻塞，除非接受端从channel中接收数据非缓存的channel很适合用于多个goroutines之间同步1234567891011package main import (“fmt” ) func f1(in chan int) &#123; fmt.Println(&lt;-in)&#125; func main() &#123; out := make(chan int) out &lt;- 2 go f1(out)&#125; 这段代码中总会出现deadlock作用。因为main这个goroutine运行到out&lt;-2时，因为是非缓存的，所以阻塞了，后面的f1还没有启动，因此所有的goroutine都进入了死亡状态 select和channel的使用 123456789 select &#123; case u:= &lt;- ch1: ... case v:= &lt;- ch2:... ... default: // no value ready to be received... &#125; 相当于是在从几个goroutine中选择已经有结果了的进行处理，如果外面加上一个无限循环的话，就能够达到不断处理的效果了针对以上这个代码如果有多个通道已经ready了的话，就随机从中选出一个如果没有ready的话它会等待如果添加了default的话，在没有一个是ready的情况下将会执行default的代码 缓存的channelcap函数可以得到buffe的大小发送端当且仅当channel满了的时候阻塞，接受端当且仅当channel空了的时候阻塞 利用goroutines和channel来进行并行编程可以讲channel当作一个信号量，用于锁住资源利用for循环的并行化 12345for i, v := range data &#123; go func (i int, v float64) &#123;&#125; doSomething(i, v)... &#125; (i, v) 利用缓存的channel来实现信号量channel的容量是我们想要进行同步的资源个数channel的现有长度是现在被占用的资源个数channel的容量－长度是现在可用的资源的个数 channel factory 模式可以在主goroutine中创建channel然后传入函数中，也可以在函数中创建channel然后返回到调用它的goroutine中来实现同步 指定channel的类型只发送或只接收 12var send_only chan&lt;- int // channel can only receive data var recv_only &lt;-chan int // channel can only send data","raw":null,"content":null,"categories":[{"name":"Programmer","slug":"Programmer","permalink":"http://yezhejack.github.io/categories/Programmer/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://yezhejack.github.io/tags/Golang/"},{"name":"信号量","slug":"信号量","permalink":"http://yezhejack.github.io/tags/信号量/"}]},{"title":"MAC OS X使用常见问题解决日志","slug":"MAC OS X使用常见问题解决日志","date":"2015-07-10T02:22:52.000Z","updated":"2016-05-04T14:16:12.000Z","comments":true,"path":"2015/07/10/MAC OS X使用常见问题解决日志/","link":"","permalink":"http://yezhejack.github.io/2015/07/10/MAC OS X使用常见问题解决日志/","excerpt":"","keywords":null,"text":"终端中显示的计算机名字如何更改这个的时候我也只是刚刚接触OS X刚刚几天的小白，但是作为一个认证的Unix系统，其中的终端是我肯定会用到的，但是在第一次开机配置MAC时没有注意，讲计算机的某个名称设置成了192，因此我的终端中就变成了这样12Last login: Fri Jul 10 11:04:22 on ttys000192:~ JackYip$ 每次看到这个该死的192就觉得特别的low因此萌生杀心，要解决这个问题，这里贡献一个比较快的方法，也是我使用的方法。打开终端，输入下面的命令，将其中的Tmp替换成自己想要的名字，可以支持 emoji表情的 1sudo scutil --set HostName Tmp 我就使用了Jack💪来作为HostName，这里也给出命令 1sudo scutil --set HostName Jack💪 然后用cmd＋Q强制退出终端，再开起来就可以了 常用快捷键 command+r刷新safari control+command+f 通常情况下可以最大化窗口","raw":null,"content":null,"categories":[{"name":"Howto","slug":"Howto","permalink":"http://yezhejack.github.io/categories/Howto/"}],"tags":[{"name":"MacBook Pro","slug":"MacBook-Pro","permalink":"http://yezhejack.github.io/tags/MacBook-Pro/"},{"name":"OS X","slug":"OS-X","permalink":"http://yezhejack.github.io/tags/OS-X/"}]}]}